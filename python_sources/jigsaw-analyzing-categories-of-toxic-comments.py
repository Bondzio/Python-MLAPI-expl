#!/usr/bin/env python
# coding: utf-8

# # Jigsaw: Analyzing categories of toxic comments
# 
# The 2019 Kaggle Jigsaw competition aims to identify toxic comments without model bias. This analysis study a new range of categories based on information present in the data, features such as female, male, christian. Those features were classified in new categories: Disability, Gender, Race, Religion and Sexual Orientation.
# 
# #### Results
# When each of those new categories were analyzed in existing (True) or not (False) in the comment, it was concluded that every time that the category exists the proportion of toxic comments is higher (see chart below). In some cases, the toxicity value increases in 3x: Disability, Race and Sexual Orientation.
# 
# #### *References:* 
# *Dataset creation:* https://www.kaggle.com/tatianass/jigsaw-dataframe-with-new-categories
# 
# *Tableau dashboard:* https://public.tableau.com/views/Jigsaw-CategoriesAnalysis/Jigsaw-CategoriesAnalysis?:embed=y&:display_count=yes&publish=yes
# 
# 

# <img src="https://i.imgur.com/K8YD9Sg.png" width="1360" height="768">
