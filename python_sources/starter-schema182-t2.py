#!/usr/bin/env python
# coding: utf-8

# In[ ]:



{
   "schemaVersion": 2,
   "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
   "config": {
      "mediaType": "application/vnd.docker.container.image.v1+json",
      "size": 19290,
      "digest": "sha256:80df1c851fa3bf3afead5d3c6dad3cc2ec8626b7dcdced8dd305a159f80ffe8d"
   },
   "layers": [
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 26692096,
         "digest": "sha256:423ae2b273f4c17ceee9e8482fa8d071d90c7d052ae208e1fe4963fceb3d6954"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 35365,
         "digest": "sha256:de83a2304fa1f7c4a13708a0d15b9704f5945c2be5cbb2b3ed9b2ccb718d0b3d"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 852,
         "digest": "sha256:f9a83bce3af0648efaa60b9bb28225b09136d2d35d0bed25ac764297076dec1b"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 163,
         "digest": "sha256:b6b53be908de2c0c78070fff0a9f04835211b3156c4e73785747af365e71a0d7"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 403170736,
         "digest": "sha256:5650063cfbfb957d6cfca383efa7ad6618337abcd6d99b247d546f94e2ffb7a9"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 81117097,
         "digest": "sha256:89142850430d0d812f21f8bfef65dcfb42efe2cd2f265b46b73f41fa65bef2fe"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 6868,
         "digest": "sha256:498b10157bcd37c3d4d641c370263e7cf0face8df82130ac1185ef6b2f532470"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 144376365,
         "digest": "sha256:a77a3b1caf74cc7c9fb700cab353313f1b95db5299642f82e56597accb419d7c"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1551901872,
         "digest": "sha256:0603289dda032b5119a43618c40948658a13e954f7fd7839c42f78fd0a2b9e44"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 467065,
         "digest": "sha256:c3ae245b40c1493b89caa2f5e444da5c0b6f225753c09ddc092252bf58e84264"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 324,
         "digest": "sha256:67e85692af8b802b6110c0a039f582f07db8ac6efc23227e54481f690f1afaae"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 450,
         "digest": "sha256:ea72ab3b716788097885d2d537d1d17c9dc6d9911e01699389fa8c9aa6cac861"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 197,
         "digest": "sha256:b02850f0d90ca01b50bbfb779bcf368507c266fc10cc1feeac87c926e9dda2c1"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 198,
         "digest": "sha256:4295de6959cedecdd0ba31406e15c19e38c13c0ebc38f3d6385725501063ef46"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 213,
         "digest": "sha256:d651a7c122d62d2869af2a5330c756f2f4b35a8e44902174be5c8ce1ad105edd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 213,
         "digest": "sha256:69e0b993e5f56695ee76b3776275dac236d38d32ba1f380fd78b900232e006ec"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 21081464,
         "digest": "sha256:76eb6a01fbbe8e12713bc5504ad1f0f52391955c553591b059f27b2f6e79024c"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 253,
         "digest": "sha256:6cfb0579f73a0c86edd04c296ec05bb34073a4d85cba73324a5f1b9f60136776"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 267,
         "digest": "sha256:bb07f87b0d491b8aeb4147b88fd88148e7844072c48218ca6cacb2b938f16b9f"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1384,
         "digest": "sha256:19a3313fd31b32db4db167d9d279ff8e7a55b265cc7c31e1fc0acb03a3ba4128"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 384,
         "digest": "sha256:179f31a0b43127df65cc699581db2c578dfd3abc6b323521908ceade6ee9bc5e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1926051283,
         "digest": "sha256:88c3f77a753ce1980969cf78cc79e130349e9be8b2bfabc5bdc76ddeae7e53a7"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 28959,
         "digest": "sha256:fb34744a245ce33bf571e7028c90c96a82c6ce082636a887bae5170de24a9c5a"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 111211777,
         "digest": "sha256:f123d9cc3bd9624cc12081283686bad71c5ee5eedce5cf8e484ae139ddbe6ef5"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 593961338,
         "digest": "sha256:3e847c367f901109e9a1913fc1c5426100526d56185d8608d31d157cdafa9d5c"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 623855132,
         "digest": "sha256:bbd6ae0789b6fb5e2783ca0474b547f111b1ad2c3a9ad72abdf3834f8f66716f"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 235210915,
         "digest": "sha256:5d5081cd74410bfe2ab5044dff743985d9da90632bdfe332edb3c99cae68ad63"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 10216,
         "digest": "sha256:68a17dee703fb005341aa2d50352d5be134e7b37a964e67f48905cc90a2fb243"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 145100843,
         "digest": "sha256:2c82d12950456bb0b9d46463b99f0f22f109ec83af2d38c1f80139123c67b01c"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 14110,
         "digest": "sha256:ad94a4adbd4de9849d7a716e7c46664ee5b8bd439cdfa5d831660b7ae3af3503"
      }
   ]
}


# In[ ]:



{
   "schemaVersion": 2,
   "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
   "config": {
      "mediaType": "application/vnd.docker.container.image.v1+json",
      "size": 31231,
      "digest": "sha256:79f52292b1d0c079b84bc79d002947be409a09b15a1320355a4de834f57b2ee8"
   },
   "layers": [
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 45339314,
         "digest": "sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95104141,
         "digest": "sha256:86534c0d13b7196a49d52a65548f524b744d48ccaf89454659637bee4811d312"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1571501372,
         "digest": "sha256:5764e90b1fae3f6050c1b56958da5e94c0d0c2a5211955f579958fcbe6a679fd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1083072,
         "digest": "sha256:ba67f7304613606a1d577e2fc5b1e6bb14b764bcc8d07021779173bcc6a8d4b6"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 526,
         "digest": "sha256:19abed793cf0a9952e1a08188dbe2627ed25836757d0e0e3150d5c8328562b4e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 458,
         "digest": "sha256:df204f1f292ae58e4c4141a950fad3aa190d87ed9cc3d364ca6aa1e7e0b73e45"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 13119161,
         "digest": "sha256:1f7809135d9076fb9ed8ee186e93e3352c861489e0e80804f79b2b5634b456dd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 555884253,
         "digest": "sha256:03a365d6218dbe33f5b17d305f5e25e412f7b83b38394c5818bde053a542f11b"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 102870915,
         "digest": "sha256:00e3d0b7af78551716541d2076836df5594948d5d98f04f382158ef26eb7c907"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95925388,
         "digest": "sha256:59782fefadba835c1e83cecdd73dc8e81121eae05ba58d3628a44a1c607feb6e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 142481172,
         "digest": "sha256:f81b01cf2c3f02e153a71704cc5ffe6102757fb7c2fcafc107a64581b0f6dc10"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1128076783,
         "digest": "sha256:f08bbb5c2bce948f0d12eea15c88aad45cdd5b804b71bee5a2cfdbf53c7ec254"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 444800302,
         "digest": "sha256:b831800c60a36c21033cb6e85f0bd3a5f5c9d96b2fa2797d0e8d4c50598180b8"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 157365696,
         "digest": "sha256:6d354ec67fa4ccf30460efadef27d48edf9599348cbab789c388f1d3a7fee232"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 63237273,
         "digest": "sha256:464f9b4eca5cdf36756cf0bef8c76b23344d0e975667becb743e8d6b9019c3cd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 427820542,
         "digest": "sha256:6c1f6bcbc63b982a86dc94301c2996505cec571938c60a434b3de196498c7b89"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 44581800,
         "digest": "sha256:c0a8110c6fede3cf54fa00a1b4e2fcb136d00b3cf490f658ec6d596e313c986e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 127637178,
         "digest": "sha256:c25df885c8dea40780f5b479bb6c7be924763399a21fa46c204d5bfac45056bd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 956429221,
         "digest": "sha256:7c1d98590e22f78a1b820f89b6ce245321437639957264e628b4abf4862e1223"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 586276809,
         "digest": "sha256:aab720d802b7d006b679ac10df4a259b3556812dea4dfc52d6111db47fc41e62"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 21717560,
         "digest": "sha256:5ee4a4cda8613a3fb172a827143aadacb98128479a22a280495604f989bf4483"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 93512644,
         "digest": "sha256:c4699852e987bc3fe9adde2544ffa690ad52ebec229c20f7e4153b015ac238ff"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 19141,
         "digest": "sha256:8d93692c8dcecacb8aca746a868f53d0b0cf1207e08ced8ffb2134bb01c1f871"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 84125618,
         "digest": "sha256:57c74d175611802a57531be97d19f586dc9cd810a5490eab04fd40b648312ead"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 3261,
         "digest": "sha256:1ac7a265bf03308e06e9cad7e77d12b22ca8bc6b7791d46398d95977e0042574"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2162,
         "digest": "sha256:1b4a5be69a4439f3de72877b7d408400e3aa0b4c37e9c70c4490b480bce682c0"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1270,
         "digest": "sha256:648046d6f6c28a42a39c9e68a9da90ccdabbd1ecfd0be77941114df4eb2406a4"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 644,
         "digest": "sha256:19a794f6956d460edfe74d5562d44366a7cf8bd46d83f408f1bf3c46e7282464"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2052,
         "digest": "sha256:880f92e310c2e03c28c5db85b342069b1a56cd13de7998ae52f699829774f075"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 875,
         "digest": "sha256:cad389727d6cd1696ed7e91b70eedd4c86fd30babb648e7be6cc1639582b0928"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 214,
         "digest": "sha256:c873da9a657a590abeae80bd3c0d0d87a6bfdfaf1d3873a0f210760a4050d6db"
      }
   ]
}


# ## Introduction
# Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue "Fork Notebook" button at the top of this kernel to begin editing.

# ## Exploratory Analysis
# To begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)

# In[ ]:


from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt # plotting
import numpy as np # linear algebra
import os # accessing directory structure
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


# There is 0 csv file in the current version of the dataset:
# 

# In[ ]:


for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


# The next hidden code cells define functions for plotting data. Click on the "Code" button in the published kernel to reveal the hidden code.

# In[ ]:


# Distribution graphs (histogram/bar graph) of column data
def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):
    nunique = df.nunique()
    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values
    nRow, nCol = df.shape
    columnNames = list(df)
    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow
    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')
    for i in range(min(nCol, nGraphShown)):
        plt.subplot(nGraphRow, nGraphPerRow, i + 1)
        columnDf = df.iloc[:, i]
        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):
            valueCounts = columnDf.value_counts()
            valueCounts.plot.bar()
        else:
            columnDf.hist()
        plt.ylabel('counts')
        plt.xticks(rotation = 90)
        plt.title(f'{columnNames[i]} (column {i})')
    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)
    plt.show()


# In[ ]:


# Correlation matrix
def plotCorrelationMatrix(df, graphWidth):
    filename = df.dataframeName
    df = df.dropna('columns') # drop columns with NaN
    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values
    if df.shape[1] < 2:
        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')
        return
    corr = df.corr()
    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')
    corrMat = plt.matshow(corr, fignum = 1)
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.columns)), corr.columns)
    plt.gca().xaxis.tick_bottom()
    plt.colorbar(corrMat)
    plt.title(f'Correlation Matrix for {filename}', fontsize=15)
    plt.show()


# In[ ]:


# Scatter and density plots
def plotScatterMatrix(df, plotSize, textSize):
    df = df.select_dtypes(include =[np.number]) # keep only numerical columns
    # Remove rows and columns that would lead to df being singular
    df = df.dropna('columns')
    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values
    columnNames = list(df)
    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots
        columnNames = columnNames[:10]
    df = df[columnNames]
    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')
    corrs = df.corr().values
    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):
        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)
    plt.suptitle('Scatter and Density Plot')
    plt.show()

     
RUN BASE_TAG=scatter
ARG TENSORFLOW_VERSION=2.1.0

FROM gcr.io/kaggle-images/python-tensorflow-whl:{TENSORFLOW_VERSION}-py37 as tensorflow_whl
FROM gcr.io/deeplearning-platform-release/base-cpu:{BASE_TAG}

ADD clean-layer.sh  /tmp/clean-layer.sh
ADD patches/nbconvert-extensions.tpl /opt/kaggle/nbconvert-extensions.tpl

# This is necessary for apt to access HTTPS sources
RUN apt-get update &&     apt-get install apt-transport-https &&     /tmp/clean-layer.sh

    # Use a fixed apt-get repo to stop intermittent failures due to flaky httpredir connections,
    # as described by Lionel Chan at http://stackoverflow.com/a/37426929/5881346
RUN sed -i "s/httpredir.debian.org/debian.uchicago.edu/" /etc/apt/sources.list &&     apt-get update &&     # Needed by vowpalwabbit & lightGBM (GPU build).
    # https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Python#installing
    # https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html#build-lightgbm
    apt-get install -y build-essential unzip cmake &&     apt-get install -y libboost-dev libboost-program-options-dev libboost-system-dev libboost-thread-dev libboost-math-dev libboost-test-dev libboost-python-dev libboost-filesystem-dev zlib1g-dev &&     pip install --upgrade pip &&     # enum34 is a backport of the Python 3.4 enum class to Python < 3.4.
    # No need since we are using Python 3.7. This is causing errors for packages
    # expecting the 3.7 version of enum. e.g. AttributeError: module 'enum' has no attribute 'IntFlag'
    pip uninstall -y enum34 &&     /tmp/clean-layer.sh

# Make sure the dynamic linker finds the right libstdc++
ENV LD_LIBRARY_PATH=/opt/conda/lib
# b/128333086: Set PROJ_LIB to points to the proj4 cartographic library.
ENV PROJ_LIB=/opt/conda/share/proj

# Install conda packages not available on pip.
# When using pip in a conda environment, conda commands should be ran first and then
# the remaining pip commands: https://www.anaconda.com/using-pip-in-a-conda-environment/
RUN conda install -c conda-forge matplotlib basemap cartopy python-igraph imagemagick pysal &&     # b/142337634#comment22 pin required to avoid torchaudio downgrade.
    conda install -c pytorch pytorch torchvision "torchaudio>=0.4.0" cpuonly &&     /tmp/clean-layer.sh

# The anaconda base image includes outdated versions of these packages. Update them to include the latest version.
# b/150498764 distributed 2.11.0 fails at import while trying to reach out to 8.8.8.8 since the network is disabled in our hermetic tests.
RUN pip install distributed==2.10.0 &&     pip install seaborn python-dateutil dask &&     pip install pyyaml joblib pytagcloud husl geopy ml_metrics mne pyshp &&     pip install pandas &&     # Install h2o from source.
    # Use `conda install -c h2oai h2o` once Python 3.7 version is released to conda.
    apt-get install -y default-jre-headless &&     pip install -f https://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o &&     /tmp/clean-layer.sh

# Install tensorflow from a pre-built wheel
COPY --from=tensorflow_whl /tmp/tensorflow_cpu/*.whl /tmp/tensorflow_cpu/
RUN pip install /tmp/tensorflow_cpu/tensorflow*.whl &&     rm -rf /tmp/tensorflow_cpu &&     /tmp/clean-layer.sh

RUN apt-get install -y libfreetype6-dev &&     apt-get install -y libglib2.0-0 libxext6 libsm6 libxrender1 libfontconfig1 --fix-missing &&     pip install gensim &&     pip install textblob &&     pip install wordcloud &&     pip install xgboost &&     # Pinned to match GPU version. Update version together.
    pip install lightgbm==2.3.1 &&     pip install git+git://github.com/Lasagne/Lasagne.git &&     pip install keras &&     pip install flake8 &&     #neon
    cd /usr/local/src &&     git clone --depth 1 https://github.com/NervanaSystems/neon.git &&     cd neon && pip install . &&     #nolearn
    pip install nolearn &&     pip install Theano &&     pip install pybrain &&     pip install python-Levenshtein &&     pip install hep_ml &&     # chainer
    pip install chainer &&     # NLTK Project datasets
    mkdir -p /usr/share/nltk_data &&     # NLTK Downloader no longer continues smoothly after an error, so we explicitly list
    # the corpuses that work
    # "yes | ..." answers yes to the retry prompt in case of an error. See b/133762095.
    yes | python -m nltk.downloader -d /usr/share/nltk_data abc alpino averaged_perceptron_tagger     basque_grammars biocreative_ppi bllip_wsj_no_aux     book_grammars brown brown_tei cess_cat cess_esp chat80 city_database cmudict     comtrans conll2000 conll2002 conll2007 crubadan dependency_treebank     europarl_raw floresta gazetteers genesis gutenberg     ieer inaugural indian jeita kimmo knbc large_grammars lin_thesaurus mac_morpho match     masc_tagged maxent_ne_chunker maxent_treebank_pos_tagger moses_sample movie_reviews     mte_teip5 names nps_chat omw opinion_lexicon paradigms     pil pl196x porter_test ppattach problem_reports product_reviews_1 product_reviews_2 propbank     pros_cons ptb punkt qc reuters rslp rte sample_grammars semcor senseval sentence_polarity     sentiwordnet shakespeare sinica_treebank smultron snowball_data spanish_grammars     state_union stopwords subjectivity swadesh switchboard tagsets timit toolbox treebank     twitter_samples udhr2 udhr unicode_samples universal_tagset universal_treebanks_v20     vader_lexicon verbnet webtext word2vec_sample wordnet wordnet_ic words ycoe &&     # Stop-words
    pip install stop-words &&     pip install scikit-image &&     /tmp/clean-layer.sh

RUN pip install ibis-framework &&     pip install mxnet &&     pip install gluonnlp &&     pip install gluoncv && \    
    tmp(/clean-layer.sh)

# scikit-learn dependencies
RUN pip install scipy &&     pip install scikit-learn &&     # HDF5 support
    pip install h5py &&     pip install biopython &&     # PUDB, for local debugging convenience
    pip install pudb &&     pip install imbalanced-learn &&     # Convex Optimization library
    # Latest version fails to install, see https://github.com/cvxopt/cvxopt/issues/77
    #    and https://github.com/cvxopt/cvxopt/issues/80
    # pip install cvxopt && \
    # Profiling and other utilities
    pip install line_profiler &&     pip install orderedmultidict &&     pip install smhasher &&     pip install bokeh &&     pip install numba &&     pip install datashader &&     # Boruta (python implementation)
    pip install Boruta &&     apt-get install -y graphviz && pip install graphviz &&     # Pandoc is a dependency of deap
    apt-get install -y pandoc &&     pip install git+git://github.com/scikit-learn-contrib/py-earth.git@issue191 &&     pip install essentia &&     /tmp/clean-layer.sh

# vtk with dependencies
RUN apt-get install -y libgl1-mesa-glx &&     pip install vtk &&     # xvfbwrapper with dependencies
    apt-get install -y xvfb &&     pip install xvfbwrapper &&     /tmp/clean-layer.sh

RUN pip install mpld3 &&     pip install mplleaflet &&     pip install gpxpy &&     pip install arrow &&     pip install nilearn &&     pip install nibabel &&     pip install pronouncing &&     pip install markovify &&     pip install imgaug &&     pip install preprocessing &&     pip install Baker &&     pip install path.py &&     pip install Geohash &&     # https://github.com/vinsci/geohash/issues/4
    sed -i -- 's/geohash/.geohash/g' /opt/conda/lib/python3.7/site-packages/Geohash/__init__.py &&     pip install deap &&     pip install tpot &&     pip install scikit-optimize &&     pip install haversine &&     pip install toolz cytoolz &&     pip install sacred &&     pip install plotly &&     pip install hyperopt &&     pip install fitter &&     pip install langid &&     # Delorean. Useful for dealing with datetime
    pip install delorean &&     pip install trueskill &&     pip install heamy &&     # Useful data exploration libraries (for missing data and generating reports)
    pip install missingno &&     pip install pandas-profiling &&     pip install s2sphere &&     pip install git+https://github.com/fmfn/BayesianOptimization.git &&     pip install matplotlib-venn &&     pip install pyldavis &&     pip install mlxtend &&     pip install altair &&     pip install pystan &&     pip install ImageHash &&     pip install ecos &&     pip install CVXcanon &&     pip install fancyimpute &&     pip install pymc3 &&     pip install tifffile &&     pip install spectral &&     pip install descartes &&     pip install geojson &&     pip install terminalplot &&     pip install pydicom &&     pip install wavio &&     pip install SimpleITK &&     pip install hmmlearn &&     pip install bayespy &&     pip install gplearn &&     pip install PyAstronomy &&     pip install squarify &&     pip install fuzzywuzzy &&     pip install python-louvain &&     pip install pyexcel-ods &&     pip install sklearn-pandas &&     pip install stemming &&     # b/148383434 remove pip install for holidays once fbprophet is compatible with latest version of holidays.
    pip install holidays==0.9.12 &&     pip install fbprophet &&     pip install holoviews &&     # 1.6.2 is not currently supported by the version of matplotlib we are using.
    # See other comments about why matplotlib is pinned.
    pip install geoviews==1.6.1 &&     pip install hypertools &&     pip install py_stringsimjoin &&     pip install nibabel &&     pip install mlens &&     pip install scikit-multilearn &&     pip install cleverhans &&     pip install leven &&     pip install catboost &&     # fastFM doesn't support Python 3.7 yet: https://github.com/ibayer/fastFM/issues/151
    # pip install fastFM && \
    pip install lightfm &&     pip install folium &&     pip install scikit-plot &&     # dipy requires the optional fury dependency for visualizations.
    pip install fury dipy &&     # plotnine 0.5 is depending on matplotlib >= 3.0 which is not compatible with basemap.
    # once basemap support matplotlib, we can unpin this package.
    pip install plotnine==0.4.0 &&     pip install scikit-surprise &&     pip install pymongo &&     pip install geoplot &&     pip install eli5 &&     pip install implicit &&     pip install dask-ml[xgboost] &&     /tmp/clean-layer.sh

RUN pip install kmeans-smote --no-dependencies &&     # Add google PAIR-code Facets
    cd /opt/ && git clone https://github.com/PAIR-code/facets && cd facets/ && jupyter nbextension install facets-dist/ --user &&     export PYTHONPATH=PYTHONPATH:/opt/facets/facets_overview/python/ &&     pip install tensorpack &&     pip install pycountry && pip install iso3166 &&     pip install pydash &&     pip install kmodes --no-dependencies &&     pip install librosa &&     pip install polyglot &&     pip install mmh3 &&     pip install fbpca &&     pip install sentencepiece &&     pip install cufflinks &&     pip install lime &&     pip install memory_profiler &&     /tmp/clean-layer.sh

# install cython & cysignals before pyfasttext
RUN pip install --upgrade cython &&     pip install --upgrade cysignals &&     pip install pyfasttext &&     # ktext has an explicit dependency on Keras 2.2.4 which is not
    # compatible with TensorFlow 2.0 (support was added in Keras 2.3.0).
    # Add the package back once it is fixed upstream.
    # pip install ktext && \
    pip install fasttext &&     apt-get install -y libhunspell-dev && pip install hunspell &&     pip install annoy &&     # Need to use CountEncoder from category_encoders before it's officially released
    pip install git+https://github.com/scikit-learn-contrib/categorical-encoding.git &&     pip install google-cloud-automl &&     # Newer version crashes (latest = 1.14.0) when running tensorflow.
    # python -c "from google.cloud import bigquery; import tensorflow". This flow is common because bigquery is imported in kaggle_gcp.py
    # which is loaded at startup.
    pip install google-cloud-bigquery==1.12.1 &&     pip install google-cloud-storage &&     pip install ortools &&     pip install scattertext &&     # Pandas data reader
    pip install pandas-datareader &&     pip install wordsegment &&     pip install pyahocorasick &&     pip install wordbatch &&     pip install emoji &&     # Add Japanese morphological analysis engine
    pip install janome &&     pip install wfdb &&     pip install vecstack &&     # Doesn't support Python 3.7 yet. Last release on pypi is from 2017.
    # Add back once this PR is released: https://github.com/scikit-learn-contrib/lightning/pull/133
    # pip install sklearn-contrib-lightning && \
    # yellowbrick machine learning visualization library
    pip install yellowbrick &&     pip install mlcrate &&     /tmp/clean-layer.sh

RUN pip install bcolz &&     pip install bleach &&     pip install certifi &&     pip install cycler &&     pip install decorator &&     pip install entrypoints &&     pip install html5lib &&     # Latest version breaks nbconvert: https://github.com/ipython/ipykernel/issues/422
    pip install ipykernel==5.1.1 &&     pip install ipython &&     pip install ipython-genutils &&     pip install ipywidgets &&     pip install isoweek &&     pip install jedi &&     pip install Jinja2 &&     pip install jsonschema &&     pip install jupyter &&     pip install jupyter-client &&     pip install jupyter-console &&     pip install jupyter-core &&     pip install MarkupSafe &&     pip install mistune &&     pip install nbconvert &&     pip install nbformat &&     pip install notebook==5.5.0 &&     pip install olefile &&     pip install opencv-python &&     pip install pandas_summary &&     pip install pandocfilters &&     pip install pexpect &&     pip install pickleshare &&     pip install Pillow &&     # Install openslide and its python binding
    apt-get install -y openslide-tools &&     # b/152402322 install latest from pip once is in: https://github.com/openslide/openslide-python/pull/76
    pip install git+git://github.com/rosbo/openslide-python.git@fix-setup &&     pip install ptyprocess &&     pip install Pygments &&     pip install pyparsing &&     pip install pytz &&     pip install PyYAML &&     pip install pyzmq &&     pip install qtconsole &&     pip install six &&     pip install terminado &&     # Latest version (6.0) of tornado breaks Jupyter notebook:
    # https://github.com/jupyter/notebook/issues/4439
    pip install tornado==5.0.2 &&     pip install tqdm &&     pip install traitlets &&     pip install wcwidth &&     pip install webencodings &&     pip install widgetsnbextension &&     pip install pyarrow &&     pip install feather-format &&     pip install fastai &&     pip install torchtext &&     pip install allennlp &&     # b/149359379 remove once allennlp 1.0 is released which won't cause a spacy downgrade.
    pip install spacy==2.2.3 && python -m spacy download en && python -m spacy download en_core_web_lg &&     /tmp/clean-layer.sh

    ###########
    #
    #      NEW CONTRIBUTORS:
    # Please add new pip/apt installs in this block. Don't forget a "&& \" at the end
    # of all non-final lines. Thanks!
    #
    ###########

RUN pip install flashtext &&     pip install wandb &&     pip install marisa-trie &&     pip install pyemd &&     pip install pyupset &&     pip install pympler &&     pip install s3fs &&     pip install featuretools &&     pip install -e git+https://github.com/SohierDane/BigQuery_Helper#egg=bq_helper && \
    pip install hpsklearn &&     pip install git+https://github.com/Kaggle/learntools &&     pip install kmapper &&     pip install shap &&     pip install ray &&     pip install gym &&     pip install tensorforce &&     pip install pyarabic &&     pip install conx &&     pip install pandasql &&     pip install tensorflow_hub &&     pip install jieba  &&     pip install git+https://github.com/SauceCat/PDPbox &&     pip install ggplot &&     pip install cesium &&     pip install rgf_python &&     # b/145404107: latest version force specific version of numpy and torch.
    pip install pytext-nlp==0.1.2 &&     pip install tsfresh &&     pip install pykalman &&     pip install optuna &&     pip install chainercv &&     pip install chainer-chemistry &&     pip install plotly_express &&     pip install albumentations &&     pip install catalyst &&     # b/145133331: latest version is causing issue with gcloud.
    pip install rtree==0.8.3 &&     # b/145133331 osmnx 0.11 requires rtree >= 0.9 which is causing issue with gcloud.
    pip install osmnx==0.10 &&     apt-get -y install libspatialindex-dev &&     pip install pytorch-ignite &&     pip install qgrid &&     pip install bqplot &&     pip install earthengine-api &&     pip install transformers &&     pip install dlib &&     pip install kaggle-environments &&     # b/149905611 The geopandas tests are broken with the version 0.7.0
    pip install geopandas==0.6.3 &&     pip install nnabla &&     pip install vowpalwabbit &&     /tmp/clean-layer.sh

# Tesseract and some associated utility packages
RUN apt-get install tesseract-ocr -y &&     pip install pytesseract &&     pip install wand==0.5.3 &&     pip install pdf2image &&     pip install PyPDF &&     pip install pyocr &&     /tmp/clean-layer.sh
ENV TESSERACT_PATH=/usr/bin/tesseract

# For Facets
ENV PYTHONPATH=PYTHONPATH:/opt/facets/facets_overview/python/
# For Theano with MKL
ENV MKL_THREADING_LAYER=GNU

# Temporary fixes and patches
    # Temporary patch for Dask getting downgraded, which breaks Keras
RUN pip install --upgrade dask &&     # Stop jupyter nbconvert trying to rewrite its folder hierarchy
    mkdir -p /root/.jupyter && touch /root/.jupyter/jupyter_nbconvert_config.py && touch /root/.jupyter/migrated &&     mkdir -p /.jupyter && touch /.jupyter/jupyter_nbconvert_config.py && touch /.jupyter/migrated &&     # Stop Matplotlib printing junk to the console on first load
    sed -i "s/^.*Matplotlib is building the font cache using fc-list.*$/# Warning removed by Kaggle/g" /opt/conda/lib/python3.7/site-packages/matplotlib/font_manager.py &&     # Make matplotlib output in Jupyter notebooks display correctly
    mkdir -p /etc/ipython/ && echo "c = get_config(); c.IPKernelApp.matplotlib = 'inline'" > /etc/ipython/ipython_config.py &&     /tmp/clean-layer.sh

# gcloud SDK https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu
RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main"     | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg |     apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - &&     apt-get update -y && apt-get install google-cloud-sdk -y &&     /tmp/clean-layer.sh

# Add BigQuery client proxy settings
ENV PYTHONUSERBASE "/root/.local"
ADD patches/kaggle_gcp.py /root/.local/lib/python3.7/site-packages/kaggle_gcp.py
ADD patches/kaggle_secrets.py /root/.local/lib/python3.7/site-packages/kaggle_secrets.py
ADD patches/kaggle_web_client.py /root/.local/lib/python3.7/site-packages/kaggle_web_client.py
ADD patches/kaggle_datasets.py /root/.local/lib/python3.7/site-packages/kaggle_datasets.py
ADD patches/log.py /root/.local/lib/python3.7/site-packages/log.py
ADD patches/sitecustomize.py /root/.local/lib/python3.7/site-packages/sitecustomize.py
# Override default imagemagick policies
ADD patches/imagemagick-policy.xml /etc/ImageMagick-6/policy.xml

# TensorBoard Jupyter extension. Should be replaced with TensorBoard's provided magic once we have
# worker tunneling support in place.
# b/139212522 re-enable TensorBoard once solution for slowdown is implemented.
# ENV JUPYTER_CONFIG_DIR "/root/.jupyter/"
# RUN pip install jupyter_tensorboard && \
#     jupyter serverextension enable jupyter_tensorboard && \
#     jupyter tensorboard enable
# ADD patches/tensorboard/notebook.py /opt/conda/lib/python3.7/site-packages/tensorboard/notebook.py

# Set backend for matplotlib
ENV MPLBACKEND "agg"

# We need to redefine TENSORFLOW_VERSION here to get the default ARG value defined above the FROM instruction.
# See: https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact
ARG TENSORFLOW_VERSION
ARG GIT_COMMIT=unknown
ARG BUILD_DATE=unknown

LABEL git-commit=GIT_COMMIT
LABEL build-date=BUILD_DATE
LABEL tensorflow-version=TENSORFLOW_VERSION

# Correlate current release with the git hash inside the kernel editor by running `!cat /etc/git_commit`.
RUN echo "$GIT_COMMIT" > /etc/git_commit && echo "$BUILD_DATE" > /etc/build_date


# Oh, no! There are no automatic insights available for the file types used in this dataset. As your Kaggle kerneler bot, I'll keep working to fine-tune my hyper-parameters. In the meantime, please feel free to try a different dataset.

# ## Conclusion
# This concludes your starter analysis! To go forward from here, click the blue "Fork Notebook" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!
