#!/usr/bin/env python
# coding: utf-8

# # Neural Network Visualization for Digit Recognition
# 
# This kernel visualizes how a [Multi-layer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) Neural Network recognizes hand-written digits from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) data set. 
# 
# A three-layer MLP does a decent job at recognizing hand-written digits. While this is a rather small model compared to other applications, it is already a black box to most people: image data goes in, classification results come out. But what happens in-between? What are the individual layers of the model doing and how do they behave on training data?
# 
# In order to investigate this question, I created this kernel to visualize all the individual neurons in this model and their output values as a function of input frame and digit.

# In[67]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation
from sklearn.model_selection import train_test_split
plt.rcParams["animation.html"] = "jshtml"

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.utils import to_categorical, plot_model
from keras.datasets import mnist
from keras import backend as K


# ## Training a Neural Network
# 
# We use a simple three-layer MLP with ReLU activation and dropout after each layer - this model is a slightly modified version of a model presented in chapter 1 of the book "Advanced Deep Learning with Keras" by Rowel Atienza, whish is available [here](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras). 
# 
# How does this model work?
# 
# The MLP takes every single pixel of the input frames as an input parameter. In the first hidden layer, each neuron takes every pixel value as input parameter. Every neuron in the second hidden layer then takes all the outputs of the first layer (after activation using ReLU) as input parameters. After applying a softmax activation, these results form the final output layer. 
# 
# The optimizer then derives linear weights in such a way as to minimize the loss function (in this case the *categorical crossentropy*) and thus maximizing the accuracy of the classification.

# In[68]:


# load mnist dataset
data = pd.read_csv('../input/train.csv')

# split data into train and test sample
x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,1:785], data.iloc[:,0],  
                                                    test_size = 0.1, random_state = 42)

x_train = x_train.values.reshape(37800, 784)
x_test = x_test.values.reshape(4200, 784)

# compute the number of labels
num_labels = len(np.unique(y_train))

# convert to one-hot vector
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# normalize
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# network parameters
input_size = x_train.shape[1]
batch_size = 64
dropout = 0.45


# In[69]:


# this model is a 3-layer MLP with ReLU and dropout after each layer
model = Sequential()
model.add(Dense(256, input_dim=input_size))
model.add(Activation('relu'))
model.add(Dropout(dropout))
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(dropout))
model.add(Dense(num_labels))
model.add(Activation('softmax'))
model.summary()

# loss function for one-hot vector using adam optimizer
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


# We train the model on the entire MNIST training data set:

# In[70]:


# train the network
model.fit(x_train, y_train, epochs=20, batch_size=batch_size)

# validate the model on test dataset to determine generalization
loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
print("\nTest accuracy: %.1f%%" % (100.0 * acc))


# This model implementation reaches an accuracy of ~98%, which is remarkable, but better results are possible using, e.g., a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network). We use the simpler MLP to make the interpretation of the visualization results easier.

# ## Extract Output and Group Information
# 
# We extract the outputs generated by each individual neuron and for each frame in the MNIST training sample and store them on a per-layer basis. 

# In[71]:


get_layer_output = K.function([model.layers[0].input, model.layers[0].input, model.layers[0].input],
                              [model.layers[1].output, model.layers[4].output, model.layers[7].output])

layer1_output, layer2_output, layer3_output = get_layer_output([x_train])


# Finally, we extract and store the indices of frames showing the same digit.

# In[72]:


train_ids = [np.arange(len(y_train))[y_train[:,i] == 1] for i in range(10)]


# ## Visualization of  Individual Frames
# 
# In this visualization, we focus on individual training data (i.e., individual frames with hand-written digits).
# 
# The following panel shows from left to right 
# * the original 28x28 pixel frame depicting a hand-written figure,
# * the output values of all neurons of the first hidden layer,
# * the output values of all neurons of the second hidden layer, and
# * the one-hot encoded output layer indicating the model classification result.
# 
# Note that in those plots showing network layers, each pixel stands for the output of a single neuron. This output is based on the input parameters passed on from the previous layer, the trained weights for each neuron, and the activation function used in this layer. Dark blue pixels stand for low output values, while yellow pixels stand for high output values. The pixels have been arranged in two dimensions to save space; just think of these layers in linear arrangements to stay in the typical picture of layers in a network. 

# In[73]:


get_ipython().run_cell_magic('capture', '', '%matplotlib inline\n\n# digit to be plotted\ndigit = 5\n\n# indices of frames to be plotted for this digit\nn = range(50)\n\n# initialize plots\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n\n# prepare plots\nax1.set_title(\'Input Layer\', fontsize=16)\nax1.axes.get_xaxis().set_visible(False)\nax1.axes.get_yaxis().set_visible(False)\n\nax2.set_title(\'Hidden Layer 1\', fontsize=16)\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\n\nax3.set_title(\'Hidden Layer 2\', fontsize=16)\nax3.axes.get_xaxis().set_visible(False)\nax3.axes.get_yaxis().set_visible(False)\n    \nax4.set_title(\'Output Layer\', fontsize=16)\nax4.axes.get_xaxis().set_visible(False)\nax4.axes.get_yaxis().set_visible(False)   \n\n# add numbers to the output layer plot to indicate label\nfor i in range(3):\n    for j in range(4):\n        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, \'\', \'\']][i][j],\n                        ha="center", va="center", color="w", fontsize=16)    \n        \ndef animate(id):\n    # plot elements that are changed in the animation\n    digit_plot = ax1.imshow(x_train[train_ids[digit][id]].reshape((28,28)), animated=True)\n    layer1_plot = ax2.imshow(layer1_output[train_ids[digit][id]].reshape((16,16)), animated=True)\n    layer2_plot = ax3.imshow(layer2_output[train_ids[digit][id]].reshape((8,8)), animated=True)\n    output_plot = ax4.imshow(np.append(layer3_output[train_ids[digit][id]], \n                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n    return digit_plot, layer1_plot, layer2_plot, output_plot,\n\n# define animation\nani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)')


# In[74]:


ani


# Scrolling through the animation, it becomes clear that in most cases the same subset of neurons fires, while other neurons remain quiescent. This is much more obvious in the second hidden layer than in the first hidden layer and can be interpreted as the first layer pre-processesing the pixel data, while the second layer deals with pattern recognition. Note that in most cases the recognition of the digit shown is unambiguous; ambiguity only occurs in somewhat pathologic cases.
# 
# You can change the digit shown by changing the `digit` value in the code block above. 

# ## Ensemble Visualization
# 
# Let's check the similarity in behavior for frames showing the same digit by looking at the ensemble properties. In this case, ensemble properties refers to how the neurons behave on average for a large number of frames showing the same digit.
# 
# We look into this by cumulating the output values of each neurons for $5 < n < 100$ frames with the same digit. The brighter a neuron, the more often it fires and the higher is its output value and hence also its impact on the overall classification.
# 
# For comparison, we will also sum up the pixel values in the original frames.

# In[75]:


get_ipython().run_cell_magic('capture', '', '%matplotlib inline\n\n# digit to be plotted\ndigit = 6\n\n# numbers of frames to be summed over\nn = np.append([1], np.linspace(5, 100, 20, dtype=int))\n\n# initialize plots\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n\n# add a counter indicating the number of frames used in the summation\ncounter = ax1.text(1, 2, \'n={}\'.format(0), color=\'white\', fontsize=16, animated=True)\n\n# prepare plots\nax1.set_title(\'Input Layer\', fontsize=16)\nax1.axes.get_xaxis().set_visible(False)\nax1.axes.get_yaxis().set_visible(False)\n\nax2.set_title(\'Hidden Layer 1\', fontsize=16)\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\n\nax3.set_title(\'Hidden Layer 2\', fontsize=16)\nax3.axes.get_xaxis().set_visible(False)\nax3.axes.get_yaxis().set_visible(False)\n    \nax4.set_title(\'Output Layer\', fontsize=16)\nax4.axes.get_xaxis().set_visible(False)\nax4.axes.get_yaxis().set_visible(False)   \n\n# add numbers to the output layer plot to indicate label\nfor i in range(3):\n    for j in range(4):\n        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, \'\', \'\']][i][j],\n                        ha="center", va="center", color="w", fontsize=16)    \n        \ndef animate(id):\n    # plot elements that are changed in the animation\n    digit_plot = ax1.imshow(np.sum(x_train[train_ids[digit][:id]], axis=0).reshape((28,28)), animated=True)\n    layer1_plot = ax2.imshow(np.sum(layer1_output[train_ids[digit][:id]], axis=0).reshape((16,16)), animated=True)\n    layer2_plot = ax3.imshow(np.sum(layer2_output[train_ids[digit][:id]], axis=0).reshape((8,8)), animated=True)\n    output_plot = ax4.imshow(np.append(np.sum(layer3_output[train_ids[digit][:id]], axis=0), \n                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n    counter.set_text(\'n={}\'.format(id))\n    return digit_plot, layer1_plot, layer2_plot, output_plot, counter,\n\n# define animation\nani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)')


# In[76]:


ani


# After summing up the responses of as little as 20-30 frames, the pattern in the second hidden layer is almost static. After combining about 70-80 frames, also the pattern in the first hidden layer appears static. This supports the idea that only a subset of all neurons is involved in the recognition of individual digits.

# ## Comparison of Layers for Different Digits
# 
# Finally, we compare the static patterns that emerge in the hidden layers for the different labels, i.e., the different digits.
# 
# ### First Hidden Layer

# In[77]:


f, ax_arr = plt.subplots(2, 5, figsize=(15,10))

f.subplots_adjust(wspace=0.05, bottom=0.5, top=0.95)

for i, ax in enumerate(np.ravel(ax_arr)):
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
    if i <= 10:
        ax.set_title('- {} -'.format(i), fontsize=16)
        layer1_plot = ax.imshow(np.sum(layer1_output[train_ids[i]], axis=0).reshape((16,16)))


# These patterns look sufficiently random - except for the digits `4` and `9`, which look remarkably similar. This makes sense as the hand-written digits themselves strongly resemble each other in the writing style used in the US, on which the MNIST dataset is based. 
# 
# We quantify the similarities between the normalized static first-hidden layer patterns for pairs of digits by multiplication and summation over these patterns.

# In[78]:


similarity_layer1 = np.zeros((10,10))

for i in range(10):
    for j in range(10):
        sum_i_normalized = np.sqrt(np.sum(layer1_output[train_ids[i]], axis=0)/np.sum(layer1_output[train_ids[i]]))
        sum_j_normalized = np.sqrt(np.sum(layer1_output[train_ids[j]], axis=0)/np.sum(layer1_output[train_ids[j]]))
        similarity_layer1[i,j] = np.sum(sum_i_normalized*sum_j_normalized)


# In[79]:


f, ax = plt.subplots()

similarity_layer1_plot = ax.imshow(similarity_layer1, origin='lower')
plt.colorbar(similarity_layer1_plot)


# This matrix shows the similarity of the static pattern for the different digits. Values on the diagonal show by default the highest possible similarity, as they compare the same digits with each other. But even off the diagonal, there are cases that show high levels of similarity; these cases include the digits `3` and `5`, `7` and `9`, as well as `4` and `9`. Indeed, all of these digits show some level of similarity when written down - depending on your writing style...

# ### Second Hidden Layer
# 
# We repeat this analysis for the second hidden layer.

# In[80]:


f, ax_arr = plt.subplots(2, 5, figsize=(15,10))

f.subplots_adjust(wspace=0.05, bottom=0.5, top=0.95)

for i, ax in enumerate(np.ravel(ax_arr)):
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
    if i <= 10:
        ax.set_title('- {} -'.format(i), fontsize=16)
        layer2_plot = ax.imshow(np.sum(layer2_output[train_ids[i]], axis=0).reshape((8,8)))


# Interestingly, patterns for the digit combinations that showed a high level of similarity in the first hidden layer (`3|5`, `7|9`, `4|9`) do not seem to show the same level of similarity in hidden layer 2. 
# 
# Keep in mind that this second hidden layer is not based on the pixel values directly - it takes outputs from hidden layer 1 as input. Hence, it is much harder to interpret the patterns shown here as they are based on patterns found in the first hidden layer.

# In[81]:


similarity_layer2 = np.zeros((10,10))

for i in range(10):
    for j in range(10):
        sum_i_normalized = np.sqrt(np.sum(layer2_output[train_ids[i]], axis=0)/np.sum(layer2_output[train_ids[i]]))
        sum_j_normalized = np.sqrt(np.sum(layer2_output[train_ids[j]], axis=0)/np.sum(layer2_output[train_ids[j]]))
        similarity_layer2[i,j] = np.sum(sum_i_normalized*sum_j_normalized)


# In[82]:


f, ax = plt.subplots()

similarity_layer2_plot = ax.imshow(similarity_layer2, origin='lower')
plt.colorbar(similarity_layer2_plot)


# This reduced similarity is reflected by the similarity matrix, which is much smoother and has fewer digit combinations stick out with elevated similarities. 

# ## Conclusions
# 
# * Despite variations in the shapes of hand-written digits, the same groups of neurons is involved in the identification of the same digits.
# * Similarities in the shapes of digits translate into similarities in the groups of neurons that are involved in their identification in the first hidden layer, but not so much in the second hidden layer.

# In[ ]:




