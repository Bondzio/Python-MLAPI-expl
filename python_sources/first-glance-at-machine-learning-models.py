#!/usr/bin/env python
# coding: utf-8

# # Predicting the price of houses in Iowa and Melbourne
# **This tutorial covers both, level 1 and level 2 of the machine learning track**
# 
# In order to start we first need to load the data:

# In[ ]:


import pandas as pd

main_file_path = '../input/house-prices-advanced-regression-techniques/train.csv'
data = pd.read_csv(main_file_path)


# For a more comfortable data manipulation,  you might want to change all columns names to lowercase.

# In[ ]:


data.columns = [col.lower() for col in data.columns]


# Next, I like to import all the classes in the same note, so we can run the most pieces of code independently:

# In[ ]:


from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import Imputer
from sklearn.pipeline import make_pipeline
from xgboost import XGBRegressor


# # LEVEL 1
# 
# Getting to know with the data

# In[ ]:


data.head()


# In order to prove that the prices of Melbourne's houses rised untill 2008 mortgage crisis, we separate the data by the year sold and the saleprice, then we check the average price.   

# In[ ]:


(data.groupby('yrsold')
    .saleprice
    .mean()
    .sort_values(ascending=False))


# As we can see, Melbourne's houses didn't escape from the global crisis
# 
# Following the price and demand law we can assume that the years with fewer houses sold should be the ones with higher average prices. Let see

# In[ ]:


(data.assign(x=0)
     .groupby('yrsold')
     .x
     .count()
     .sort_values(ascending=False)
)


# Here is a counterintuitive outcome, it seems that the economical context of that those years allowed many people to have resources to buy a house no matter the prices. Maybe people's intuition led them to believe that houses' prices were going to rise and they see a chance of doing bussiness. 
# 
# Now familiarized with the data, let's start constructing the model:
# 
# First, we define X by convention this variable contains de variables used to predict the target variable, y, which contains the saleprices of the houses. For now, X will only contain numeric features and replace NaN values for 0.  

# In[ ]:


X = data.drop(['saleprice'], axis=1).select_dtypes(exclude='object').fillna(0)
y = data.saleprice


# Now we choose a model. In this case we will use a simple decission tree model.
# 
# **Decission Tree Regressor:**  It is a nested labels based model which classifies data with similar characteristics.  Web page: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html
# 
# We instance the model, previously imported, then we compute de parameters of the model:

# In[ ]:


data_model = DecisionTreeRegressor()
data_model.fit(X, y)


# In[ ]:


print('Predicting the next houses prices')
print(X.head())
print('The predictions are: ')
print(data_model.predict(X.head()))


# This information is not enough to judge the model so we will implement some function to evaluate it.
# 
# **Mean Absolute Error:** is a commonly use metric to evaluate a model, It is the mean obtained by resting the real values from the values generated by the model, all of them in absolute value. Web page: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html

# In[ ]:


prediction = data_model.predict(X)
mean_absolute_error(y, prediction)


# **Our model is perfect!!!**
# 
# Well perfectly wrong, what we seeing here is a case of overfitting, our model perfectly learned the patterns of the training data but It is useless in the real world. Our mission is to create a model that can give us useful information for new data, data that was not use in the training. So we can use train_test_split to separate the data and prove the model with unseen data:

# In[ ]:


train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)

valid_model = DecisionTreeRegressor()
valid_model.fit(train_X, train_y)
valid_model_prediction = valid_model.predict(val_X)
mean_absolute_error(val_y, valid_model_prediction)


# The outcome is really disappointing, but real world models are much likely to be like that, it is the very known trade-off between learning and generalization.
# 
# Next we can prove sistematically the same model with different parameters to find the most accurate one:

# In[ ]:


def get_mae(max_leaf_nodes, predictor_train, predictor_val, targ_train, targ_val):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=36)
    model.fit(predictor_train, targ_train)
    pred_val = model.predict(predictor_val)
    mae = mean_absolute_error(targ_val, pred_val)
    return mae


# In[ ]:


for max_leaf_nodes in [5, 50, 500, 5000, None]:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print('Max leaf nodes: {} \t\t Mean Absolute Error: {}'.format(max_leaf_nodes, my_mae))


# In this particular data, as the model goes deeper in nested classificators the accuracy is lower. We can assume that as it goes deeper it learns the patterns of the training data which are not useful to predict unseen data.
# 
# **ENSEMBLE MODELS**
# 
# These models consists in running two or more related models with the same data to synthesize the result in one score, which is usually more accurated.
# 
# **Random Forest Regressor** is an ensemble model that run many random Decision Tree Regressor models and synthesize their results. Web page: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html
# 

# In[ ]:


rand_model = RandomForestRegressor(random_state=500)
rand_model.fit(train_X, train_y)
rand_model_pred = rand_model.predict(val_X)
mean_absolute_error(val_y, rand_model_pred)


# In[ ]:


def get_forest_mae(max_leaf_nodes, predictor_train, predictor_val, targ_train, targ_val):
    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=500)
    model.fit(predictor_train, targ_train)
    pred_val = model.predict(predictor_val)
    mae = mean_absolute_error(targ_val, pred_val)
    return mae
for max_leaf_nodes in [5, 50, 500, 5000, None]:
    my_mae = get_forest_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print('Max leaf nodes: {} \t\t Mean Absolute Error: {}'.format(max_leaf_nodes, my_mae))


# # LEVEL 2
# 
# Now we will apply more sophisticated techniques to the Melbourne data.

# In[ ]:


melb = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')
melb.columns = [col.lower() for col in melb.columns]
melb.head()


# In[ ]:


X2 = melb.drop(['price'], axis=1).select_dtypes(exclude='object')
y2 = melb.price


# Now we will use some techniques to replace NaN values, more complicated than replace all NaN's with zero. And in order to evaluate their efficience we will create a function that returns the Mean Absolute Error:

# In[ ]:


def score_dataset(train_X, val_X, train_y, val_y):
    model = RandomForestRegressor(n_jobs=4)
    model.fit(train_X, train_y)
    model_predictions = model.predict(val_X)
    mae = mean_absolute_error(val_y, model_predictions)
    return mae


# **1)** Eliminate the columns with NaN values: 

# In[ ]:


train_X2, val_X2, train_y2, val_y2 = train_test_split(X2, y2, random_state=0)
col_miss = [col for col in train_X2.columns
                       if train_X2[col].isnull().any()]
reduced_train_X2 = train_X2.drop(col_miss, axis=1)
reduced_val_X2 = val_X2.drop(col_miss, axis=1)
print('Mean Absolute Error: {}'.format(score_dataset(reduced_train_X2, reduced_val_X2, train_y2, val_y2)))


# **2)** Use an imputer to replace NaN values: 
# 
# **Imputer:** It is a function which replaces NaN values with the mean value of the column, because it is the most common value in it, so by probability it is the best way to replace those values. You can also choose the mode or the median. Web page: http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html

# In[ ]:


impute = SimpleImputer()
imp_train_X2 = impute.fit_transform(train_X2)
imp_val_X2 = impute.transform(val_X2)
print('Mean Absolute Error: {}'.format(score_dataset(imp_train_X2, imp_val_X2, train_y2, val_y2)))


# **3)** Highlighting the rows we are going to replace with the imputer:

# In[ ]:


impute_train_X = train_X2.copy()
impute_val_X = val_X2.copy()

for col in col_miss:
    impute_train_X[col + ' was missing'] = impute_train_X[col].isnull()
    impute_val_X[col + ' was missing'] = impute_val_X[col].isnull()
impute_train_X2 = impute.fit_transform(impute_train_X)
impute_val_X2 = impute.transform(impute_val_X)
print('Mean Absolute Error: {}'.format(score_dataset(impute_train_X2, impute_val_X2, train_y2, val_y2)))


# Now that we can create more accurate models with impute techniques, let's go one step further and incorporate categorical variables to our models. To acomplish that we will need to encode this categoricals. One way to do that is with One Hot Encoder:
# 
# **One Hot Encoder:** It is an encoding technique that creates new columns to every categorical and assign 1 for rows that contains that categorical and 0 to the ones that not. It is commonly used in datasets that doesn't have more than 10 different categoricals per column. Pandas has a simple function that implements this technique, pd.get_dummies(). Web page:  https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html 

# In[ ]:


melbX = pd.get_dummies(melb.drop(['price'], axis=1))
melby = melb.price


# Before training a model with these variables let's see how large is the predictors dataframe:

# In[ ]:


melbX.shape


# We can see that our predictors dataframe is extremely large, which means that using One Hot Encoder is not the best way to create a better model. In spite of this fact if we run the following code we can get a more precise model. 

# In[ ]:


melbX_train, melbX_val, melby_train, melby_val = train_test_split(melbX, melby, test_size=0.25)
impute_melbX_train = impute.fit_transform(melbX_train)
impute_melbX_val = impute.transform(melbX_val)


# By reusing the above function score_dataset() we can see how the accuracy improve, I will leave the code below in case you want to run it yourself:
# 
# > score_dataset(impute_melbX_train, impute_melbX_val, melby_train, melby_val)
# 
# On the other hand, if we want to get a better evaluation for a model, we should use cross validation.
# 
# **Cross Validation:** Consist in separating the train data from the test data in an iterative process which always pick different samples in every run. Scikit-learn's function is cross_val_score. Web page: http://scikit-learn.org/stable/modules/cross_validation.html
# 
# This process should be used in relatively small datasets so we are going to reduce our dataframes dropping the columns with NaN values.

# In[ ]:


temp_X = melb.drop(['price'], axis=1) 
enc = pd.get_dummies(temp_X)
enc_colmiss = [col for col in enc.columns if enc[col].isnull().any()]
encoded = enc.drop(enc_colmiss, axis=1)
noenc_colmiss = [col for col in X2.columns if X2[col].isnull().any()]
no_encoded = X2.drop(noenc_colmiss, axis=1)


# In[ ]:


def scorer(X, y):
    return -1 * cross_val_score(RandomForestRegressor(50, n_jobs=4), X, y, scoring='neg_mean_absolute_error').mean()

score_encoded = scorer(encoded, y2)
score_no_encoded = scorer(no_encoded, y2)
print('Score with One Hot Encoder: {} \n Score without One Hot Encoder: {}'.format(str(score_encoded), str(score_no_encoded)))


# Some people say that data science models are black boxes, because you cannot obtain information about the relationship between the variables which compose the model. Most of those people do not know Partial Dependence Plots. 
# 
# **Partial Dependence Plots:** Graphs that show the relationship between a variable and the target. Web page: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.plot_partial_dependence.html
# 
# Note: For now this method only works with Gradient Boosting Regressor model. Web page: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html

# In[ ]:


plotX = impute.fit_transform(X2)
grad_model = GradientBoostingRegressor()
grad_model.fit(plotX, y2)
plots = plot_partial_dependence(grad_model, 
                                features=[1, 6], 
                                X=plotX, 
                                feature_names=['rooms', 'distance', 'postcode', 'bedroom2', 'bathroom', 'car',
     'landsize', 'buildingarea', 'yearbuilt', 'lattitude', 'longtitude', 'propertycount'], 
                                grid_resolution=15, 
                                n_jobs=4)


# As we can see, the left graph show us a negative correlation between distance and price and the right graph show us a positive correlation between landsize and price. Both are intuitive but this will not be always the case, datasets can surprise us in many ways.
# 
# ### Returning to the Iowa dataset to prove some of these methods and some new ones:
# 
# Managing NaN values: 

# In[ ]:


iX = data.drop(['saleprice'], axis=1).select_dtypes(exclude='object')
iy = data.saleprice
train_iX, val_iX, train_iy, val_iy = train_test_split(iX, iy, random_state=0)
i_train_iX = impute.fit_transform(train_iX)
i_val_iX = impute.transform(val_iX)
print('Mean Absolute Error: {}'.format(score_dataset(i_train_iX, i_val_iX, train_iy, val_iy)))


# In[ ]:


impute_train_iX = train_iX.copy()
impute_val_iX = val_iX.copy()

for col in iX.columns:
    impute_train_iX[col + ' was missing'] = impute_train_iX[col].isnull()
    impute_val_iX[col + ' was missing'] = impute_val_iX[col].isnull()
impute_train_iX_plus = impute.fit_transform(impute_train_iX)
impute_val_iX_plus = impute.transform(impute_val_iX)
print('Mean Absolute Error: {}'.format(score_dataset(impute_train_iX_plus, impute_val_iX_plus, train_iy, val_iy)))


# For the last method, we will introduce Pipelines.
# 
# **Pipelines:** Functions that allow us to run a preprocessing method and a model in the same line, making our code more legible and easy to follow. Web page: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html

# In[ ]:


pip = make_pipeline(SimpleImputer(), RandomForestRegressor(n_jobs=4))
cross_scores = cross_val_score(pip, iX, iy, scoring='neg_mean_absolute_error')
print('Mean Absolute Error: {}'.format(cross_scores.mean() * -1))


# **Encoding the data**

# In[ ]:


encX = pd.get_dummies(data.drop(['saleprice'], axis=1))
missed = [col for col in encX.columns if encX[col].isnull().any()]
encodedX = encX.drop(missed, axis=1)
mis = [col for col in iX.columns if iX[col].isnull().any()]
noencodedX = iX.drop(mis, axis=1)


# In[ ]:


iowa_encoded = scorer(encodedX, iy)
iowa_noencoded = scorer(noencodedX, iy)
print('Score with One Hot Encoder: {} \n Score without One Hot Encoder: {}'.format(iowa_encoded, iowa_noencoded))


# # Leading Model For Predictions: XGboost
# 
# **XGBoost:** is an implementation of the Gradient Boosted Decision Trees algorithm, which means that it go through cycles building ensemble models that it uses to keep improving the final model. This might not be an accurate definition but you can think of it like an ensemble model of ensemble models. Web page: https://xgboost.readthedocs.io/en/latest/python/python_api.html

# In[ ]:


Xboost = pd.get_dummies(data.drop(['saleprice'], axis=1))
yboost = data.saleprice


# In[ ]:


def model_pipeline(transform, model, X, y):
    pip = make_pipeline(transform, model)
    cross_scores = cross_val_score(pip, X, y, scoring='neg_mean_absolute_error')
    return print('Mean Absolute Error: {}'.format(cross_scores.mean() * -1))


# In[ ]:


model_pipeline(SimpleImputer(), XGBRegressor(n_jobs=4), Xboost, yboost)
model_pipeline(SimpleImputer(), XGBRegressor(n_estimators=1000, n_jobs=4), Xboost, yboost)
model_pipeline(SimpleImputer(), XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=4), Xboost, yboost)


# Finally, we can se how this model is the one with the best results, and how as the introduction continued our code become more legible and more efficient. But the most important part is that we get to this code without jumping the fundamentals, which means that we know exactly what is happening behind this elegant code.
# 
# ## I really hope this tutorial was helpful!  
# 
# 
# 
