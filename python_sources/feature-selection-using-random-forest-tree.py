#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category = DeprecationWarning)
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.


# **Reading the datafile using pandas**

# In[ ]:


df = pd.read_csv("../input/dataFile.csv", header = None)


# **Looking at the first 5 rows of data **

# In[ ]:


df.head()


# It can be seen frm the data that 
# 1. There are 50 Columns all together
# 2. Column 50 , is the label which we need to predict
# 
# Let us seperate the features and labels

# In[ ]:


features = df.drop(columns = 50)
labels  = df[50]


# 
# 
# Let us begin with standardizing the dataset 

# In[ ]:


features.head()


# In[ ]:


scaled  = StandardScaler()
scaled_df = scaled.fit_transform(features.values)


# In[ ]:


scaled_df = pd.DataFrame(scaled_df)


# In[ ]:


scaled_df.head()


# **Splitting the data into Test and train**

# In[ ]:


X_train, X_test, y_train, y_test  = train_test_split(scaled_df, labels, test_size = 0.4, random_state = 42 )


# In[ ]:


print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)


# **Using a KNN Model to test the accuracy**

# In[ ]:


clf = KNeighborsClassifier()
clf.fit(X_train,y_train)
results_knn  = clf.predict(X_test)
accuracy = accuracy_score(y_test,results_knn)
print(round(accuracy,3))


# **Using Random Forest Classifier ensemble to get the importance of the features**

# In[ ]:


clf = RandomForestClassifier()
clf.fit(X_train,y_train)
result_rfc = clf.predict(X_test)
accuracy = accuracy_score(y_test,result_rfc)
print(round(accuracy,3))


# **Looking at the importance of the features generated by random forest classifier model**

# In[ ]:


clf.feature_importances_


# **Getting the indicies of the features from low importance to high importance**

# In[ ]:


feature_ranking  = np.argsort(clf.feature_importances_)
feature_ranking


# **This indicates that the feature 33 is the most import and feature 43 is the least important for the RFC**
# > We will now loop through the features and removed the features with each iteration and record the accuracy of KNN model

# In[ ]:


accuracy_obtained = []
features_removed = []
for cols in range(0,len(X_train.columns)):
    feature_deletion = feature_ranking[0:cols]
    features_train = np.delete(X_train.values, feature_deletion, axis =1)
    features_test = np.delete(X_test.values, feature_deletion, axis =1)
    clf = KNeighborsClassifier()
    clf.fit(features_train,y_train)
    results_knn  = clf.predict(features_test)
    accuracy = accuracy_score(y_test,results_knn)
    accuracy_obtained.append(accuracy)
    features_removed.append(len(feature_deletion)) 
    print(f'Accuarcy = {round(accuracy,3)} where features removed are {len(feature_deletion)}' )
    


# **We will now plot the graph between number of features removed and the accuracy obtained using Matplotlib**

# In[ ]:


plt.figure()
plt.xlabel('Number of Features Removed')
plt.ylabel('Accuracy achieved')
plt.plot(features_removed,accuracy_obtained)
plt.show()


# **It can be observed that the accuracy of the model depends upon the correct selection of features and how features play important role in getting better results.**
# 
# > **Please upvote if it helped :)**
