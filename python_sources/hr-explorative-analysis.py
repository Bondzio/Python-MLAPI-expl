{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6cad692c-9335-b8e9-56b2-5c4396e35f06"},"source":"**INTRODUCTION**  \nA company success greatly depends upon the value of its employees. Thus, it is important to monitor which one leave the company in order to retain the best talent as long as possible.\nThis dataset provides information about employees performance, satisfaction level and whether they have left the company.   \nIn this notebook I perform an exploratory analysis in order to get some insights about the current trends in the company, based on that I create a predictive model that allows to foresee which employee is going to leave next."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e407ec8e-03f5-4358-b6b4-9ac1e767896e"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndata = pd.read_csv('../input/HR_comma_sep.csv', header=0)\ndata.tail()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8daa788-5c19-54be-bced-3f3ce687aede"},"source":"**DATASET**  \nThe dataset provides 10 attributes, 8 of which are numerical. Conveniently there are no missing values.  \nAttribute *left* is a boolean value that tells us whether the employee left yet, it is going to be used as label for my predictive model.\n\nDespite being a categorical value, *salary* can be converted in a numerical value given its sortable nature. I added a new column called *nsalary* with value ranging between 1 and 3 corresponding to the category in *salary*. This is done to include *salary* information in all those functions that only accept numerical values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5854a477-8b57-2a34-04ea-dcac2b1ff28c"},"outputs":[],"source":"data['n_salary'] = np.where(data['salary']=='low', 1, np.where(data['salary']=='medium',2,3))\ndata.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"737f3103-03fa-225e-83ce-aa05c0598deb"},"source":"**GOING SPECIFIC**  \nAs previously said, the purpose of this analysis is to increase the retain rate of *good* employees. In this regard, it is better to slice the dataset in order to work only on data corresponding to those employees that, according to some criteria, qualify as *good*.  \n\nA possible criterium is to set a threshold in the evaluation score (*last evaluation*), above which an employee is good by definition. **A data-driven option for such threshold can be the median of attribute *last evaluation***.  \nWhile some may argue that this stretch the definition of what a good employee is, I believe that it nicely match our case."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32d573fd-714b-c3b5-6dd6-d820e37feb9d"},"outputs":[],"source":"med = data['last_evaluation'].median()\ndata_best = data[data['last_evaluation']>=med]\ndata_worst = data[data['last_evaluation']<med]"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc210365-5598-57e9-3efb-06ed4caf9e8c"},"source":"**TEST AND TRAIN SET**  \nBefore proceeding further, I split the dataset in training and testing set. The test set size is 20% of the total amount of records.  \nTest data will only be used to assess the accuracy of the predictive model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8332227e-cd0c-8a5a-fa05-8b0741995851"},"outputs":[],"source":"msk = np.random.rand(len(data_best)) < 0.8\ndf = data_best[msk]\nX_test = data_best[~msk]\ny_test = X_test['left']\nX_test = X_test.drop(['left','sales','salary'],1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a5c25a5a-4e97-3619-efda-938981723fea"},"source":"**THE DEPARTMENTS**  \nSometimes employees leave because of bad colleagues or management. If this applies to our company,  some departments should have a much higher turnover with respect to the average.  \nAccording to this plot, employees retaining is not homogeneous across different departments. In particular *sales*, *technical* and *support* are losing far more employees than the other departments.  \n\n**But is that really the case?**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fdcd4de6-b79f-098c-5da6-a7ee77dfe04a"},"outputs":[],"source":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nX_left = df[df['left']==1]\nplt.figure(figsize=(12,5))\nsns.countplot(x='sales', data=X_left, order=X_left['sales'].unique())"},{"cell_type":"markdown","metadata":{"_cell_guid":"eccaf13f-003b-dca7-da65-670cf8d5f95f"},"source":"What if *sales* department is simply bigger, and consequently it has more chances of losing an employee?\nCounting and normalization come to the rescue in this plot that shows the number of employees that left, relative to the overall size of the departments.  \nResults are completely different. Leaving rate tends to be homogenous across different departments, with an average value of 24% and standard deviation 0.04%. In other words, **it seems that the employees leave regardless of their role inside the company**.  \nNot surprisingly, *management* is the only exception since managers are paid more and usually spend more time in the same company in order to build their careers."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44fd5dae-1258-bd25-9301-93cf27508dbe"},"outputs":[],"source":"plt.figure(figsize=(12,5))\nX_dep = pd.DataFrame()\nX_dep['count_total'] = df.groupby(['sales']).count()['left']\nX_dep['count_left'] = X_left.groupby(['sales']).count()['left']\nX_dep['left_ratio'] = X_dep['count_left']/X_dep['count_total']\nX_dep = X_dep.reset_index()\nsns.barplot(x='sales', y='left_ratio', data=X_dep, order=X_left['sales'].unique())\nprint(\"Average leaving rate: \"+ \n     str(np.round(X_dep['left_ratio'].mean(), decimals=2)*100)+\n     \"% with standard deviation: \"+\n     str(np.round(X_dep['left_ratio'].std(), decimals=2)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"77a0106c-5aa8-9f20-e332-13b8d1a7aea5"},"source":"**CORRELATION MATRIX**  \nThe correlation matrix of the dataset let me understand how much each attribute correlate with each other. Plotting an heatmap give me an additional visual aid to speed up the process.  \n\nIn this map the more colorful is the box, the higher is the correlation between attributes(either positive or negative). As we can see, some of the attributes are nicely correlated with each other, revealing some of the mechanisms that guide employees choices.  \n\n**In particular, notice how employees that *left* are highly correlated with working a lot, being in the company for many years and having a low satisfaction.** This insight alone is enough to have a reasonable explanation of what makes employees leave, **but let's dig deeper**."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c3b3cf9-f803-681c-193d-8c99df47bd84"},"outputs":[],"source":"\ncorr = df.corr()\nplt.figure(figsize=(10, 6))\nax = sns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d5f09b86-ffc6-a852-ab39-6c43a436da6d"},"source":"**PATTERNS**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"303db863-d3da-1905-9570-c551ea6abb11"},"outputs":[],"source":"sns.pairplot(data=X_left[['satisfaction_level', \n                          'number_project', \n                          'average_montly_hours', \n                          'time_spend_company']],\n                          hue='time_spend_company',\n                          palette='Blues',\n                          plot_kws={'alpha':0.3})"},{"cell_type":"markdown","metadata":{"_cell_guid":"297b12fb-fa7f-5d15-3e19-17e858f882bf"},"source":"**PRINCIPAL COMPONENTS**  \nWhenever a good correlation between variables exists, it is wise to attempt dimensional reduction. This is usually done by PCA, that consists in finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.\n\nEigenvectors define the axis of the new subspace. In order to decide which one can dropped without losing too much information in the lower-dimensional subspace, we need to inspect the corresponding eigenvalues: **The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped**.\n\nHere we can observe how variance is distributed between the different components. The first 3 components contain more than 50% of the total variance, but we need 6 out of 8 components to get close to 90% of it.  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b21f671-4924-ef3a-f85c-9c0fb5d4baeb"},"outputs":[],"source":"X = df.drop(['sales','salary','left'],1)\ny = df['left']\n\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\nX_std = pd.DataFrame(X_std)\n\ncorr = X_std.corr()\neig_vals, eig_vecs = np.linalg.eig(corr)\ntot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nplt.style.context('seaborn-whitegrid')\nplt.figure(figsize=(10, 5))\n\nplt.bar(range(len(eig_vals)), var_exp, alpha=0.5, align='center',\n        label='individual explained variance')\nplt.step(range(len(eig_vals)), cum_var_exp, where='mid',\n         label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"740e46ce-064d-2938-22c3-1e02a63896c1"},"source":"**VISUALIZE DATA**  \nVisualize data is often helpful to identify segments in the dataset.  \nSince plotting high dimensional data in a meaningful way is difficult, I've taken advantage of PCA to get a scatter plot of the principal components of the dataset.  \nIn this plot, red dots represents employees that did not leave, while the blue ones correspond to those that do left. It seems that most of the employees that left the company are nicely separated from the majority of the remaining ones, thus suggesting that it will not be difficult to forecast which one are going to leave soon.  \n\nMore important than that, **it appears that exist two categories of good employees that left**, this is very evident in both the plot below."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39d592df-7b70-acc6-9b61-9822dafd45c4"},"outputs":[],"source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=7)\nX_pca = pca.fit_transform(X_std)\n\nplt.figure(figsize=(10, 8))\nfor i in range(0,2):\n    plt.subplot(2,1,i+1)\n    plt.style.context('seaborn-whitegrid')\n    for lab, col in zip((1,0),\n                        ('blue', 'red')):\n        plt.scatter(X_pca[np.array(y==lab), 0],\n                    X_pca[np.array(y==lab), i+1],\n                    label=lab,\n                    c=col, \n                    alpha=0.1)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component '+str(i+1+1))\n    plt.legend(loc='upper left')\n    plt.tight_layout()\n    #plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2563131f-77f6-195d-b0da-8c55b17c93a7"},"source":"**PREDICTING WHO IS GOING TO LEAVE**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0279d00b-c2e4-a582-0a28-1f0b0b019479"},"outputs":[],"source":"from sklearn import ensemble\n\nclf = ensemble.RandomForestClassifier()\nclf.fit(X_pca, y)\nX_test_pca = pca.transform(X_test)\nprint(clf.score(X_test_pca, y_test))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f65e217-d2a6-f3c1-ef07-f49800b1eb12"},"outputs":[],"source":"result=np.array([])\nfor i in range(len(y_test)):\n    if X_test_pca[i,0]<-2 or X_test_pca[i,0]>1.9:\n        result = np.append(result, 1)\n    else:\n        result = np.append(result, 0)\nscore = np.absolute(np.subtract(result, y_test)).sum()\nscore = score/len(y_test)\nscore"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7be53077-1c5f-7705-15e7-01e2affa9a50"},"outputs":[],"source":"clf.fit(X, y)\nprint(clf.score(X_test, y_test))"},{"cell_type":"markdown","metadata":{"_cell_guid":"76cf4510-45ba-c1a6-8480-ef7835364bac"},"source":"**TO BE CONTINUED**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2bb60ff7-d252-ee22-953f-00b8d60d6e9b"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}