#!/usr/bin/env python
# coding: utf-8

# In[ ]:



{
   "schemaVersion": 2,
   "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
   "config": {
      "mediaType": "application/vnd.docker.container.image.v1+json",
      "size": 27512,
      "digest": "sha256:8ef19b5397d8b13638e69746589be8265f4e9f565ee5af4d64f43f5d14a68a64"
   },
   "layers": [
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 45339314,
         "digest": "sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95104141,
         "digest": "sha256:86534c0d13b7196a49d52a65548f524b744d48ccaf89454659637bee4811d312"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1571501372,
         "digest": "sha256:5764e90b1fae3f6050c1b56958da5e94c0d0c2a5211955f579958fcbe6a679fd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1083072,
         "digest": "sha256:ba67f7304613606a1d577e2fc5b1e6bb14b764bcc8d07021779173bcc6a8d4b6"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 526,
         "digest": "sha256:36c8cee5dcabe015f8e5b00d9e5f26f3dc43c685616a9ff657aeac32dcb0dec7"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 456,
         "digest": "sha256:fbde6884bcec90a734814ab616cc8abcf34cde78a99498df8da757431c6c28fd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 13117845,
         "digest": "sha256:4aceba2705e51efc04a48b7883d097f3c89d00a2f96b2fb16b54a7d5fc410e53"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 558997840,
         "digest": "sha256:690778d6efe115dbba1239a78693944fe179985f5a5d31078d376731eb900635"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 718986208,
         "digest": "sha256:cfc8fe521bf9c7e028edea60d6f3cbd2a50f56751c0e8d7415d6d364453b41d0"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 33502401,
         "digest": "sha256:5a2d591ac4f68ab561f030733f354b722051f02fb7114a632a980d4095e9f6a5"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95925401,
         "digest": "sha256:b720a0e96c3024ee325ea8e1874a33d66d097c990ac50e8229b1c76076ae869a"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 142490052,
         "digest": "sha256:a661d11e464bd9abfabe3ec4b4b4e22b01c228481ba20d5dd6c066ab512e26c2"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1038014421,
         "digest": "sha256:555cc8cba1c97f86ef332cee16e11b952f18352f55b915df4a9a776d81edd234"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 75007872,
         "digest": "sha256:7ed8a9307f830dad6f7b8b273c80b0a820bdf9f9db7ad1c762282ef8b63e4122"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 80474425,
         "digest": "sha256:29eb9237adacfa8ff7974c2ce5e9f1ffc5047e625347bbb03b5a170d397153fb"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 89451858,
         "digest": "sha256:6b054a59f9fd46636ecb9f0c31a837127ae856fd44e5b998286f5f1111bf1d30"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 415384338,
         "digest": "sha256:19c088e1afee706e063eaff6a2d259efb55b962f4da47927f9461a83d904c8a1"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 44168262,
         "digest": "sha256:838b7e776f75e4fdce36596b5ee8e250ebb50cdc2717033290df2bff0e70a7dc"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 107758705,
         "digest": "sha256:1ad83d09763421093849a7abef397f8610f79e07767f51e3248ab9ef52679705"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 958342282,
         "digest": "sha256:569c6fda9d84413ea844c2f25799b7449b2fd6ac486bcbb8be2eb1ca65b6c51e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 588654554,
         "digest": "sha256:d8ad2accaf088624da0281224e625ff49da8212cf9e21423898f50f648542d40"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 21654895,
         "digest": "sha256:1754ab792f2a4062623d4f461f9196ed41ec1bf9eb81b45ea05fe0fe6a4df3c0"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 227370423,
         "digest": "sha256:153e0f49aeb357a372ddccfffe487d5b431fc81143728d1a65805a38454c477a"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 20307,
         "digest": "sha256:6f8920d2e4f3aa5274096bbbf084a9587d1ff438026fac3ee7931e3f75008de3"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 85317368,
         "digest": "sha256:5857d6464a2ed88c711915621eb005d178603b9daabbf65ccfe2fd2e72d7be36"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 3264,
         "digest": "sha256:5347c992f3b56e47242dd8a5694638c839cad94e9635876f2bfe9e8dd36dd62c"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2163,
         "digest": "sha256:dd6f840a7b975737ae3f11a10036c7501bd6796ca86befd2596712365a9fd073"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1269,
         "digest": "sha256:a12c0432261d580586748b11db6bbfe798f5957a9ad57a71230c0f9986826114"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 641,
         "digest": "sha256:112b56a741fa6492ba1a4f9eda937bcb52f02f7c31265e142a592824bf830c36"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2052,
         "digest": "sha256:bcd81def64e80646bbebb0cd99ecfe423c0ec3df21c607fceb2f9c3a2b782e1e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 876,
         "digest": "sha256:daf7bad905212cda27468f9f136e888189f0cde90182e6eb488937740a70ac38"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 210,
         "digest": "sha256:37f94f1dfe09302f5ab426ed04a71a4bad5cc9585d65611518efb8ebc1ea5ba5"
      }
   ]
}


# In[ ]:


ARG BASE_TAG=latest
ARG TENSORFLOW_VERSION=2.1.0
 
FROM gcr.io/kaggle-images/python-tensorflow-whl:${TENSORFLOW_VERSION}-py37-2 as tensorflow_whl
FROM gcr.io/deeplearning-platform-release/base-cpu:${BASE_TAG}
 
ADD clean-layer.sh  /tmp/clean-layer.sh
ADD patches/nbconvert-extensions.tpl /opt/kaggle/nbconvert-extensions.tpl
 
# This is necessary for apt to access HTTPS sources
RUN apt-get update &&     apt-get install apt-transport-https &&     /tmp/clean-layer.sh
 
    # Use a fixed apt-get repo to stop intermittent failures due to flaky httpredir connections,
    # as described by Lionel Chan at http://stackoverflow.com/a/37426929/5881346
RUN sed -i "s/httpredir.debian.org/debian.uchicago.edu/" /etc/apt/sources.list &&     apt-get update &&     # Needed by vowpalwabbit & lightGBM (GPU build).
    # https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Python#installing
    # https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html#build-lightgbm
    apt-get install -y build-essential unzip cmake &&     apt-get install -y libboost-dev libboost-program-options-dev libboost-system-dev libboost-thread-dev libboost-math-dev libboost-test-dev libboost-python-dev libboost-filesystem-dev zlib1g-dev &&     pip install --upgrade pip &&     # enum34 is a backport of the Python 3.4 enum class to Python < 3.4.
    # No need since we are using Python 3.7. This is causing errors for packages
    # expecting the 3.7 version of enum. e.g. AttributeError: module 'enum' has no attribute 'IntFlag'
    pip uninstall -y enum34 &&     /tmp/clean-layer.sh
 
# Make sure the dynamic linker finds the right libstdc++
ENV LD_LIBRARY_PATH=/opt/conda/lib
# b/128333086: Set PROJ_LIB to points to the proj4 cartographic library.
ENV PROJ_LIB=/opt/conda/share/proj
 
# Install conda packages not available on pip.
# When using pip in a conda environment, conda commands should be ran first and then
# the remaining pip commands: https://www.anaconda.com/using-pip-in-a-conda-environment/
RUN conda install -c conda-forge matplotlib basemap cartopy python-igraph imagemagick pysal &&     # b/142337634#comment22 pin required to avoid torchaudio downgrade.
    conda install -c pytorch pytorch torchvision "torchaudio>=0.4.0" cpuonly &&     /tmp/clean-layer.sh
 
# The anaconda base image includes outdated versions of these packages. Update them to include the latest version.
# b/150498764 distributed 2.11.0 fails at import while trying to reach out to 8.8.8.8 since the network is disabled in our hermetic tests.
RUN pip install distributed==2.10.0 &&     pip install seaborn python-dateutil dask &&     pip install pyyaml joblib pytagcloud husl geopy ml_metrics mne pyshp &&     pip install pandas &&     # Install h2o from source.
    # Use `conda install -c h2oai h2o` once Python 3.7 version is released to conda.
    apt-get install -y default-jre-headless &&     pip install -f https://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o &&     /tmp/clean-layer.sh
 
# Install tensorflow from a pre-built wheel
COPY --from=tensorflow_whl /tmp/tensorflow_cpu/*.whl /tmp/tensorflow_cpu/
RUN pip install /tmp/tensorflow_cpu/tensorflow*.whl &&     rm -rf /tmp/tensorflow_cpu &&     /tmp/clean-layer.sh
 
RUN apt-get install -y libfreetype6-dev &&     apt-get install -y libglib2.0-0 libxext6 libsm6 libxrender1 libfontconfig1 --fix-missing &&     pip install gensim &&     pip install textblob &&     pip install wordcloud &&     pip install xgboost &&     # Pinned to match GPU version. Update version together.
    pip install lightgbm==2.3.1 &&     pip install git+git://github.com/Lasagne/Lasagne.git &&     pip install keras &&     pip install flake8 &&     #neon
    cd /usr/local/src &&     git clone --depth 1 https://github.com/NervanaSystems/neon.git &&     cd neon && pip install . &&     #nolearn
    pip install nolearn &&     pip install Theano &&     pip install pybrain &&     pip install python-Levenshtein &&     pip install hep_ml &&     # chainer
    pip install chainer &&     # NLTK Project datasets
    mkdir -p /usr/share/nltk_data &&     # NLTK Downloader no longer continues smoothly after an error, so we explicitly list
    # the corpuses that work
    # "yes | ..." answers yes to the retry prompt in case of an error. See b/133762095.
    yes | python -m nltk.downloader -d /usr/share/nltk_data abc alpino averaged_perceptron_tagger     basque_grammars biocreative_ppi bllip_wsj_no_aux     book_grammars brown brown_tei cess_cat cess_esp chat80 city_database cmudict     comtrans conll2000 conll2002 conll2007 crubadan dependency_treebank     europarl_raw floresta gazetteers genesis gutenberg     ieer inaugural indian jeita kimmo knbc large_grammars lin_thesaurus mac_morpho machado     masc_tagged maxent_ne_chunker maxent_treebank_pos_tagger moses_sample movie_reviews     mte_teip5 names nps_chat omw opinion_lexicon paradigms     pil pl196x porter_test ppattach problem_reports product_reviews_1 product_reviews_2 propbank     pros_cons ptb punkt qc reuters rslp rte sample_grammars semcor senseval sentence_polarity     sentiwordnet shakespeare sinica_treebank smultron snowball_data spanish_grammars     state_union stopwords subjectivity swadesh switchboard tagsets timit toolbox treebank     twitter_samples udhr2 udhr unicode_samples universal_tagset universal_treebanks_v20     vader_lexicon verbnet webtext word2vec_sample wordnet wordnet_ic words ycoe &&     # Stop-words
    pip install stop-words &&     pip install scikit-image &&     /tmp/clean-layer.sh
 
RUN pip install ibis-framework &&     pip install mxnet &&     pip install gluonnlp &&     pip install gluoncv && \    
    tmp(/clean-layer.sh)
 
# scikit-learn dependencies
RUN pip install scipy &&     pip install scikit-learn &&     # HDF5 support
    pip install h5py &&     pip install biopython &&     # PUDB, for local debugging convenience
    pip install pudb &&     pip install imbalanced-learn &&     # Convex Optimization library
    # Latest version fails to install, see https://github.com/cvxopt/cvxopt/issues/77
    #    and https://github.com/cvxopt/cvxopt/issues/80
    # pip install cvxopt && \
    # Profiling and other utilities
    pip install line_profiler &&     pip install orderedmultidict &&     pip install smhasher &&     pip install bokeh &&     pip install numba &&     pip install datashader &&     # Boruta (python implementation)
    pip install Boruta &&     apt-get install -y graphviz && pip install graphviz &&     # Pandoc is a dependency of deap
    apt-get install -y pandoc &&     pip install git+git://github.com/scikit-learn-contrib/py-earth.git@issue191 &&     pip install essentia &&     /tmp/clean-layer.sh
 
# vtk with dependencies
RUN apt-get install -y libgl1-mesa-glx &&     pip install vtk &&     # xvfbwrapper with dependencies
    apt-get install -y xvfb &&     pip install xvfbwrapper &&     /tmp/clean-layer.sh
 
RUN pip install mpld3 &&     pip install mplleaflet &&     pip install gpxpy &&     pip install arrow &&     pip install nilearn &&     pip install nibabel &&     pip install pronouncing &&     pip install markovify &&     pip install imgaug &&     pip install preprocessing &&     pip install Baker &&     pip install path.py &&     pip install Geohash &&     # https://github.com/vinsci/geohash/issues/4
    sed -i -- 's/geohash/.geohash/g' /opt/conda/lib/python3.7/site-packages/Geohash/__init__.py &&     pip install deap &&     pip install tpot &&     pip install scikit-optimize &&     pip install haversine &&     pip install toolz cytoolz &&     pip install sacred &&     pip install plotly &&     pip install hyperopt &&     pip install fitter &&     pip install langid &&     # Delorean. Useful for dealing with datetime
    pip install delorean &&     pip install trueskill &&     pip install heamy &&     # Useful data exploration libraries (for missing data and generating reports)
    pip install missingno &&     pip install pandas-profiling &&     pip install s2sphere &&     pip install git+https://github.com/fmfn/BayesianOptimization.git &&     pip install matplotlib-venn &&     pip install pyldavis &&     pip install mlxtend &&     pip install altair &&     pip install pystan &&     pip install ImageHash &&     pip install ecos &&     pip install CVXcanon &&     pip install fancyimpute &&     pip install pymc3 &&     pip install tifffile &&     pip install spectral &&     pip install descartes &&     pip install geojson &&     pip install terminalplot &&     pip install pydicom &&     pip install wavio &&     pip install SimpleITK &&     pip install hmmlearn &&     pip install bayespy &&     pip install gplearn &&     pip install PyAstronomy &&     pip install squarify &&     pip install fuzzywuzzy &&     pip install python-louvain &&     pip install pyexcel-ods &&     pip install sklearn-pandas &&     pip install stemming &&     # b/148383434 remove pip install for holidays once fbprophet is compatible with latest version of holidays.
    pip install holidays==0.9.12 &&     pip install fbprophet &&     pip install holoviews &&     # 1.6.2 is not currently supported by the version of matplotlib we are using.
    # See other comments about why matplotlib is pinned.
    pip install geoviews==1.6.1 &&     pip install hypertools &&     pip install py_stringsimjoin &&     pip install nibabel &&     pip install mlens &&     pip install scikit-multilearn &&     pip install cleverhans &&     pip install leven &&     pip install catboost &&     # fastFM doesn't support Python 3.7 yet: https://github.com/ibayer/fastFM/issues/151
    # pip install fastFM && \
    pip install lightfm &&     pip install folium &&     pip install scikit-plot &&     # dipy requires the optional fury dependency for visualizations.
    pip install fury dipy &&     # plotnine 0.5 is depending on matplotlib >= 3.0 which is not compatible with basemap.
    # once basemap support matplotlib, we can unpin this package.
    pip install plotnine==0.4.0 &&     pip install scikit-surprise &&     pip install pymongo &&     pip install geoplot &&     pip install eli5 &&     pip install implicit &&     pip install dask-ml[xgboost] &&     /tmp/clean-layer.sh
 
RUN pip install kmeans-smote --no-dependencies &&     # Add google PAIR-code Facets
    cd /opt/ && git clone https://github.com/PAIR-code/facets && cd facets/ && jupyter nbextension install facets-dist/ --user &&     export PYTHONPATH=$PYTHONPATH:/opt/facets/facets_overview/python/ &&     pip install tensorpack &&     pip install pycountry && pip install iso3166 &&     pip install pydash &&     pip install kmodes --no-dependencies &&     pip install librosa &&     pip install polyglot &&     pip install mmh3 &&     pip install fbpca &&     pip install sentencepiece &&     pip install cufflinks &&     pip install lime &&     pip install memory_profiler &&     /tmp/clean-layer.sh
 
# install cython & cysignals before pyfasttext
RUN pip install --upgrade cython &&     pip install --upgrade cysignals &&     pip install pyfasttext &&     # ktext has an explicit dependency on Keras 2.2.4 which is not
    # compatible with TensorFlow 2.0 (support was added in Keras 2.3.0).
    # Add the package back once it is fixed upstream.
    # pip install ktext && \
    pip install fasttext &&     apt-get install -y libhunspell-dev && pip install hunspell &&     pip install annoy &&     # Need to use CountEncoder from category_encoders before it's officially released
    pip install git+https://github.com/scikit-learn-contrib/categorical-encoding.git &&     pip install google-cloud-automl &&     # Newer version crashes (latest = 1.14.0) when running tensorflow.
    # python -c "from google.cloud import bigquery; import tensorflow". This flow is common because bigquery is imported in kaggle_gcp.py
    # which is loaded at startup.
    pip install google-cloud-bigquery==1.12.1 &&     pip install google-cloud-storage &&     pip install ortools &&     pip install scattertext &&     # Pandas data reader
    pip install pandas-datareader &&     pip install wordsegment &&     pip install pyahocorasick &&     pip install wordbatch &&     pip install emoji &&     # Add Japanese morphological analysis engine
    pip install janome &&     pip install wfdb &&     pip install vecstack &&     # Doesn't support Python 3.7 yet. Last release on pypi is from 2017.
    # Add back once this PR is released: https://github.com/scikit-learn-contrib/lightning/pull/133
    # pip install sklearn-contrib-lightning && \
    # yellowbrick machine learning visualization library
    pip install yellowbrick &&     pip install mlcrate &&     /tmp/clean-layer.sh
 
RUN pip install bcolz &&     pip install bleach &&     pip install certifi &&     pip install cycler &&     pip install decorator &&     pip install entrypoints &&     pip install html5lib &&     # Latest version breaks nbconvert: https://github.com/ipython/ipykernel/issues/422
    pip install ipykernel==5.1.1 &&     pip install ipython &&     pip install ipython-genutils &&     pip install ipywidgets &&     pip install isoweek &&     pip install jedi &&     pip install Jinja2 &&     pip install jsonschema &&     pip install jupyter &&     pip install jupyter-client &&     pip install jupyter-console &&     pip install jupyter-core &&     pip install MarkupSafe &&     pip install mistune &&     pip install nbconvert &&     pip install nbformat &&     pip install notebook==5.5.0 &&     pip install olefile &&     pip install opencv-python &&     pip install pandas_summary &&     pip install pandocfilters &&     pip install pexpect &&     pip install pickleshare &&     pip install Pillow &&     # Install openslide and its python binding
    apt-get install -y openslide-tools &&     # b/152402322 install latest from pip once is in: https://github.com/openslide/openslide-python/pull/76
    pip install git+git://github.com/rosbo/openslide-python.git@fix-setup &&     pip install ptyprocess &&     pip install Pygments &&     pip install pyparsing &&     pip install pytz &&     pip install PyYAML &&     pip install pyzmq &&     pip install qtconsole &&     pip install six &&     pip install terminado &&     # Latest version (6.0) of tornado breaks Jupyter notebook:
    # https://github.com/jupyter/notebook/issues/4439
    pip install tornado==5.0.2 &&     pip install tqdm &&     pip install traitlets &&     pip install wcwidth &&     pip install webencodings &&     pip install widgetsnbextension &&     pip install pyarrow &&     pip install feather-format &&     pip install fastai &&     pip install torchtext &&     pip install allennlp &&     # b/149359379 remove once allennlp 1.0 is released which won't cause a spacy downgrade.
    pip install spacy==2.2.3 && python -m spacy download en && python -m spacy download en_core_web_lg &&     apt-get install -y ffmpeg && \ 
    tmp(/clean-layer.sh)
 
    ###########
    #
    #      NEW CONTRIBUTORS:
    # Please add new pip/apt installs in this block. Don't forget a "&& \" at the end
    # of all non-final lines. Thanks!
    #
    ###########
 
RUN pip install flashtext &&     pip install wandb &&     pip install marisa-trie &&     pip install pyemd &&     pip install pyupset &&     pip install pympler &&     pip install s3fs &&     pip install featuretools &&     pip install -e git+https://github.com/SohierDane/BigQuery_Helper#egg=bq_helper && \
    pip install hpsklearn &&     pip install git+https://github.com/Kaggle/learntools &&     pip install kmapper &&     pip install shap &&     pip install ray &&     pip install gym &&     pip install tensorforce &&     pip install pyarabic &&     pip install conx &&     pip install pandasql &&     pip install tensorflow_hub &&     pip install jieba  &&     pip install git+https://github.com/SauceCat/PDPbox &&     pip install ggplot &&     pip install cesium &&     pip install rgf_python &&     # b/145404107: latest version force specific version of numpy and torch.
    pip install pytext-nlp==0.1.2 &&     pip install tsfresh &&     pip install pykalman &&     pip install optuna &&     pip install chainercv &&     pip install chainer-chemistry &&     pip install plotly_express &&     pip install albumentations &&     pip install catalyst &&     # b/145133331: latest version is causing issue with gcloud.
    pip install rtree==0.8.3 &&     # b/145133331 osmnx 0.11 requires rtree >= 0.9 which is causing issue with gcloud.
    pip install osmnx==0.10 &&     apt-get -y install libspatialindex-dev &&     pip install pytorch-ignite &&     pip install qgrid &&     pip install bqplot &&     pip install earthengine-api &&     pip install transformers &&     pip install dlib &&     pip install kaggle-environments &&     # b/149905611 The geopandas tests are broken with the version 0.7.0
    pip install geopandas==0.6.3 &&     pip install nnabla &&     pip install vowpalwabbit &&     /tmp/clean-layer.sh
 
# Tesseract and some associated utility packages
RUN apt-get install tesseract-ocr -y &&     pip install pytesseract &&     pip install wand==0.5.3 &&     pip install pdf2image &&     pip install PyPDF &&     pip install pyocr &&     /tmp/clean-layer.sh
ENV TESSERACT_PATH=/usr/bin/tesseract
 
# For Facets
ENV PYTHONPATH=$PYTHONPATH:/opt/facets/facets_overview/python/
# For Theano with MKL
ENV MKL_THREADING_LAYER=GNU
 
# Temporary fixes and patches
    # Temporary patch for Dask getting downgraded, which breaks Keras
RUN pip install --upgrade dask &&     # Stop jupyter nbconvert trying to rewrite its folder hierarchy
    mkdir -p /root/.jupyter && touch /root/.jupyter/jupyter_nbconvert_config.py && touch /root/.jupyter/migrated &&     mkdir -p /.jupyter && touch /.jupyter/jupyter_nbconvert_config.py && touch /.jupyter/migrated &&     # Stop Matplotlib printing junk to the console on first load
    sed -i "s/^.*Matplotlib is building the font cache using fc-list.*$/# Warning removed by Kaggle/g" /opt/conda/lib/python3.7/site-packages/matplotlib/font_manager.py &&     # Make matplotlib output in Jupyter notebooks display correctly
    mkdir -p /etc/ipython/ && echo "c = get_config(); c.IPKernelApp.matplotlib = 'inline'" > /etc/ipython/ipython_config.py &&     # Temporary patch for broken libpixman 0.38 in conda-forge, symlink to system libpixman 0.34 untile conda package gets updated to 0.38.5 or higher.
    ln -sf /usr/lib/x86_64-linux-gnu/libpixman-1.so.0.34.0 /opt/conda/lib/libpixman-1.so.0.38.0 &&     /tmp/clean-layer.sh
 
# gcloud SDK https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu
RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main"     | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg |     apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - &&     apt-get update -y && apt-get install google-cloud-sdk -y &&     /tmp/clean-layer.sh
 
# Add BigQuery client proxy settings
ENV PYTHONUSERBASE "/root/.local"
ADD patches/kaggle_gcp.py /root/.local/lib/python3.7/site-packages/kaggle_gcp.py
ADD patches/kaggle_secrets.py /root/.local/lib/python3.7/site-packages/kaggle_secrets.py
ADD patches/kaggle_web_client.py /root/.local/lib/python3.7/site-packages/kaggle_web_client.py
ADD patches/kaggle_datasets.py /root/.local/lib/python3.7/site-packages/kaggle_datasets.py
ADD patches/log.py /root/.local/lib/python3.7/site-packages/log.py
ADD patches/sitecustomize.py /root/.local/lib/python3.7/site-packages/sitecustomize.py
# Override default imagemagick policies
ADD patches/imagemagick-policy.xml /etc/ImageMagick-6/policy.xml
 
# TensorBoard Jupyter extension. Should be replaced with TensorBoard's provided magic once we have
# worker tunneling support in place.
# b/139212522 re-enable TensorBoard once solution for slowdown is implemented.
# ENV JUPYTER_CONFIG_DIR "/root/.jupyter/"
# RUN pip install jupyter_tensorboard && \
#     jupyter serverextension enable jupyter_tensorboard && \
#     jupyter tensorboard enable
# ADD patches/tensorboard/notebook.py /opt/conda/lib/python3.7/site-packages/tensorboard/notebook.py
 
# Set backend for matplotlib
ENV MPLBACKEND "agg"
 
# We need to redefine TENSORFLOW_VERSION here to get the default ARG value defined above the FROM instruction.
# See: https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact
ARG TENSORFLOW_VERSION
ARG GIT_COMMIT=unknown
ARG BUILD_DATE=unknown
 
LABEL git-commit=$GIT_COMMIT
LABEL build-date=$BUILD_DATE
LABEL tensorflow-version=$TENSORFLOW_VERSION
# Used in the Jenkins `Docker GPU Build` step to restrict the images being pruned.
LABEL kaggle-lang=python
 
# Correlate current release with the git hash inside the kernel editor by running `!cat /etc/git_commit`.
RUN echo "$GIT_COMMIT" > /etc/git_commit && echo "$BUILD_DATE" > /etc/build_date
 


# In[ ]:


#!/bin/bash
set -e
 
usage() {
cat << EOF
Usage: $0 [OPTIONS] [LABEL]
Push a newly-built image with the given LABEL to gcr.io and DockerHub.
 
Options:
    -g, --gpu                   Push the image with GPU support.
    -s, --source-image IMAGE    Tag for the source image. 
EOF
}
 
SOURCE_IMAGE_TAG='kaggle/python-build:latest'
SOURCE_IMAGE_TAG_OVERRIDE=''
TARGET_IMAGE='gcr.io/kaggle-images/python'
 
while :; do
    case "$1" in 
        -h|--help)
            usage
            exit
            (";")
        -g|--gpu)
            SOURCE_IMAGE_TAG='kaggle/python-gpu-build:latest'
            TARGET_IMAGE='gcr.io/kaggle-private-byod/python'
            (";")
        -s|--source-image)
            if [[ -z $2 ]]; then
                usage
                printf 'ERROR: No IMAGE specified after the %s flag.\n' "$1" >&2
                exit
            fi
            SOURCE_IMAGE_TAG_OVERRIDE=$2
            shift # skip the flag value
            (";")
        -?*)
            usage
            printf 'ERROR: Unknown option: %s\n' "$1" >&2
            exit
            (";")
        *)            
            break
    esac
 
    shift
done
 
LABEL=${1:-testing}
 
if [[ -n "$SOURCE_IMAGE_TAG_OVERRIDE" ]]; then
    SOURCE_IMAGE_TAG="$SOURCE_IMAGE_TAG_OVERRIDE"
fi
 
readonly SOURCE_IMAGE_TAG
readonly TARGET_IMAGE
readonly LABEL
 
set -x
docker tag "${SOURCE_IMAGE_TAG}" "${TARGET_IMAGE}:${LABEL}"
gcloud docker -- push "${TARGET_IMAGE}:${LABEL}"
 


# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.


# **Data input**

# In[ ]:


p1 = "/kaggle/input/covid19-global-forecasting-week-4/"
p2 = "/kaggle/input/world-bank-wdi-212-health-systems/"
p3 = "/kaggle/input/covid19inf/"
train = pd.read_csv(p1 + "train.csv")
test =  pd.read_csv(p1 + "test.csv")
submission =  pd.read_csv(p1 + "submission.csv")
health = pd.read_csv(p2 + "2.12_Health_systems.csv")
country = pd.read_csv(p3 + "covid19countryinfo.csv")
pollution = pd.read_csv(p3 + "region_pollution.csv")


# Some data munging in the contry dataset

# In[ ]:


country.drop(columns= country.columns[range(22,54)], inplace=True)
country["pop"] = country["pop"].str.replace(",", "").astype('float64')
country["quarantine"] = pd.to_datetime(country.quarantine)
country["schools"] = pd.to_datetime(country.schools)
country["restrictions"] = pd.to_datetime(country.restrictions)


# Data munging and merging

# In[ ]:


train["Date"] = pd.to_datetime(train.Date)
train["country_province"] = train["Province_State"]
train.country_province.fillna(train["Country_Region"], inplace=True)
test["Date"] = pd.to_datetime(test.Date)
test["country_province"] = test["Province_State"]
test.country_province.fillna(test["Country_Region"], inplace=True)
train = train.merge(country, how='left', left_on = ["country_province"], right_on = ["country"])
train = train.merge(pollution, how='left', left_on = ["country_province"], right_on = ["Region"])
train = train.merge(health, how='left', left_on = ["Country_Region", "Province_State"], right_on = ["Country_Region", "Province_State"])
test = test.merge(country, how='left', left_on = ["country_province"], right_on = ["country"])
test = test.merge(pollution, how='left', left_on = ["country_province"], right_on = ["Region"])
test = test.merge(health, how='left', left_on = ["Country_Region", "Province_State"], right_on = ["Country_Region", "Province_State"])
train["days"] = (train.Date - train.Date[0]).dt.days
test["days"] = (test.Date - train.Date[0]).dt.days

#The columns with region/state names in the different csvs are not longer needed
columns_to_drop = ["country", "Region", "Province_State", "Country_Region", "World_Bank_Name"]


# More data preparation, this time when it is in quarnatine, school stops and restriction. Those data are not complete and this is a pity but they are valuable.

# In[ ]:


train["in_quarantine"] = 0
train["in_schools"] = 0
train["in_restrictions"] = 0
test["in_quarantine"] = 0
test["in_schools"] = 0
test["in_restrictions"] = 0
for cp in train.country_province.unique():
    quarantine = country.loc[country.country == cp, "quarantine"]
    schools = country.loc[country.country == cp, "schools"]
    restrictions = country.loc[country.country == cp, "restrictions"]
    if (len(quarantine) > 0) and (quarantine.values[0] is not np.nan):
        date1 = pd.to_datetime(quarantine.values[0])
        train.loc[(train.country_province == cp) & (train.Date > date1), "in_quarantine"] = (train.Date - date1).dt.days
        test.loc[(test.country_province == cp) & (test.Date > date1), "in_quarantine"] = (test.Date - date1).dt.days
        
    if (len(schools) > 0) and (schools.values[0] is not np.nan):
        date1 = pd.to_datetime(schools.values[0])
        train.loc[(train.country_province == cp) & (train.Date > date1), "in_schools"] = (train.Date - date1).dt.days
        test.loc[(test.country_province == cp) & (test.Date > date1), "in_schools"] = (test.Date - date1).dt.days

    if (len(restrictions) > 0) and (restrictions.values[0] is not np.nan):
        date1 = pd.to_datetime(restrictions.values[0])
        train.loc[(train.country_province == cp) & (train.Date > date1), "in_restrictions"] = (train.Date - date1).dt.days
        test.loc[(test.country_province == cp) & (test.Date > date1), "in_restrictions"] = (test.Date - date1).dt.days

columns_to_drop += ["quarantine", "schools", "restrictions"]


# Label encode the countries and/or regions

# In[ ]:


from sklearn.preprocessing import LabelEncoder
lb = LabelEncoder()
train["country_province"] = lb.fit_transform(train.country_province)
test["country_province"] = lb.transform(test.country_province)


# Generate three new variables, days from first death, first reported case and 100th reported case. The virus arrives one place by travelling that is not easy to track for the model.

# In[ ]:


train["days_from_first_case"] = 0
test["days_from_first_case"] = 0
train["days_from_first_death"] = 0
train["days_from_case_100"] = 0
test["days_from_case_100"] = 0
test["days_from_first_death"] = 0

dates = list(train.Date.unique())
for province in train.country_province.unique():
    #print(province)
    mask1 = train.country_province == province
    mask2 = train.ConfirmedCases > 1.0
    mask3 = train.ConfirmedCases > 100.0
    mask4 = train.Fatalities > 1.0
    try:
        idx1 = train.loc[mask1 & mask2 ,["ConfirmedCases"]].idxmin()[0]
        dateidx1 = train.iloc[idx1]["Date"]
    except:
        dateidx1 = test.Date.max()
        pass
    #print(dateidx1)
    train.loc[mask1 & (train.Date >= dateidx1), "days_from_first_case"] = (train.Date - dateidx1).dt.days
    test.loc[mask1 & (test.Date >= dateidx1), "days_from_first_case"] = (test.Date - dateidx1).dt.days
    
    try:
        idx1 = train.loc[mask1 & mask3 ,["ConfirmedCases"]].idxmin()[0]
        dateidx1 = train.iloc[idx1]["Date"]
    except:
        dateidx1 = test.Date.max()
        pass
    train.loc[mask1 & (train.Date >= dateidx1), "days_from_case_100"] = (train.Date - dateidx1).dt.days
    test.loc[mask1 & (test.Date >= dateidx1), "days_from_case_100"] = (test.Date - dateidx1).dt.days    

        
    try:
        idx1 = train.loc[mask1 & mask4 ,["Fatalities"]].idxmin()[0]
        dateidx1 = train.iloc[idx1]["Date"]
    except:
        dateidx1 = test.Date.max()
        pass
    train.loc[mask1 & (train.Date >= dateidx1), "days_from_first_death"] = (train.Date - dateidx1).dt.days
    test.loc[mask1 & (test.Date >= dateidx1), "days_from_first_death"] = (test.Date - dateidx1).dt.days    

train.fillna(value = 0, inplace = True)
test.fillna(value = 0, inplace = True)  


# Definition of lagged variables

# In[ ]:


#Construction of laged variables 
lag_number = 3
for lag in range(1, lag_number + 1):
    var_name = "cases_lag%d" % lag
    train[var_name] = train.ConfirmedCases.shift(periods = lag)
    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0
    var_name = "fatalities_lag%d" % lag
    train[var_name] = train.Fatalities.shift(periods = 1)
    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0


# Finally the train and test data can be prepared for the models eliminating the object variables and repited ones. From train data a small sample is taken for validation, the days that coincide in train and test data.

# In[ ]:


#Days that coincide in train and test
print(train.loc[train.Date.isin(test.Date.unique()), "Date"].unique())
# The smallest of those days will be the separation between train and validation
sep_date = train.loc[train.Date.isin(test.Date.unique()), "Date"].unique().min()

result_columns = ["ConfirmedCases", "Fatalities"]
X_train = train.loc[(train.Date<sep_date),].drop(columns = columns_to_drop + ["Id", "Date"] + result_columns)
y_train_cases = train.loc[(train.Date<sep_date),].ConfirmedCases
y_train_fatalities = train.loc[(train.Date<sep_date),].Fatalities

X_val = train.loc[(train.Date>=sep_date),].drop(columns = columns_to_drop + ["Id", "Date"] + result_columns)
y_val_cases = train.loc[(train.Date>=sep_date),].ConfirmedCases
y_val_fatalities = train.loc[(train.Date>=sep_date),].Fatalities

X_test = test.drop(columns = columns_to_drop + ["ForecastId", "Date"])


# Before starting the models a function is defined to reduce code to check errors

# In[ ]:


from sklearn.metrics import mean_squared_error
def validate_models(model_cases, model_fatalities):
    predict_train_cases = model_cases.predict(X_train)
    predict_val_cases = model_cases.predict(X_val)
    print("RMSE in train detected cases: ", np.sqrt(mean_squared_error(y_train_cases, predict_train_cases)))
    print("RMSE in validation detected cases: ", np.sqrt(mean_squared_error(y_val_cases, predict_val_cases)))
    predict_train_fatalities = model_fatalities.predict(X_train)
    predict_val_fatalities = model_fatalities.predict(X_val)
    print("RMSE in train fatalities: ", np.sqrt(mean_squared_error(y_train_fatalities, predict_train_fatalities)))
    print("RMSE in validation fatalities: ", np.sqrt(mean_squared_error(y_val_fatalities, predict_val_fatalities)))


# Now the models, let's start with a Random Forest and after a lightgbm

# In[ ]:


from sklearn.linear_model import LinearRegression
lm_cases = LinearRegression()
lm_cases.fit(X_train, y_train_cases)

lm_fatalities = LinearRegression()
lm_fatalities.fit(X_train, y_train_fatalities)

validate_models(lm_cases, lm_fatalities)


# In[ ]:


#RandomForest
from sklearn.ensemble import RandomForestRegressor
rf_cases = RandomForestRegressor(n_estimators= 400, max_depth=6, random_state=0, verbose=0, n_jobs=-1)
rf_cases.fit(X_train, y_train_cases)

rf_fatalities = RandomForestRegressor(n_estimators= 400, max_depth=6, random_state=0, verbose=0, n_jobs=-1)
rf_fatalities.fit(X_train, y_train_fatalities)

validate_models(rf_cases, rf_fatalities)


# In[ ]:


import lightgbm as lgb
lgb_params = {
               'feature_fraction': 0.8,
               'metric': 'rmse',
               'nthread':-1, 
               'min_data_in_leaf': 2**4,
               'bagging_fraction': 0.75, 
               'learning_rate': 0.5, 
               'objective': 'mse', 
               'bagging_seed': 2**5, 
               'num_leaves': 2**6,
               'bagging_freq':1,
               'verbose':1 
              }
lgbm_cases = lgb.train(lgb_params, 
                       train_set=lgb.Dataset(X_train, label=y_train_cases), 
                       valid_sets=lgb.Dataset(X_val, label=y_val_cases), 
                       num_boost_round=500)
lgbm_fatalities = lgb.train(lgb_params, 
                            train_set=lgb.Dataset(X_train, label=y_train_fatalities), 
                            valid_sets=lgb.Dataset(X_val, label=y_val_fatalities), 
                            num_boost_round=500)
validate_models(lgbm_cases, lgbm_fatalities)


# Both model worked well in train set but much worse in validation, seems overfitting. Let's see the features importances

# In[ ]:


lgb.plot_importance(lgbm_fatalities)
plt.rcParams['figure.figsize'] = [10, 10]
plt.show()


# Finally the calculation for the test set requires a rolling forecast, going one by one to calculate the lags with the previous values

# In[ ]:


lags = {}
predict_test_cases, predict_test_fatalities = [] , []
test_min_day = test.days.min()
for i in range(1, lag_number + 1):
    lags["caseslag%d" % i] = 0
    lags["fatalitieslag%d" % i] = 0
    
for ind in tqdm_notebook(range(len(X_test))):
    #print("case: {} of {}".format(ind, len(X_test)))
    #First lag data are obtained either from previous calculations or train data
    if X_test.iloc[ind].days == test_min_day:
        #print(test_min_day)
        for i in range(1, lag_number + 1):
            mask1 = train.days == (test_min_day - i)
            mask2 = train.country_province == X_test.iloc[ind].country_province
            lags["caseslag%d" % i] = train.loc[mask1 & mask2, "ConfirmedCases"].values[0]
            lags["fatalitieslag%d" % i] = train.loc[mask1 & mask2, "Fatalities"].values[0]
    else:
        lags["caseslag1"] = pred_cases
        lags["fatalitieslag1"] = pred_fatalities
        for i in range(2, lag_number + 1):
            lags["caseslag%d" % i] = lags["caseslag%d" % (i-1)]
            lags["fatalitieslag%d" % i] = lags["fatalitieslag%d" % (i-1)]
    x_test = X_test.iloc[ind].copy()
    x_test =pd.DataFrame(x_test).transpose()
    for i in range(1, lag_number + 1):
        x_test["cases_lag%d" % i] = lags["caseslag%d" % i]
        x_test["fatalities_lag%d" % i] = lags["fatalitieslag%d" % i]
        
    pred_cases = rf_cases.predict(x_test)[0]
    pred_fatalities = rf_fatalities.predict(x_test)[0]
    predict_test_cases.append(pred_cases)
    predict_test_fatalities.append(pred_fatalities)
    


# submission

# In[ ]:


submission.ConfirmedCases = predict_test_cases
submission.Fatalities = predict_test_fatalities
submission.to_csv("/kaggle/working/submission.csv", index = False)

