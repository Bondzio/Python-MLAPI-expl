#!/usr/bin/env python
# coding: utf-8

# # Index of my kaggle notebooks
# 
# ## 1. exploratory data analysis
# * [Anscombe's quartet and the importance of EDA](https://www.kaggle.com/carlmcbrideellis/anscombe-s-quartet-and-the-importance-of-eda) (+ [dataset](https://www.kaggle.com/carlmcbrideellis/data-anscombes-quartet))
# * [Exploratory data analysis using pandas pivot table](https://www.kaggle.com/carlmcbrideellis/exploratory-data-analysis-using-pandas-pivot-table)
# * [House Prices: EDA in one line: 'pandas profiling'](https://www.kaggle.com/carlmcbrideellis/house-prices-eda-in-one-line-pandas-profiling)
# * [Titanic: view missing values with missingno](https://www.kaggle.com/carlmcbrideellis/titanic-view-missing-values-with-missingno)
# 
# ## 2. feature selection
# 
# * [Feature selection using BorutaShap](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-borutashap)
# * [Recursive Feature Elimination (RFE) example](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-rfe-example)
# * [House Prices: Permutation Importance example](https://www.kaggle.com/carlmcbrideellis/house-prices-permutation-importance-example)
# 
# ## 3. classification / regression
# 
# This is a collection of my python example scripts for either classification, using the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition data, or regression, for which I use the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition data:
# 
# | algorithm | classification | regression |
# | :--- | --- | --- |
# | Logistic regression | [link](https://www.kaggle.com/carlmcbrideellis/logistic-regression-classifier-minimalist-script) | --- |
# | Support vector machines (SVM) | [link](https://www.kaggle.com/carlmcbrideellis/support-vector-classifier-minimalist-script) | --- |
# | Decision tree| [link](https://www.kaggle.com/carlmcbrideellis/titanic-some-sex-a-bit-of-class-and-a-tree) | --- |
# | Random forest | [link](https://www.kaggle.com/carlmcbrideellis/random-forest-classifier-minimalist-script) | [link](https://www.kaggle.com/carlmcbrideellis/random-forest-regression-minimalist-script) |
# | Regularized Greedy Forest (RGF) | [link](https://www.kaggle.com/carlmcbrideellis/introduction-to-the-regularized-greedy-forest) | [link](https://www.kaggle.com/carlmcbrideellis/introduction-to-the-regularized-greedy-forest) |
# | Gradient boosting: XGBoost | [link](https://www.kaggle.com/carlmcbrideellis/xgboost-classification-minimalist-python-script) | [link](https://www.kaggle.com/carlmcbrideellis/very-simple-xgboost-regression)|
# | Gradient boosting: CatBoost | --- | [link](https://www.kaggle.com/carlmcbrideellis/catboost-regression-minimalist-script)|
# | Neural networks (using keras) | [link](https://www.kaggle.com/carlmcbrideellis/very-simple-neural-network-for-classification) | [link](https://www.kaggle.com/carlmcbrideellis/very-simple-neural-network-regression) |
# | Gaussian process | [link](https://www.kaggle.com/carlmcbrideellis/gaussian-process-classification-sample-script) | [link](https://www.kaggle.com/carlmcbrideellis/gaussian-process-regression-sample-script) |
# | Hyperparameter grid search | [link](https://www.kaggle.com/carlmcbrideellis/hyperparameter-grid-search-sample-code) | [link](https://www.kaggle.com/carlmcbrideellis/hyperparameter-grid-search-simple-example) |
# 
# 
# ## 4. ensemble methods
# 
# * [Ensemble methods: majority voting example](https://www.kaggle.com/carlmcbrideellis/ensemble-methods-majority-voting-example)
# 
# 
# ## 5. didactic notebooks
# * [Beautiful math in your notebook](https://www.kaggle.com/carlmcbrideellis/beautiful-math-in-your-notebook): a guide to using $\LaTeX$ math markup in kaggle notebooks.
# * [Titanic: In all the confusion...](https://www.kaggle.com/carlmcbrideellis/titanic-in-all-the-confusion) which looks at the confusion matrix, ROC curves, $F_1$ scores etc.
# * [False positives, false negatives and the discrimination threshold](https://www.kaggle.com/carlmcbrideellis/discrimination-threshold-false-positive-negative)
# * [treeplot: See your classification tree!](https://www.kaggle.com/carlmcbrideellis/treeplot-see-your-classification-tree)
# * [Data anonymization using Faker (Titanic example)](https://www.kaggle.com/carlmcbrideellis/data-anonymization-using-faker-titanic-example)
# * [Introduction to the Regularized Greedy Forest](https://www.kaggle.com/carlmcbrideellis/introduction-to-the-regularized-greedy-forest) (using [rgf_python](https://github.com/RGF-team/rgf/tree/master/python-package))
# 
# 
# ## 6. miscellaneous
# * [Titanic leaderboard: a score > 0.8 is great!](https://www.kaggle.com/carlmcbrideellis/titanic-leaderboard-a-score-0-8-is-great)
# * [House Prices: How to work offline](https://www.kaggle.com/carlmcbrideellis/house-prices-how-to-work-offline) (+ [dataset](https://www.kaggle.com/carlmcbrideellis/house-prices-how-to-work-offline))
# * [House Prices: my score using only 'OverallQual'](https://www.kaggle.com/carlmcbrideellis/house-prices-my-score-using-only-overallqual)
# * [Animated histogram of the central limit theorem](https://www.kaggle.com/carlmcbrideellis/animated-histogram-of-the-central-limit-theorem)
# 
# ## 7. fun with the [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset
# * [Notebooks: number of views per vote](https://www.kaggle.com/carlmcbrideellis/notebooks-number-of-views-per-vote)
# * [kaggle discussions: busiest time of the day?](https://www.kaggle.com/carlmcbrideellis/kaggle-discussions-busiest-time-of-the-day)
# * [The kaggle working week](https://www.kaggle.com/carlmcbrideellis/the-kaggle-working-week)
# * [WordCloud of gold medal winning notebook titles](https://www.kaggle.com/carlmcbrideellis/wordcloud-of-gold-medal-winning-notebook-titles)
