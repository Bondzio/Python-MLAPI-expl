#!/usr/bin/env python
# coding: utf-8

# # Notes on the CouchDB book
# * https://guide.couchdb.org/editions/1/en/index.html
# 
# ## Background
# * Apache CouchDB is a NoSQL database written in Erlang. The underlying semantics of the database is that every user has their own authoritative copy of the data, which is synchronized asynchronously between the nodes. This is an early example of what Martin Kleppman (the "Designing Data Intensive Applications" guy) calls a "local-first application" in his paper [Local-First Software: You Own Your Data, In Spite of the Cloud](https://martin.kleppmann.com/papers/local-first.pdf).
# * In fact this paper mentions the CouchDB definitive guide as a great case study on topics involved in distributed systems in general. Hearing this, and having a couple of loose hours on hand, I decided that I'd like to skim it.
# 
# ## Introduction
# * CouchDB is fundamentally designed with developer productivity in mind.
# * CouchDB is "schema-less" (of course what this actually means is: CouchDB is schema-on-read instead of schema-on-write).
# * One design tenet: "local data is king". This database architecture is designed to work with local data as the source of truth when possible. As per DDIA, this adds reliability, at the cost of developer complexity (synchronization).
# 
# ## Eventual Consistency
# * CouchDB is eventually consistent&mdash;meaning it picks AP from the CAP theorem.
# * CouchDB uses B-tree storage. Recall that this is the traditional storage organization choice. Contrast with append-only logs (very simple) and SSL tables (very new).
# * CouchDB is lockless. Specifically, it does not lock on write. Reads that co-process with a write will read the full old version of the document. Under the hood, there's an atomic compare-and-swap operation at play, I'm sure. This is *snapshot isolation*.
# * Queries are performed in the Map-Reduce style.
# * CouchDB embeds a validation language (taken from Map Reduce) into the database layer, which is meant to save CPU cycles performing validation after reads. Recall that this validation is only necessary because we are schema-on-read, though. :)
# 
# 
# * Multi-machine scale-out is achieved using incremental replication. Non-write servers are not gauranteed to have the latest copy of your data, e.g. there is no consistency guarantee.
# * CouchDB is shared-nothing, e.g. it has no permanent master nodes. This enables "throw another server at it" scaling.
# * CouchDB maintains version history. Indeed, this is required for its conflict resolution strategy.
# * Conflicting writes are handled using last write wins with history: when a non-mergeable conflict is detected, a consistent winning write is chosen, but the losing write is not destroyed, and instead gets pushed to an intervening object version.
# * Mergeable conflicts are handled automatically (CRDTs?), if doing so is possible.
# * **It is your responsibility as the developer to handle merge conflicts**.
# * This is where Klepmann's critism from his paper comes in. Yes
# 
# 
# ## Getting Started
# * CouchDB feels like the answer to the following question: "what if you used MapReduce as the front-end to a database"?
# * It is a document database, e.g. it allows arbitrarily deeply nested keys.
# * Instead of a SQL statement you have a Map and then a Reduce.
# 
# ## The Core API
# * Queries are HTTP-based (as I should hope so!).
# * Reading of this section reveals something that I had expected. Because CouchDB doesn't have any consistency guarantees, there is no read-your-own-write guarantee, e.g. you can create and modify a document and then fail to read it back. This will happen if you are switched to a different node which has not yet received the required asynchronous update, due to a network partition.
# * Recall the Amazon shopping cart example: allowing you to put something in the cart is more important than immediately representing the contents of your cart in an accurate way.
# 
# 
# * CouchDB uses a history system known as **Multi-Version Concurrency Control**. I am not immediately familiar with this terminology, so let's dig into what this means for CouchDB:
#   * CouchDB queries read and write entire documents at once (e.g. no partial read/write).
#   * There is a `_rev` field, which represents a revision number. This number is generated by CouchDB, is sequential in nature, and is expected to be provided in all write requests.
#   * If you submit a write with a stale revision number, this indicates old date. The system will return an error telling you to fetch latest again.
#   * CouchDB in this way prevents *lost updates* (writes based on stale data). It does so at the cost of developer complexity (you must now pull the document again, and implement merge-and-retry logic for a resubmit). Full transactionality with locking would provide a stronger consistency guarantee, and you would never need to implement retry logic, but of course this will slow the database down when under load. NoSQL! :)
# 
# 
# * In sticking to the HTTP abstraction, CouchDB makes the interesting step of allowing for attachments. I suppose that expands the scope of objects that CouchDB can work with (beyond just built-in types). Since these are treated as blobs, they have no version clashing control&mdash;they are last write wins (LWW) only.
# 
# 
# * Replications can be triggered manually.
# 
# 
# ## Designing Documents
# * Interestingly enough CouchDB creators think that there are basically two kinds of documents:
#   * Denormalized documents. E.g. everything that you need to display a Twitter user's profile. Getting away from performing joins here as much as possible is hugely benefitial.
#   * Event logs. E.g. append-only logs. This is good for creating and managing asynchronous events. This approach requires minimal input validation (faster write), but requires relational-style job work later on (slower read). A background task can compact the logs to keep reads fast.
# * CouchDB is supposedly fast for this log style, but a question I immediately have is this one. Logs are fast if you write entries in them in an append-only way, but CouchDB requires transacting whole documents at a time. That requires a lot of unnecessary I/O overhead.
# 
# 
# * Internal applications like views and validators are implemented using "design documents". What this gets right: everything is data. What this gets wrong: holy crap, you're writing JavaScript code as JSON strings?!? Ew!
# 
# 
# ## Finding Your Data with Views
# * Note: this is the last section of the first part I will take notes on before skipping ahead to the scale-out section of the documentation.
# * The map step operates on *all of the documents that are in scope* (wow! where scope is the start key and end key, btw) and emits zero-to-many keys (key-value pairs) per document. As in file-based map-reduce, keys are sorted prior to the reduce step.
# * Running a map job has equivalence to constructing a *view*, in the relational database sense. Views are managed using B-trees and writeable to disk (B-trees are efficient for off-disk queries, but are they actually a good in-memory format...?). Views are materialized to disk. Every query for the same view in the future need only update for values which have changed. This has the following performance characteristics for queries:
#   * The first time you run a view is very slow.
#   * No work is performed on inserts and updates. That work is deferred until view (re)run time, at which point an incremental recompute is performed. The cost of that recompute is variable with the amount of updating which occurred.
#   * But, assuming no changes to do the data, a view request is an `O(1)` operation.
# * The map function is structured how you'd expect it: it takes a document as input, and emits one or more simple values as output.
# * The reducer is more complex. It operates on data batched on key, but unlike e.g. `pandas` `groupby` reducers it is hierarchical. There is a `rereduce` flag on the function which is set to `False` when running on the root data. The map is applied to that data, and the result is collected into another B-tree. Now the reducer is run on *this* B-tree again, this time with the `rereduce` flag set to `True`&mdash;the reducer is now processing its own prior output!
# * If the B-tree has more than two layers, the rereducer is run multiple times.
# 
# 
# ## Replication
# * CouchDB maintains sequence numbers for the purposes of replication. The sequence number is a tracker of state equivalence, less failures. **Replication failures, when they occur, are simply logged**. A replication is not considered complete until the keys that still need to be replicated are carried over. All of this also means, by the way, that partial replication is possible, and that you can have a database in an indeterminate intermediate state. So replications are not atomic. Fun times!
# * Replication is triggerable, but can also be set to continuous (not truly continuous; an internal algorithm determines the batch times).
# 
# 
# ## Conflict management
# * What does CouchDB do about split brain (the scenario where two nodes independently act as primary)? This is traditionally an intensely dangerous scenario because it creates database differences which are intrinsically very hard to merge.
# * Most databases use distributed consensus on a majority quorum of nodes to avoid this scenario.
# * CouchDB does not. CouchDB is very focused on development off of a local database, with a remote database that may be deeply conflicted with your local. This means that you are *already* thinking very hard about your conflict resolution strategy for every possible data conflict.
# * Key histories have a *revision id*, which is a deterministic MD5 hash. Conflicts are only declared when they represent different revision IDs. If the same write occurred in two places, that piece of data will not be in conflict at sync time because the content is the same, so the revision ID is the same. We did something similar at Quilt.
# 
# 
# ## Clustering
# * There's a daemon you run that handle sending requests to the correct shards and aggregating them.
# * The partitioning is done using hash partitions (against the first few bytes of the deterministic MD5 revision ID, presumably). For planning for future cluster growth, it's recommended.
# * No mention of what to do about hot keys or whatever.
