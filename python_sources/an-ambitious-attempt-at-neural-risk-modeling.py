#!/usr/bin/env python
# coding: utf-8

# TL;DR
# I devised a way to take better advantage of the temporal nature of the extra data tables.
# This approach uses neural layers to embed each sequence into a flattened feature representation, culminating in a learned feature representation that's used in the final classification layer.
# My team is releasing our code, which hints at a method for distributed training of a large, LSTM based model.  Pointers to using Temporal Convolutions are in the code as well.

# # GitHub repo
# https://github.com/jvmancuso/default-risk

# # Intro Proposal
# https://docs.google.com/document/d/1BD6ZXB6OSEE-FoJjtsitb5cR36fBKpO5K11O08kOHwU/edit?usp=sharing

# In[ ]:




