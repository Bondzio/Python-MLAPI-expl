#!/usr/bin/env python
# coding: utf-8

# **Speech Emotion Recognition (SER)**
# 
# Speech Emotion Recognition (SER) is one of the most challenging tasks in speech signal analysis domain, it is a research area problem which tries to infer the emotion from the speech signals. The importance of emotion recognition is getting popular with improving user experience and the engagement of Voice User Interfaces (VUIs). For example, customer services, recommender systems, and healthcare applications.
# 
# **Objective:** 
# 
# In this mini project,Building and training simple Speech Emotion Recognizer that predicts human emotions from audio files using Python, Sci-kit learn, librosa, and Keras. firstly, we will load the data (Ravdess dataset), extract features (MFCC) from it, and split it into training and testing sets. Then, we will initialize two models (MLP and LSTM) as emotion classifiers and train them. Finally, we will calculate the accuracy of our models.

# **The whole pipeline is as follows (as same as any machine learning pipeline):**
# 
# * (1)Loading the Dataset: This process is about loading the dataset in Python which involves extracting audio features, such as MFCC
# * (2)Training the Model: After we prepare and load the dataset, we simply train it on a suited model.
# * (3)Testing the Model: Measuring how good our model is doing.

# **Import Python Packages:**

# In[ ]:


import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os # to use operating system dependent functionality
import librosa # to extract speech features
import wave # read and write WAV files
import matplotlib.pyplot as plt # to generate the visualizations

# MLP Classifier
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# LSTM Classifier
import keras
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import *
from keras.optimizers import rmsprop


# **(1) Loading the Dataset**
# 
# We will use Speech audio-only files (16bit, 48kHz .wav) from the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset. This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. **for More details:** (https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio)

# * **What is Mel-frequency Cepstrum Coefficients (MFCC)?** 
# The sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. 
# ![Mel-frequency Cepstrum Coefficients](https://miro.medium.com/max/3430/1*pzE4i1TXaLCmzTXgdxFZjQ.jpeg)

# In[ ]:


def extract_mfcc(wav_file_name):
    #This function extracts mfcc features and obtain the mean of each dimension
    #Input : path_to_wav_file
    #Output: mfcc_features'''
    y, sr = librosa.load(wav_file_name)
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)
    
    return mfccs


# In[ ]:


##### load radvess speech data #####
radvess_speech_labels = [] # to save extracted label/file
ravdess_speech_data = [] # to save extracted features/file
for dirname, _, filenames in os.walk('/kaggle/input/ravdess-emotional-speech-audio/'):
    for filename in filenames:
        #print(os.path.join(dirname, filename))
        radvess_speech_labels.append(int(filename[7:8]) - 1) # the index 7 and 8 of the file name represent the emotion label
        wav_file_name = os.path.join(dirname, filename)
        ravdess_speech_data.append(extract_mfcc(wav_file_name)) # extract MFCC features/file
        
print("Finish Loading the Dataset")


# **Preparing the dataset for machine learning models:**

# In[ ]:


#### convert data and label to array
ravdess_speech_data_array = np.asarray(ravdess_speech_data) # convert the input to an array
ravdess_speech_label_array = np.array(radvess_speech_labels)
ravdess_speech_label_array.shape # get tuple of array dimensions

#### make categorical labels
labels_categorical = to_categorical(ravdess_speech_label_array) # converts a class vector (integers) to binary class matrix
ravdess_speech_data_array.shape
labels_categorical.shape


# *  **Example 1: MLP Classifier**

# *  Split the dataset into training and testing sets: (training set) a subset to train a model, (test set) a subset to test the trained model.
# *  **Note:**(train_test_split) function will split arrays or matrices into random train and test subsets, in our example, training set 80% and testing set 20% 

# In[ ]:


x_train,x_test,y_train,y_test= train_test_split(np.array(ravdess_speech_data_array),labels_categorical, test_size=0.20, random_state=9)


# * **What is A multilayer perceptron (MLP)?**
# * A multilayer perceptron (MLP) is a deep, artificial neural network. It is composed of more than one perceptron. They are composed of an input layer to receive the signal, an output layer that makes a decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP. **More Details:** (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)

# * Define MLP and adjust its **hyperparameter** (a parameter whose value is set before the learning process begins.)

# In[ ]:


# Initialize the Multi Layer Perceptron Classifier
model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)


# * ** Trains the model for a fixed number of epochs (iterations on a dataset).**

# In[ ]:


# Train the model
model.fit(x_train,y_train)


# *  **Generates output predictions for the input samples.**

# In[ ]:


# Predict for the test set
y_pred=model.predict(x_test)


# *  **(accuracy)**: is a function that is used to judge the performance of your model.

# In[ ]:


# Calculate the accuracy of our model
accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)
# Print the accuracy
print("Accuracy: {:.2f}%".format(accuracy*100))


# We have obtained **40.10% accuracy** by using MLP as classifier, but this is can be improved by adjusting the training hyperparameter, increasing the dataset, or trying other machine learning models. 

# **Example 2: LSTM Classifier**
# 
# In this example, we will use another model to improve the accuracy which is LSTM model.

# * **What is Long short-term memory (LSTM)?**
# 
# Recurrent neural networks, of which LSTMs are the most powerful and well known subset, are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting and the spoken word. **More Details:** (https://keras.io/layers/recurrent/#lstm) 

# In[ ]:


# Split the training, validating, and testing sets
number_of_samples = ravdess_speech_data_array.shape[0]
training_samples = int(number_of_samples * 0.8)
validation_samples = int(number_of_samples * 0.1)
test_samples = int(number_of_samples * 0.1)


# In[ ]:


# Define the LSTM model
def create_model_LSTM():
    model = Sequential()
    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))
    model.add(Dense(64))
    model.add(Dropout(0.4))
    model.add(Activation('relu'))
    model.add(Dense(32))
    model.add(Dropout(0.4))
    model.add(Activation('relu'))
    model.add(Dense(8))
    model.add(Activation('softmax'))
    
    # Configures the model for training
    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
    return model


# In[ ]:


### train using LSTM model
model_A = create_model_LSTM()
history = model_A.fit(np.expand_dims(ravdess_speech_data_array[:training_samples],-1), labels_categorical[:training_samples], validation_data=(np.expand_dims(ravdess_speech_data_array[training_samples:training_samples+validation_samples], -1), labels_categorical[training_samples:training_samples+validation_samples]), epochs=100, shuffle=True)


# In[ ]:


### loss plots using LSTM model
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'ro', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()


# In[ ]:


### accuracy plots using LSTM model
plt.clf()                                                

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

plt.plot(epochs, acc, 'ro', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()


# *  **(evaluate function): Returns the loss value & metrics values for the model in test mode.**

# In[ ]:


### evaluate using model A
model_A.evaluate(np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], -1), labels_categorical[training_samples + validation_samples:])


# * **  Save the weights of the model as a HDF5 file **

# In[ ]:


model_A.save_weights("Model_LSTM.h5")

