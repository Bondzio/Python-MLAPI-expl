#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
get_ipython().run_line_magic('matplotlib', 'inline')


# This notebook is based off chapter 3 from Aurelien Geron's book: *Hands-On Machine Learning with Scikit-Learn & TensorFlow*. The notebook is my journey to the world of data science and may serve as a gentle introduction for beginner and not an extensive guide on how to get best LB score. 

# In[ ]:


mnist_train = pd.read_csv("/kaggle/input/digit-recognizer/train.csv")
mnist_test = pd.read_csv("/kaggle/input/digit-recognizer/test.csv")                          


# In[ ]:


print("Train: \n",mnist_train.shape)
print("Test: \n",mnist_test.shape)


# In[ ]:


mnist_train.head()


# # Exploratory Data Analysis
# 
# ## Check the frequency of labels

# In[ ]:


mnist_train.label.value_counts().plot.bar()


# ## Visualize a digit

# In[ ]:


some_digit = mnist_train.drop("label", axis = 1).iloc[40000]
some_digit_image = some_digit.values.reshape(28,28)

plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = "nearest")


# In[ ]:


# The image looks like a 2, let's verify if the label indeed returns a 2
print("Label: ",mnist_train.iloc[40000].label)


# # Training a Binary Classifier
# 
# Let's simplify the problem and try to only identify one digit - digit 5. This classifier will be able to distringuish two classes, 5 and not-5. 

# In[ ]:


from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(mnist_train.drop("label", axis = 1), mnist_train.label, test_size=0.2, random_state=42)
X_train = mnist_train.drop("label", axis = 1)
y_train = mnist_train.label

y_train_5  = (y_train == 5)
y_test_5 = (y_train == 5)


# In[ ]:


# Train using Stochastic Gradient Descent (SGD)
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state = 42)
sgd_clf.fit(X_train, y_train_5)


# In[ ]:


# Let's try this classifier on the previous digit example:

sgd_clf.predict([some_digit])


# In[ ]:


# The classifier correctly predict the digit 2 as not-5, but let's see if it can correctly predict a 5:
a_digit_5 = mnist_train[mnist_train['label'] == 5].drop('label', axis = 1).sample(1, random_state = 123)

sgd_clf.predict(a_digit_5)


# ## Performance Measure
# 
# Let's use cross_val_score() to evaluate the classifier using Kfold cross-validation:

# In[ ]:


from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring = "accuracy")


# We obtained 3 accuracy scores for each fold, let's take the average to see roughly how well the classifier does:

# In[ ]:


cv_scores.mean()


# A cool 95.6% accuracy! However, we need to consider problem of class imbalance, where negative class (not-5) is overwhelmingly more than the positive class (5). Therefore, even a dumb classifier that play safe and call everything as not-5, it will still have a very high accuracy. Let's see the dumb classifier in action:

# In[ ]:


from sklearn.base import BaseEstimator

class Never5Classifier(BaseEstimator):
    def fit(self, X, y=None):
        pass
    def predict(self, X):
        return np.zeros((len(X), 1), dtype = bool)
    
never_5_clf = Never5Classifier()
cross_val_score(never_5_clf, X_train, y_train_5, cv = 3, scoring = "accuracy").mean()


# A classifier that only predict not-5 can archive a 90% accuracy. This is simply because only 10% of data are 5s, so if you always guess an image is not a 5, you will be correct 90% of the time.

# ## A Better Performance Measure
# 
# A much better way to measure the performance of a classifier is to look at the confusion matrix.

# In[ ]:


from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3)


# Just like the cross_val_score(), cross_val_predict() performs K-fold cross_validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold.
# 
# Now let's construct a confusion matrix based on the target class and the predicted classes

# In[ ]:


from sklearn.metrics import confusion_matrix

confusion_matrix(y_train_5, y_train_pred)


# From this matrix, we can derive precision and recall score that better reflect the errors generated by the classfier:

# In[ ]:


from sklearn.metrics import precision_score, recall_score
print("precision_score: ", precision_score(y_train_5, y_train_pred))
print("recall_score: ", recall_score(y_train_5, y_train_pred))


# Now our classifier doesn't look so good as it did when we only look at accuracy. In other words, when the classifier claims an image represents a 5, it is correct only 76.9% of the time. And it only detects 75% of the 5s. 
# 
# It's often convinience to have just a single metric that combine these two scores together. In particular, this single metric is called F1 score, where it take on the *harmonic mean* of precision and recall. What is harmonic you ask? 
# 
# Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. Therefore, if one of presicion or recall tanked, the whole F1 score stay low.

# In[ ]:


from sklearn.metrics import f1_score

f1_score(y_train_5, y_train_pred)


# F1 score favors classfiders that have similar precision and recall. This might not be the optimal choice for the specific problem that you are trying to solve. In certains context, you will want to care more about precision or recall.

# ## The ROC Curve
# 
# The receiver operating characteristic (ROC) curve is another very common tool in binary classification problem. ROC curve plots the true positive rate (recall) against the false positive rate. This is also known as plots of sensitivity vs 1 - specificity. Let's plot our ROC curve:

# In[ ]:


y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3, method = 'decision_function')

from sklearn.metrics import roc_curve, roc_auc_score

fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)
roc_auc = roc_auc_score(y_train_5, y_scores)


# In[ ]:


def plot_roc_curve(fpr, tpr, label = None):
    plt.plot(fpr, tpr, label = label)
    plt.plot([0,1], [0,1], 'k--')
    plt.axis([0,1,0,1])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")

plot_roc_curve(fpr,tpr, label='ROC curve (area = {})'.format(roc_auc))


# One way to compare the classifiers is to measure the area under the ROC curve. A perfect classifier will have ROC AUC equal to 1, whereas a purely random (dumb?) classifier will have the AUC qual to 0.5 (area under the dashed line)
# 
# ## A better classifier
# 
# Now that we have the tool to compare one classifier to another, it's time to call in a more sophisicated classifier - Random Forest

# In[ ]:


from sklearn.ensemble import RandomForestClassifier

forest_clf = RandomForestClassifier(random_state = 42)

y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv = 3, method = 'predict_proba')

y_scores_forest = y_probas_forest[:,1]
fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)
roc_auc_forest = roc_auc_score(y_train_5, y_scores_forest)


# In[ ]:


plot_roc_curve(fpr,tpr, label='ROC curve (area = {})'.format(roc_auc))
plt.plot(fpr_forest, tpr_forest, "b:", label = "Random Forest (area = {})".format(roc_auc_forest))
plt.legend(loc = 'lower right')


# **Observation**: As we can see, the Random Forest classifier ROC curve looks much better than the SGD classifier. The random forest is clearly the superior classifier for our binary problem. We can confirm this with F1 score:

# In[ ]:


y_probas_forest_binary = cross_val_predict(forest_clf, X_train, y_train_5, cv = 3, method = 'predict')

print("Random Forest F1:", f1_score(y_train_5, y_probas_forest_binary))
print("SGD F1:", f1_score(y_train_5, y_train_pred))


# # Multiclass Classification
# 
# We want to expand the capability of our model with the ability of classify all the digits. Scikitlearn automatically know from the label that we are trying to to a multiclass classification:

# In[ ]:


forest_clf.fit(X_train, y_train)
forest_clf.predict(a_digit_5)


# Under the hood, our random forest classifier run all binary classifiers from 0-9 and select the class whose classifier outputs the highest score.

# In[ ]:


forest_clf.predict_proba(a_digit_5)


# We can see that digit 5 was predicted with the highest probability. Meaning the classifier is fairly confident about its prediction, with a 80% proabbility that the image is a 5, and it also think that there is a lower 20% chance that the image could be an 8.
# 
# Similar to a binary classifier, we can calculate the accuracy for our classifier:

# In[ ]:


cross_val_score(forest_clf, X_train, y_train, cv = 3, scoring = "accuracy").mean()


# ## Improve the model
# 
# ### Scaling the input

# In[ ]:


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))
cross_val_score(forest_clf, X_train_scaled, y_train, cv = 3, scoring = "accuracy").mean()


# ## Select a better algorithm
# 
# One of the main problem with ensemble or linear classifiers is that they heavily rely on the position and the rotation of the image. Therefore, any digit that have a slight shift or off axis will cause the these classifiers to struggle.
# 
# Among the simple algorithms, KNN stands out to be a very appropriate algorithm for our problem.

# In[ ]:


from sklearn.neighbors import KNeighborsClassifier

KNN_clf = KNeighborsClassifier(n_jobs = -1)
KNN_clf.fit(X_train, y_train)


# In[ ]:


cross_val_score(KNN_clf, X_train, y_train, cv = 3, scoring = "accuracy").mean()


# The KNN already give us a little boost of accuracy compared to the previous algorithms. Let's fine-tune our model to get a little bit of performance out of our classifier. We'll search a few parameters that already used by the default KNN:

# In[ ]:


# from sklearn.model_selection import GridSearchCV

# param_grid = {
#     "n_neighbors" : [3,4,5,6,7,8],
#     "weights" : ['uniform', 'distance']
# }

# KNN_clf = KNeighborsClassifier(n_jobs = -1)

# grid_search = GridSearchCV(KNN_clf, param_grid , cv = 5, scoring = "accuracy")

# grid_search.fit(X_train, y_train)

# grid_search.best_params_


# If you don't want to waste a couple hours to run this grid search, here are the optimal params:
# 
# {'n_neighbors': 4, 'weights': 'distance'}
# 
# Now let's re-fit the model with the new parameters:

# In[ ]:


KNN_clf = KNeighborsClassifier(n_jobs = -1, n_neighbors = 4, weights = "distance")
KNN_clf.fit(X_train, y_train)
cross_val_score(KNN_clf, X_train, y_train, cv = 3, scoring = "accuracy").mean()


# # Submission

# In[ ]:


sample_submission = pd.read_csv("/kaggle/input/digit-recognizer/sample_submission.csv")
test_predictions = KNN_clf.predict(mnist_test)
submission = sample_submission.drop('Label', axis = 1)
submission['Label'] = test_predictions


# In[ ]:


submission.to_csv("mnist_submission.csv",index=False)

