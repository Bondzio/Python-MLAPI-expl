#!/usr/bin/env python
# coding: utf-8

# In[ ]:



{
   "schemaVersion": 2,
   "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
   "config": {
      "mediaType": "application/vnd.docker.container.image.v1+json",
      "size": 31231,
      "digest": "sha256:79f52292b1d0c079b84bc79d002947be409a09b15a1320355a4de834f57b2ee8"
   },
   "layers": [
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 45339314,
         "digest": "sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95104141,
         "digest": "sha256:86534c0d13b7196a49d52a65548f524b744d48ccaf89454659637bee4811d312"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1571501372,
         "digest": "sha256:5764e90b1fae3f6050c1b56958da5e94c0d0c2a5211955f579958fcbe6a679fd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1083072,
         "digest": "sha256:ba67f7304613606a1d577e2fc5b1e6bb14b764bcc8d07021779173bcc6a8d4b6"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 526,
         "digest": "sha256:19abed793cf0a9952e1a08188dbe2627ed25836757d0e0e3150d5c8328562b4e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 458,
         "digest": "sha256:df204f1f292ae58e4c4141a950fad3aa190d87ed9cc3d364ca6aa1e7e0b73e45"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 13119161,
         "digest": "sha256:1f7809135d9076fb9ed8ee186e93e3352c861489e0e80804f79b2b5634b456dd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 555884253,
         "digest": "sha256:03a365d6218dbe33f5b17d305f5e25e412f7b83b38394c5818bde053a542f11b"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 102870915,
         "digest": "sha256:00e3d0b7af78551716541d2076836df5594948d5d98f04f382158ef26eb7c907"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95925388,
         "digest": "sha256:59782fefadba835c1e83cecdd73dc8e81121eae05ba58d3628a44a1c607feb6e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 142481172,
         "digest": "sha256:f81b01cf2c3f02e153a71704cc5ffe6102757fb7c2fcafc107a64581b0f6dc10"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1128076783,
         "digest": "sha256:f08bbb5c2bce948f0d12eea15c88aad45cdd5b804b71bee5a2cfdbf53c7ec254"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 444800302,
         "digest": "sha256:b831800c60a36c21033cb6e85f0bd3a5f5c9d96b2fa2797d0e8d4c50598180b8"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 157365696,
         "digest": "sha256:6d354ec67fa4ccf30460efadef27d48edf9599348cbab789c388f1d3a7fee232"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 63237273,
         "digest": "sha256:464f9b4eca5cdf36756cf0bef8c76b23344d0e975667becb743e8d6b9019c3cd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 427820542,
         "digest": "sha256:6c1f6bcbc63b982a86dc94301c2996505cec571938c60a434b3de196498c7b89"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 44581800,
         "digest": "sha256:c0a8110c6fede3cf54fa00a1b4e2fcb136d00b3cf490f658ec6d596e313c986e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 127637178,
         "digest": "sha256:c25df885c8dea40780f5b479bb6c7be924763399a21fa46c204d5bfac45056bd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 956429221,
         "digest": "sha256:7c1d98590e22f78a1b820f89b6ce245321437639957264e628b4abf4862e1223"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 586276809,
         "digest": "sha256:aab720d802b7d006b679ac10df4a259b3556812dea4dfc52d6111db47fc41e62"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 21717560,
         "digest": "sha256:5ee4a4cda8613a3fb172a827143aadacb98128479a22a280495604f989bf4483"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 93512644,
         "digest": "sha256:c4699852e987bc3fe9adde2544ffa690ad52ebec229c20f7e4153b015ac238ff"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 19141,
         "digest": "sha256:8d93692c8dcecacb8aca746a868f53d0b0cf1207e08ced8ffb2134bb01c1f871"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 84125618,
         "digest": "sha256:57c74d175611802a57531be97d19f586dc9cd810a5490eab04fd40b648312ead"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 3261,
         "digest": "sha256:1ac7a265bf03308e06e9cad7e77d12b22ca8bc6b7791d46398d95977e0042574"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2162,
         "digest": "sha256:1b4a5be69a4439f3de72877b7d408400e3aa0b4c37e9c70c4490b480bce682c0"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1270,
         "digest": "sha256:648046d6f6c28a42a39c9e68a9da90ccdabbd1ecfd0be77941114df4eb2406a4"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 644,
         "digest": "sha256:19a794f6956d460edfe74d5562d44366a7cf8bd46d83f408f1bf3c46e7282464"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2052,
         "digest": "sha256:880f92e310c2e03c28c5db85b342069b1a56cd13de7998ae52f699829774f075"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 875,
         "digest": "sha256:cad389727d6cd1696ed7e91b70eedd4c86fd30babb648e7be6cc1639582b0928"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 214,
         "digest": "sha256:c873da9a657a590abeae80bd3c0d0d87a6bfdfaf1d3873a0f210760a4050d6db"
      }
   ]
}


# In[ ]:



{
   "schemaVersion": 2,
   "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
   "config": {
      "mediaType": "application/vnd.docker.container.image.v1+json",
      "size": 11621,
      "digest": "sha256:ac0101a73ff90949b8db69f4d80bf59b29b60f7133e5760331e9ce54333a3433"
   },
   "layers": [
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 45339314,
         "digest": "sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95104141,
         "digest": "sha256:86534c0d13b7196a49d52a65548f524b744d48ccaf89454659637bee4811d312"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1571501372,
         "digest": "sha256:5764e90b1fae3f6050c1b56958da5e94c0d0c2a5211955f579958fcbe6a679fd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1083072,
         "digest": "sha256:ba67f7304613606a1d577e2fc5b1e6bb14b764bcc8d07021779173bcc6a8d4b6"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 13102021,
         "digest": "sha256:0c87949db41f992cda232eb94bbced866f473796b91c76b01546ca856cc69940"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 262,
         "digest": "sha256:4ded3b93756b7e71c7273c3629f675ca03c8b44644789e2e4da3100230c193d8"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 276,
         "digest": "sha256:06674d9a1341e2081650f2e5be3c29712f29f8bacf88bffae8d28cd1900d8c17"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 12905,
         "digest": "sha256:913d50cd2d0481f15a1649e26f7cd81d98cff48790634ce1041b5115c4618721"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1677540768,
         "digest": "sha256:f4055de019e1540c147ca77507eb1998e4c3b99b246b16d819d65c0e6f3b0e8a"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 876384458,
         "digest": "sha256:b80edbbaab383c197a983091bc3a7925ff63b867aad006539ae0d324fcc974b4"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 210085421,
         "digest": "sha256:be3f3dde94b4840193aa29f79757f042e98f7ff0c375ecdb84c4be92c9668c41"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 535156494,
         "digest": "sha256:24510da92ee4f7b1faa772bcc9aabe54aac8a02b9fa03d23f5e84bec53c9441a"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 623293155,
         "digest": "sha256:c32a0b5bc2f073442c9014ef576d8ecf42b3661883adb905b84301c1873f8ba0"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 236572106,
         "digest": "sha256:ccd8b3ded2f08340fa55f9af4fd23d259da2b74cedca5e76467daf7693784a59"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 12469,
         "digest": "sha256:1a5ed5dd60c8000c1a5c89daddc1338ab928d364da9b44db87fe4f528d45ea2b"
      }
   ]
}


# In[ ]:



{
   "schemaVersion": 2,
   "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
   "config": {
      "mediaType": "application/vnd.docker.container.image.v1+json",
      "size": 31231,
      "digest": "sha256:79f52292b1d0c079b84bc79d002947be409a09b15a1320355a4de834f57b2ee8"
   },
   "layers": [
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 45339314,
         "digest": "sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95104141,
         "digest": "sha256:86534c0d13b7196a49d52a65548f524b744d48ccaf89454659637bee4811d312"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1571501372,
         "digest": "sha256:5764e90b1fae3f6050c1b56958da5e94c0d0c2a5211955f579958fcbe6a679fd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1083072,
         "digest": "sha256:ba67f7304613606a1d577e2fc5b1e6bb14b764bcc8d07021779173bcc6a8d4b6"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 526,
         "digest": "sha256:19abed793cf0a9952e1a08188dbe2627ed25836757d0e0e3150d5c8328562b4e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 458,
         "digest": "sha256:df204f1f292ae58e4c4141a950fad3aa190d87ed9cc3d364ca6aa1e7e0b73e45"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 13119161,
         "digest": "sha256:1f7809135d9076fb9ed8ee186e93e3352c861489e0e80804f79b2b5634b456dd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 555884253,
         "digest": "sha256:03a365d6218dbe33f5b17d305f5e25e412f7b83b38394c5818bde053a542f11b"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 102870915,
         "digest": "sha256:00e3d0b7af78551716541d2076836df5594948d5d98f04f382158ef26eb7c907"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 95925388,
         "digest": "sha256:59782fefadba835c1e83cecdd73dc8e81121eae05ba58d3628a44a1c607feb6e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 142481172,
         "digest": "sha256:f81b01cf2c3f02e153a71704cc5ffe6102757fb7c2fcafc107a64581b0f6dc10"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1128076783,
         "digest": "sha256:f08bbb5c2bce948f0d12eea15c88aad45cdd5b804b71bee5a2cfdbf53c7ec254"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 444800302,
         "digest": "sha256:b831800c60a36c21033cb6e85f0bd3a5f5c9d96b2fa2797d0e8d4c50598180b8"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 157365696,
         "digest": "sha256:6d354ec67fa4ccf30460efadef27d48edf9599348cbab789c388f1d3a7fee232"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 63237273,
         "digest": "sha256:464f9b4eca5cdf36756cf0bef8c76b23344d0e975667becb743e8d6b9019c3cd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 427820542,
         "digest": "sha256:6c1f6bcbc63b982a86dc94301c2996505cec571938c60a434b3de196498c7b89"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 44581800,
         "digest": "sha256:c0a8110c6fede3cf54fa00a1b4e2fcb136d00b3cf490f658ec6d596e313c986e"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 127637178,
         "digest": "sha256:c25df885c8dea40780f5b479bb6c7be924763399a21fa46c204d5bfac45056bd"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 956429221,
         "digest": "sha256:7c1d98590e22f78a1b820f89b6ce245321437639957264e628b4abf4862e1223"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 586276809,
         "digest": "sha256:aab720d802b7d006b679ac10df4a259b3556812dea4dfc52d6111db47fc41e62"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 21717560,
         "digest": "sha256:5ee4a4cda8613a3fb172a827143aadacb98128479a22a280495604f989bf4483"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 93512644,
         "digest": "sha256:c4699852e987bc3fe9adde2544ffa690ad52ebec229c20f7e4153b015ac238ff"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 19141,
         "digest": "sha256:8d93692c8dcecacb8aca746a868f53d0b0cf1207e08ced8ffb2134bb01c1f871"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 84125618,
         "digest": "sha256:57c74d175611802a57531be97d19f586dc9cd810a5490eab04fd40b648312ead"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 3261,
         "digest": "sha256:1ac7a265bf03308e06e9cad7e77d12b22ca8bc6b7791d46398d95977e0042574"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2162,
         "digest": "sha256:1b4a5be69a4439f3de72877b7d408400e3aa0b4c37e9c70c4490b480bce682c0"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 1270,
         "digest": "sha256:648046d6f6c28a42a39c9e68a9da90ccdabbd1ecfd0be77941114df4eb2406a4"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 644,
         "digest": "sha256:19a794f6956d460edfe74d5562d44366a7cf8bd46d83f408f1bf3c46e7282464"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 2052,
         "digest": "sha256:880f92e310c2e03c28c5db85b342069b1a56cd13de7998ae52f699829774f075"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 875,
         "digest": "sha256:cad389727d6cd1696ed7e91b70eedd4c86fd30babb648e7be6cc1639582b0928"
      },
      {
         "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
         "size": 214,
         "digest": "sha256:c873da9a657a590abeae80bd3c0d0d87a6bfdfaf1d3873a0f210760a4050d6db"
      }
   ]
}


# # COVID-19 Forecasting Ongoing Data Updates
# 
# This notebook prepares the daily updates and leaderboards scores in Kaggle's COVID-19 forecasting competitions:
# 
#  - [Week 2](https://www.kaggle.com/c/covid19-global-forecasting-week-2) (global, currently open for submissions)
#  - Week 1 ([global](https://www.kaggle.com/c/covid19-global-forecasting-week-1), [California state](https://www.kaggle.com/c/covid19-local-us-ca-forecasting-week-1/overview/description)) (closed for submissions; currently in final evaluation scoring)
#  
# The source data comes from [JHU CSSE's COVID-19 data repository on GitHub](https://github.com/CSSEGISandData/COVID-19).
# 
# The notebooks used to prepare the original data at the launch of the competition are here: [Week 1](https://www.kaggle.com/benhamner/covid-19-forecasting-challenges-week-1-data-prep), [Week 2](https://www.kaggle.com/benhamner/covid-19-forecasting-challenges-week-2-data-prep)

# In[ ]:


from datetime import date, datetime, timedelta
import numpy as np
import pandas as pd


# In[ ]:


get_ipython().system(' ls ../input/jhucovid19/csse_covid_19_data/csse_covid_19_daily_reports')


# In[ ]:


get_ipython().system(' ls ../input/jhucovid19/csse_covid_19_data/csse_covid_19_time_series')


# ## Update this date!

# In[ ]:


latest_train_date = date(2020, 4, 18)


# In[ ]:


confirmed = pd.read_csv("../input/jhucovid19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv")
deaths    = pd.read_csv("../input/jhucovid19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv")

wk1_original_test  = pd.read_csv("../input/covid19-forecasting-week-one-launch-data/test.csv")
wk1_original_test.rename(columns={"Province/State": "Province_State", "Country/Region": "Country_Region"}, inplace=True)
wk1_original_test.drop(columns=["Lat", "Long"], inplace=True)

wk2_original_test  = pd.read_csv("../input/covid19-forecasting-week-two-launch-data/test.csv")
wk3_original_test  = pd.read_csv("../input/covid19-forecasting-week-three-launch-data/test.csv")
wk4_original_test  = pd.read_csv("../input/covid19-forecasting-week-four-launch-data/test.csv")


# In[ ]:


assert datetime.strptime(confirmed.columns[-1], '%m/%d/%y').date() == latest_train_date
assert datetime.strptime(deaths.columns[-1], '%m/%d/%y').date() == latest_train_date


# In[ ]:


deaths.head()


# In[ ]:


wk1_launch_date = date(2020, 3, 19)
wk1_public_leaderboard_start_date = wk1_launch_date - timedelta(7)
wk1_close_date = wk1_launch_date + timedelta(6)
wk1_final_evaluation_start_date = wk1_launch_date + timedelta(7)
wk1_final_evaluation_end_date = wk1_launch_date + timedelta(35)

wk2_launch_date = date(2020, 3, 26)
wk2_public_leaderboard_start_date = wk2_launch_date - timedelta(7)
wk2_close_date = wk2_launch_date + timedelta(6)
wk2_final_evaluation_start_date = wk2_launch_date + timedelta(7)
wk2_final_evaluation_end_date = wk2_launch_date + timedelta(35)

wk3_launch_date = date(2020, 4, 2)
wk3_public_leaderboard_start_date = wk3_launch_date - timedelta(7)
wk3_close_date = wk3_launch_date + timedelta(6)
wk3_final_evaluation_start_date = wk3_launch_date + timedelta(7)
wk3_final_evaluation_end_date = wk3_launch_date + timedelta(35)

wk4_launch_date = date(2020, 4, 9)
wk4_public_leaderboard_start_date = wk4_launch_date - timedelta(7)
wk4_close_date = wk4_launch_date + timedelta(6)
wk4_final_evaluation_start_date = wk4_launch_date + timedelta(7)
wk4_final_evaluation_end_date = wk4_launch_date + timedelta(35)


# Move to ISO 8601 dates

# In[ ]:


confirmed.columns = list(confirmed.columns[:4]) + [datetime.strptime(d, "%m/%d/%y").date().strftime("%Y-%m-%d") for d in confirmed.columns[4:]]
deaths.columns    = list(deaths.columns[:4])    + [datetime.strptime(d, "%m/%d/%y").date().strftime("%Y-%m-%d") for d in deaths.columns[4:]]


# In[ ]:


deaths


# In[ ]:


# Filter out problematic data points (The West Bank and Gaza had a negative value, cruise ships were associated with Canada, etc.)
removed_states = "Recovered|Grand Princess|Diamond Princess"
removed_countries = "US|The West Bank and Gaza"

confirmed.rename(columns={"Province/State": "Province_State", "Country/Region": "Country_Region"}, inplace=True)
deaths.rename(columns={"Province/State": "Province_State", "Country/Region": "Country_Region"}, inplace=True)
confirmed = confirmed[~confirmed["Province_State"].replace(np.nan, "nan").str.match(removed_states)]
deaths    = deaths[~deaths["Province_State"].replace(np.nan, "nan").str.match(removed_states)]
confirmed = confirmed[~confirmed["Country_Region"].replace(np.nan, "nan").str.match(removed_countries)]
deaths    = deaths[~deaths["Country_Region"].replace(np.nan, "nan").str.match(removed_countries)]

confirmed.drop(columns=["Lat", "Long"], inplace=True)
deaths.drop(columns=["Lat", "Long"], inplace=True)


# In[ ]:


us_keys = pd.read_csv("../input/jhucovid19/csse_covid_19_data/csse_covid_19_daily_reports/%s.csv" % latest_train_date.strftime("%m-%d-%Y"))
us_keys = us_keys[us_keys["Country_Region"]=="US"]
us_keys = us_keys.groupby(["Province_State", "Country_Region"])[["Confirmed", "Deaths"]].sum().reset_index()

us_keys = us_keys[~us_keys.Province_State.str.match("Diamond Princess|Grand Princess|Recovered|Northern Mariana Islands|American Samoa")].reset_index(drop=True)
us_keys


# In[ ]:


confirmed = confirmed.append(us_keys[["Province_State", "Country_Region"]], sort=False).reset_index(drop=True)
deaths = deaths.append(us_keys[["Province_State", "Country_Region"]], sort=False).reset_index(drop=True)


# In[ ]:


for col in confirmed.columns[2:]:
    confirmed[col].fillna(0, inplace=True)
    deaths[col].fillna(0, inplace=True)


# In[ ]:


confirmed


# Add in daily US data

# In[ ]:


us_start_date = date(2020, 3, 10)
day_date = us_start_date

while day_date <= latest_train_date:
    day = pd.read_csv("../input/jhucovid19/csse_covid_19_data/csse_covid_19_daily_reports/%s.csv" % day_date.strftime("%m-%d-%Y"))
    
    if "Country/Region" in day.columns:
        day.rename(columns={"Country/Region": "Country_Region", "Province/State": "Province_State"}, inplace=True)
    
    us = day[day["Country_Region"]=="US"]
    us = us.groupby(["Province_State", "Country_Region"])[["Confirmed", "Deaths"]].sum().reset_index()
    
    unused_data = []
    untouched_states = set(confirmed[confirmed["Country_Region"]=="US"]["Province_State"])
    
    for (i, row) in us.iterrows():
        if confirmed[(confirmed["Country_Region"]=="US") & (confirmed["Province_State"]==row["Province_State"])].shape[0]==1:
            confirmed.loc[(confirmed["Country_Region"]=="US") & (confirmed["Province_State"]==row["Province_State"]), day_date.strftime("%Y-%m-%d")] = row["Confirmed"]
            deaths.loc[(deaths["Country_Region"]=="US") & (deaths["Province_State"]==row["Province_State"]), day_date.strftime("%Y-%m-%d")] = row["Deaths"]
            untouched_states.remove(row["Province_State"])
        else:
            unused_data.append(row["Province_State"])
            
    print(day_date, "Untouched", untouched_states)
    print(day_date, "Unused", unused_data)

    day_date = day_date + timedelta(1)


# In[ ]:


confirmed


# In[ ]:


deaths


# # Leaderboard Updates for Week 1

# In[ ]:


wk1_original_test


# In[ ]:


confirmed[confirmed["Province_State"]=="Puerto Rico"]


# In[ ]:


# Correcting some location renames between week 1 and the current set
# Aruba, Puerto Rico, Virgin Islands were all in as two separate duplicate locations for the original test set. Not correcting those

wk1_confirmed = confirmed.copy()
wk1_deaths = deaths.copy()

wk1_confirmed.loc[wk1_confirmed["Province_State"].isna() & (wk1_confirmed["Country_Region"]=="France"), "Province_State"] = "France"
wk1_deaths.loc[wk1_confirmed["Province_State"].isna() & (wk1_deaths["Country_Region"]=="France"), "Province_State"] = "France"

wk1_confirmed.loc[wk1_confirmed["Province_State"].isna() & (wk1_confirmed["Country_Region"]=="United Kingdom"), "Province_State"] = "United Kingdom"
wk1_deaths.loc[wk1_deaths["Province_State"].isna() & (wk1_deaths["Country_Region"]=="United Kingdom"), "Province_State"] = "United Kingdom"

wk1_confirmed.loc[wk1_confirmed["Province_State"].isna() & (wk1_confirmed["Country_Region"]=="Netherlands"), "Province_State"] = "Netherlands"
wk1_deaths.loc[wk1_deaths["Province_State"].isna() & (wk1_deaths["Country_Region"]=="Netherlands"), "Province_State"] = "Netherlands"

wk1_confirmed.loc[wk1_confirmed["Province_State"].isna() & (wk1_confirmed["Country_Region"]=="Denmark"), "Province_State"] = "Denmark"
wk1_deaths.loc[wk1_deaths["Province_State"].isna() & (wk1_deaths["Country_Region"]=="Denmark"), "Province_State"] = "Denmark"

wk1_confirmed.loc[(wk1_confirmed["Province_State"]=="Greenland") & (wk1_confirmed["Country_Region"]=="Denmark"), "Country_Region"] = "Greenland"
wk1_confirmed.loc[(wk1_confirmed["Province_State"]=="Greenland") & (wk1_confirmed["Country_Region"]=="Greenland"), "Province_State"] = np.nan
wk1_deaths.loc[(wk1_deaths["Province_State"]=="Greenland") & (wk1_deaths["Country_Region"]=="Denmark"), "Country_Region"] = "Greenland"
wk1_deaths.loc[(wk1_deaths["Province_State"]=="Greenland") & (wk1_deaths["Country_Region"]=="Greenland"), "Province_State"] = np.nan

wk1_confirmed.loc[wk1_confirmed["Province_State"].isna() & (wk1_confirmed["Country_Region"]=="Bahamas"), "Country_Region"] = "The Bahamas"
wk1_deaths.loc[wk1_deaths["Province_State"].isna() & (wk1_deaths["Country_Region"]=="Bahamas"), "Country_Region"] = "The Bahamas"


# In[ ]:


wk1_original_test[wk1_original_test["Province_State"]=="United States Virgin Islands"]


# In[ ]:


case_locations = set([(wk1_confirmed.loc[i, "Province_State"], wk1_confirmed.loc[i, "Country_Region"]) for i in wk1_confirmed.index])
wk1_original_test_locations = set([(wk1_original_test.loc[i, "Province_State"], wk1_original_test.loc[i, "Country_Region"]) for i in wk1_original_test.index])


# In[ ]:


case_locations.difference(wk1_original_test_locations)


# In[ ]:


# Aruba, Puerto Rico, Virgin Islands were all in as two separate duplicate locations for the original test set. Not correcting those
# Not correcting the cruise ship ones - those are messy to track and probably not helpful to forecast

missing_locations = wk1_original_test_locations.difference(case_locations)
missing_locations


# In[ ]:


# Map locations back to the original launch set

wk1_confirmed = wk1_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(wk1_confirmed, on=["Province_State", "Country_Region"], how="left")
wk1_deaths    = wk1_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(wk1_deaths,    on=["Province_State", "Country_Region"], how="left")


# In[ ]:


# Change any missing data to 0 from locations that got dropped

for col in wk1_confirmed.columns[2:]:
    wk1_confirmed[col].fillna(0, inplace=True)
    wk1_deaths[col].fillna(0, inplace=True)


# Adding the rows to be forecast

# In[ ]:


this_date = latest_train_date

while this_date <= wk1_final_evaluation_end_date:
    if this_date.strftime("%Y-%m-%d") not in wk1_confirmed:
        wk1_confirmed.insert(len(wk1_confirmed.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
        wk1_deaths.insert(len(wk1_deaths.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
    this_date = this_date + timedelta(1)


# Melting the data to a version that will be friendlier to Kaggle's evaluation system.

# In[ ]:


wk1_confirmed_melted = wk1_confirmed.melt(wk1_confirmed.columns[:2], wk1_confirmed.columns[2:], "Date", "ConfirmedCases")
wk1_deaths_melted = wk1_deaths.melt(wk1_deaths.columns[:2], wk1_deaths.columns[2:], "Date", "Fatalities")

wk1_confirmed_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk1_deaths_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)

assert wk1_confirmed_melted.shape==wk1_deaths_melted.shape
assert list(wk1_confirmed_melted["Province_State"])==list(wk1_deaths_melted["Province_State"])
assert list(wk1_confirmed_melted["Country_Region"])==list(wk1_deaths_melted["Country_Region"])
assert list(wk1_confirmed_melted["Date"])==list(wk1_deaths_melted["Date"])

wk1_cases = wk1_confirmed_melted.merge(wk1_deaths_melted, on=["Province_State", "Country_Region", "Date"], how="inner")

wk1_cases.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk1_cases.insert(0, "Id", range(1, wk1_cases.shape[0]+1))
wk1_cases


# In[ ]:


wk1_forecast = wk1_cases.loc[wk1_cases["Date"]>=wk1_public_leaderboard_start_date.strftime("%Y-%m-%d")].copy()
wk1_forecast.drop(columns="Id", inplace=True)
wk1_forecast.insert(0, "ForecastId", range(1, wk1_forecast.shape[0]+1))
wk1_forecast.insert(6, "Usage", "Ignored")
wk1_forecast.loc[wk1_forecast["Date"]<=min(latest_train_date, wk1_close_date).strftime("%Y-%m-%d"),"Usage"]="Public"
wk1_forecast.loc[(wk1_forecast["Date"]>=wk1_final_evaluation_start_date.strftime("%Y-%m-%d")) & (wk1_forecast["Date"]<=latest_train_date.strftime("%Y-%m-%d")),"Usage"]="Private"

for loc in missing_locations:
    if (type(loc[0]) is not str) and np.isnan(loc[0]):
        wk1_forecast.loc[(wk1_forecast["Province_State"].isna()) & (wk1_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"
    else:
        wk1_forecast.loc[(wk1_forecast["Province_State"]==loc[0]) & (wk1_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"

wk1_forecast


# ### Week 1 Global competition data

# In[ ]:


# wk1_train = wk1_cases[wk1_cases["Date"]<=latest_train_date.strftime("%Y-%m-%d")].copy()
# wk1_train.to_csv("wk1_train.csv", index=False)
# wk1_train


# In[ ]:


# wk1_test = wk1_forecast[wk1_forecast.columns[:-3]].copy()
# wk1_test.to_csv("wk1_test.csv", index=False)
# wk1_test


# In[ ]:


wk1_solution = wk1_forecast[["ForecastId", "ConfirmedCases", "Fatalities", "Usage"]].copy()
wk1_solution["ConfirmedCases"].fillna(1, inplace=True)
wk1_solution["Fatalities"].fillna(1, inplace=True)
wk1_solution.to_csv("wk1_solution.csv", index=False)
wk1_solution


# In[ ]:


wk1_solution.head(25)


# In[ ]:


wk1_submission = wk1_forecast[["ForecastId", "ConfirmedCases", "Fatalities"]].copy()
wk1_submission["ConfirmedCases"] = 1
wk1_submission["Fatalities"] = 1
wk1_submission.to_csv("wk1_submission.csv", index=False)

wk1_submission


# ### Week 1 California competition data

# In[ ]:


# wk1_ca_cases = wk1_cases[(wk1_cases["Country_Region"]=="US") & (wk1_cases["Province_State"]=="California")].copy()
# wk1_ca_cases["Id"] = range(1, wk1_ca_cases.shape[0]+1)
# wk1_ca_train = wk1_ca_cases[wk1_ca_cases["Date"]<=latest_train_date.strftime("%Y-%m-%d")]
# wk1_ca_train.to_csv("wk1_ca_train.csv", index=False)
# wk1_ca_train


# In[ ]:


wk1_ca_forecast = wk1_forecast[(wk1_forecast["Country_Region"]=="US") & (wk1_forecast["Province_State"]=="California")].copy()
wk1_ca_forecast["ForecastId"] = range(1, wk1_ca_forecast.shape[0]+1)
wk1_ca_forecast


# In[ ]:


# wk1_ca_test = wk1_ca_forecast[wk1_ca_forecast.columns[:-3]].copy()
# wk1_ca_test.to_csv("wk1_ca_test.csv", index=False)
# wk1_ca_test


# In[ ]:


wk1_ca_solution = wk1_ca_forecast[["ForecastId", "ConfirmedCases", "Fatalities", "Usage"]].copy()
wk1_ca_solution["ConfirmedCases"].fillna(1, inplace=True)
wk1_ca_solution["Fatalities"].fillna(1, inplace=True)
wk1_ca_solution.to_csv("wk1_ca_solution.csv", index=False)
wk1_ca_solution


# In[ ]:


wk1_ca_submission = wk1_ca_forecast[["ForecastId", "ConfirmedCases", "Fatalities"]].copy()
wk1_ca_submission["ConfirmedCases"] = 1
wk1_ca_submission["Fatalities"] = 1
wk1_ca_submission.to_csv("wk1_ca_submission.csv", index=False)
wk1_ca_submission


# # Data Updates for Week 2

# In[ ]:


wk2_original_test


# In[ ]:


case_locations = set([(confirmed.loc[i, "Province_State"], confirmed.loc[i, "Country_Region"]) for i in confirmed.index])
wk2_original_test_locations = set([(wk2_original_test.loc[i, "Province_State"], wk2_original_test.loc[i, "Country_Region"]) for i in wk2_original_test.index])


# In[ ]:


case_locations.difference(wk2_original_test_locations)


# In[ ]:


missing_locations = wk2_original_test_locations.difference(case_locations)
missing_locations


# In[ ]:


# Map locations back to the original launch set

wk2_confirmed = wk2_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(confirmed, on=["Province_State", "Country_Region"], how="left")
wk2_deaths    = wk2_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(deaths,    on=["Province_State", "Country_Region"], how="left")


# In[ ]:


# Change any missing data to 0 from locations that got dropped

for col in wk2_confirmed.columns[2:]:
    wk2_confirmed[col].fillna(0, inplace=True)
    wk2_deaths[col].fillna(0, inplace=True)


# In[ ]:


this_date = latest_train_date

while this_date <= wk2_final_evaluation_end_date:
    if this_date.strftime("%Y-%m-%d") not in wk2_confirmed:
        wk2_confirmed.insert(len(wk2_confirmed.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
        wk2_deaths.insert(len(wk2_deaths.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
    this_date = this_date + timedelta(1)


# In[ ]:


wk2_confirmed_melted = wk2_confirmed.melt(wk2_confirmed.columns[:2], wk2_confirmed.columns[2:], "Date", "ConfirmedCases")
wk2_deaths_melted = wk2_deaths.melt(wk2_deaths.columns[:2], wk2_deaths.columns[2:], "Date", "Fatalities")

wk2_confirmed_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk2_deaths_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)

assert wk2_confirmed_melted.shape==wk2_deaths_melted.shape
assert list(wk2_confirmed_melted["Province_State"])==list(wk2_deaths_melted["Province_State"])
assert list(wk2_confirmed_melted["Country_Region"])==list(wk2_deaths_melted["Country_Region"])
assert list(wk2_confirmed_melted["Date"])==list(wk2_deaths_melted["Date"])

wk2_cases = wk2_confirmed_melted.merge(wk2_deaths_melted, on=["Province_State", "Country_Region", "Date"], how="inner")

wk2_cases.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk2_cases.insert(0, "Id", range(1, wk2_cases.shape[0]+1))
wk2_cases


# In[ ]:


wk2_forecast = wk2_cases[wk2_cases["Date"]>=wk2_public_leaderboard_start_date.strftime("%Y-%m-%d")].copy()
wk2_forecast.drop(columns="Id", inplace=True)
wk2_forecast.insert(0, "ForecastId", range(1, wk2_forecast.shape[0]+1))
wk2_forecast.insert(6, "Usage", "Ignored")
wk2_forecast.loc[wk2_forecast["Date"]<=min(latest_train_date, wk2_close_date).strftime("%Y-%m-%d"),"Usage"]="Public"
wk2_forecast.loc[(wk2_forecast["Date"]>=wk2_final_evaluation_start_date.strftime("%Y-%m-%d")) & (wk2_forecast["Date"]<=latest_train_date.strftime("%Y-%m-%d")),"Usage"]="Private"

for loc in missing_locations:
    if (type(loc[0]) is not str) and np.isnan(loc[0]):
        wk2_forecast.loc[(wk1_forecast["Province_State"].isna()) & (wk2_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"
    else:
        wk2_forecast.loc[(wk1_forecast["Province_State"]==loc[0]) & (wk2_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"

wk2_forecast


# In[ ]:


# wk2_train = wk2_cases[wk2_cases["Date"]<=latest_train_date.strftime("%Y-%m-%d")].copy()
# wk2_train.to_csv("train.csv", index=False)
# wk2_train


# In[ ]:


# wk2_test = wk2_forecast[wk2_forecast.columns[:-3]].copy()
# wk2_test.to_csv("test.csv", index=False)
# wk2_test


# In[ ]:


wk2_solution = wk2_forecast[["ForecastId", "ConfirmedCases", "Fatalities", "Usage"]].copy()
wk2_solution["ConfirmedCases"].fillna(1, inplace=True)
wk2_solution["Fatalities"].fillna(1, inplace=True)
wk2_solution.to_csv("wk2_solution.csv", index=False)
wk2_solution


# In[ ]:


wk2_submission = wk2_forecast[["ForecastId", "ConfirmedCases", "Fatalities"]].copy()
wk2_submission["ConfirmedCases"] = 1
wk2_submission["Fatalities"] = 1
wk2_submission.to_csv("wk2_submission.csv", index=False)

wk2_submission


# # Data Updates for Week 3

# In[ ]:


wk3_original_test


# In[ ]:


case_locations = set([(confirmed.loc[i, "Province_State"], confirmed.loc[i, "Country_Region"]) for i in confirmed.index])
wk3_original_test_locations = set([(wk3_original_test.loc[i, "Province_State"], wk3_original_test.loc[i, "Country_Region"]) for i in wk3_original_test.index])


# In[ ]:


case_locations.difference(wk3_original_test_locations)


# In[ ]:


missing_locations = wk3_original_test_locations.difference(case_locations)
missing_locations


# In[ ]:


# Map locations back to the original launch set

wk3_confirmed = wk3_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(confirmed, on=["Province_State", "Country_Region"], how="left")
wk3_deaths    = wk3_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(deaths,    on=["Province_State", "Country_Region"], how="left")


# In[ ]:


# Change any missing data to 0 from locations that got dropped

for col in wk3_confirmed.columns[2:]:
    wk3_confirmed[col].fillna(0, inplace=True)
    wk3_deaths[col].fillna(0, inplace=True)


# In[ ]:


this_date = latest_train_date

while this_date <= wk3_final_evaluation_end_date:
    if this_date.strftime("%Y-%m-%d") not in wk3_confirmed:
        wk3_confirmed.insert(len(wk3_confirmed.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
        wk3_deaths.insert(len(wk3_deaths.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
    this_date = this_date + timedelta(1)


# In[ ]:


wk3_confirmed_melted = wk3_confirmed.melt(wk3_confirmed.columns[:2], wk3_confirmed.columns[2:], "Date", "ConfirmedCases")
wk3_deaths_melted = wk3_deaths.melt(wk3_deaths.columns[:2], wk3_deaths.columns[2:], "Date", "Fatalities")

wk3_confirmed_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk3_deaths_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)

assert wk3_confirmed_melted.shape==wk3_deaths_melted.shape
assert list(wk3_confirmed_melted["Province_State"])==list(wk3_deaths_melted["Province_State"])
assert list(wk3_confirmed_melted["Country_Region"])==list(wk3_deaths_melted["Country_Region"])
assert list(wk3_confirmed_melted["Date"])==list(wk3_deaths_melted["Date"])

wk3_cases = wk3_confirmed_melted.merge(wk3_deaths_melted, on=["Province_State", "Country_Region", "Date"], how="inner")

wk3_cases.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk3_cases.insert(0, "Id", range(1, wk3_cases.shape[0]+1))
wk3_cases


# In[ ]:


wk3_forecast = wk3_cases[wk3_cases["Date"]>=wk3_public_leaderboard_start_date.strftime("%Y-%m-%d")].copy()
wk3_forecast.drop(columns="Id", inplace=True)
wk3_forecast.insert(0, "ForecastId", range(1, wk3_forecast.shape[0]+1))
wk3_forecast.insert(6, "Usage", "Ignored")
wk3_forecast.loc[wk3_forecast["Date"]<=min(latest_train_date, wk3_close_date).strftime("%Y-%m-%d"),"Usage"]="Public"
wk3_forecast.loc[(wk3_forecast["Date"]>=wk3_final_evaluation_start_date.strftime("%Y-%m-%d")) & (wk3_forecast["Date"]<=latest_train_date.strftime("%Y-%m-%d")),"Usage"]="Private"

for loc in missing_locations:
    if (type(loc[0]) is not str) and np.isnan(loc[0]):
        wk3_forecast.loc[(wk1_forecast["Province_State"].isna()) & (wk3_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"
    else:
        wk3_forecast.loc[(wk1_forecast["Province_State"]==loc[0]) & (wk3_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"

wk3_forecast


# In[ ]:


# wk3_train = wk3_cases[wk3_cases["Date"]<=latest_train_date.strftime("%Y-%m-%d")]
# wk3_train.to_csv("train.csv", index=False)
# wk3_train


# In[ ]:


# wk3_test = wk3_forecast[wk3_forecast.columns[:-3]]
# wk3_test.to_csv("test.csv", index=False)
# wk3_test


# In[ ]:


wk3_solution = wk3_forecast[["ForecastId", "ConfirmedCases", "Fatalities", "Usage"]].copy()
wk3_solution["ConfirmedCases"].fillna(1, inplace=True)
wk3_solution["Fatalities"].fillna(1, inplace=True)
wk3_solution.to_csv("wk3_solution.csv", index=False)
wk3_solution


# In[ ]:


wk3_submission = wk3_forecast[["ForecastId", "ConfirmedCases", "Fatalities"]].copy()
wk3_submission["ConfirmedCases"] = 1
wk3_submission["Fatalities"] = 1
wk3_submission.to_csv("wk3_submission.csv", index=False)

wk3_submission


# # Data Updates for Week 4

# In[ ]:


wk4_original_test


# In[ ]:


case_locations = set([(confirmed.loc[i, "Province_State"], confirmed.loc[i, "Country_Region"]) for i in confirmed.index])
wk4_original_test_locations = set([(wk4_original_test.loc[i, "Province_State"], wk4_original_test.loc[i, "Country_Region"]) for i in wk4_original_test.index])


# In[ ]:


case_locations.difference(wk4_original_test_locations)


# In[ ]:


missing_locations = wk4_original_test_locations.difference(case_locations)
missing_locations


# In[ ]:


# Map locations back to the original launch set

wk4_confirmed = wk4_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(confirmed, on=["Province_State", "Country_Region"], how="left")
wk4_deaths    = wk4_original_test[["Province_State", "Country_Region"]].drop_duplicates().reset_index(drop=True).merge(deaths,    on=["Province_State", "Country_Region"], how="left")


# In[ ]:


# Change any missing data to 0 from locations that got dropped

for col in wk4_confirmed.columns[2:]:
    wk4_confirmed[col].fillna(0, inplace=True)
    wk4_deaths[col].fillna(0, inplace=True)


# In[ ]:


this_date = latest_train_date

while this_date <= wk4_final_evaluation_end_date:
    if this_date.strftime("%Y-%m-%d") not in wk4_confirmed:
        wk4_confirmed.insert(len(wk4_confirmed.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
        wk4_deaths.insert(len(wk4_deaths.columns), this_date.strftime("%Y-%m-%d"), np.NaN)
    this_date = this_date + timedelta(1)


# In[ ]:


wk4_confirmed_melted = wk4_confirmed.melt(wk4_confirmed.columns[:2], wk4_confirmed.columns[2:], "Date", "ConfirmedCases")
wk4_deaths_melted = wk4_deaths.melt(wk4_deaths.columns[:2], wk4_deaths.columns[2:], "Date", "Fatalities")

wk4_confirmed_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk4_deaths_melted.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)

assert wk4_confirmed_melted.shape==wk4_deaths_melted.shape
assert list(wk4_confirmed_melted["Province_State"])==list(wk4_deaths_melted["Province_State"])
assert list(wk4_confirmed_melted["Country_Region"])==list(wk4_deaths_melted["Country_Region"])
assert list(wk4_confirmed_melted["Date"])==list(wk4_deaths_melted["Date"])

wk4_cases = wk4_confirmed_melted.merge(wk4_deaths_melted, on=["Province_State", "Country_Region", "Date"], how="inner")

wk4_cases.sort_values(by=["Country_Region", "Province_State", "Date"], inplace=True)
wk4_cases.insert(0, "Id", range(1, wk4_cases.shape[0]+1))
wk4_cases


# In[ ]:


wk4_forecast = wk4_cases[wk4_cases["Date"]>=wk4_public_leaderboard_start_date.strftime("%Y-%m-%d")].copy()
wk4_forecast.drop(columns="Id", inplace=True)
wk4_forecast.insert(0, "ForecastId", range(1, wk4_forecast.shape[0]+1))
wk4_forecast.insert(6, "Usage", "Ignored")
wk4_forecast.loc[wk4_forecast["Date"]<=min(latest_train_date, wk4_close_date).strftime("%Y-%m-%d"),"Usage"]="Public"
wk4_forecast.loc[(wk4_forecast["Date"]>=wk4_final_evaluation_start_date.strftime("%Y-%m-%d")) & (wk4_forecast["Date"]<=latest_train_date.strftime("%Y-%m-%d")),"Usage"]="Private"

for loc in missing_locations:
    if (type(loc[0]) is not str) and np.isnan(loc[0]):
        wk4_forecast.loc[(wk1_forecast["Province_State"].isna()) & (wk4_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"
    else:
        wk4_forecast.loc[(wk1_forecast["Province_State"]==loc[0]) & (wk4_forecast["Country_Region"]==loc[1]), "Usage"]="Ignored"

wk4_forecast


# In[ ]:


wk4_train = wk4_cases[wk4_cases["Date"]<=latest_train_date.strftime("%Y-%m-%d")]
wk4_train.to_csv("train.csv", index=False)
wk4_train


# In[ ]:


wk4_test = wk4_forecast[wk4_forecast.columns[:-3]]
wk4_test.to_csv("test.csv", index=False)
wk4_test


# In[ ]:


wk4_solution = wk4_forecast[["ForecastId", "ConfirmedCases", "Fatalities", "Usage"]].copy()
wk4_solution["ConfirmedCases"].fillna(1, inplace=True)
wk4_solution["Fatalities"].fillna(1, inplace=True)
wk4_solution.to_csv("wk4_solution.csv", index=False)
wk4_solution


# In[ ]:


wk4_submission = wk4_forecast[["ForecastId", "ConfirmedCases", "Fatalities"]].copy()
wk4_submission["ConfirmedCases"] = 1
wk4_submission["Fatalities"] = 1
wk4_submission.to_csv("wk4_submission.csv", index=False)
wk4_submission.to_csv("submission.csv", index=False)
wk4_submission


# In[ ]:




