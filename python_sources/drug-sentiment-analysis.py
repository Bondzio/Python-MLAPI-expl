#!/usr/bin/env python
# coding: utf-8

# # Innoplexus online hiring Hackathon
# 
# ### by Ankur Singh

# ## Please upvote this Kaggle kernel if you find it helpful :)

# In this kernel, I will showcase my approach of takling [drug sentiment analysis](https://datahack.analyticsvidhya.com/contest/innoplexus-online-hiring-hackathon/) hosted by [Analytics Vidhya](https://analyticsvidhya.com). You are free to use any part of the code. See this [medium article](https://medium.com/@ankursingh_82471/text-classification-5e119f23905e) for detailed dicussion.
# 
# I will be using 2 different approaches. They are listed as follows:
# - Fastai (ULMfit + TextClasLearner)
# - Logistic Regression
# 

# ## Fastai (Approach - 1)

# In[ ]:


from fastai import *
from fastai.text import *

import pandas as pd


# In[ ]:


bs = 32
path = Path('../input/innoplexusav/')


# In[ ]:


## Lets look at our data first
train = pd.read_csv(path/'train.csv'); train.head()


# ### Language Model

# In[ ]:


## Language model data
data_lm = TextLMDataBunch.from_csv(path, 'train.csv', text_cols='text')


# In[ ]:


## Language model Learner
learn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=1.0, model_dir='/tmp/models')


# > Note: I am changing the *model_dir* because kaggle does not allow us to save files inside *../input/* (it is in read-only mode)

# In[ ]:


## Look at some of the text generated by our language model

TEXT = "The color of the sky is"
N_WORDS = 40
N_SENTENCES = 2
print("\n".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))


# In[ ]:


learn_lm.lr_find()
learn_lm.recorder.plot()


# In[ ]:


lr = 1e-2/2
lr *= bs/48
learn_lm.to_fp16(); # converting the model to 1/2 precision. Helps the model to train faster


# In[ ]:


learn_lm.fit_one_cycle(1, lr*10, moms=(0.8,0.7)) # training only the head


# In[ ]:


learn_lm.unfreeze()
learn_lm.fit_one_cycle(10, lr, moms=(0.8,0.7)) # training the complete model


# In[ ]:


learn_lm.save('ft_lm') # saving the complete language model


# In[ ]:


learn_lm.save_encoder('ft_enc') # saving only the encoder part of the language model


# ### Classifier

# In[ ]:


# Classifier model data
data_clas = TextClasDataBunch.from_csv(path, 'train.csv',test='test.csv', text_cols='text', label_cols='sentiment', vocab=data_lm.vocab, bs=bs)


# In[ ]:


# learner for classification
learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir='/tmp/models', metrics=[accuracy, FBeta(average='macro',beta=1)])
learn.load_encoder('ft_enc')


# > Note: Again the *model_dir* is set to '/tmp/model/' for the same reason as above.
# 
# > Note: *Metrics* is changed to FBeta because the completions used FBeta as evaluation metric.

# In[ ]:


learn.lr_find()


# In[ ]:


learn.recorder.plot()


# In[ ]:


learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))


# In[ ]:


learn.save('first')


# In[ ]:


## Gradually unfreezing 
learn.freeze_to(-2)
learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))


# In[ ]:


learn.save('2nd')


# In[ ]:


# learn.load('2nd')


# In[ ]:


learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))


# In[ ]:


learn.save('3rd')


# In[ ]:


## training the complete model
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))


# In[ ]:


learn.save('clas')


# In[ ]:


test_df = pd.read_csv(path/'test.csv')
test_df.head()


# To make prediction you can either add the test set while creating *TextClasDataBunch* or your can also add it later (what I did here). You can read more about it [here](https://stackoverflow.com/questions/56327207/not-able-to-predict-output-in-fastai)

# In[ ]:


test = TextList.from_csv(path, 'test.csv', cols='text')


# In[ ]:


learn.data.add_test(test) 


# In[ ]:


predictions, *_ = learn.get_preds(DatasetType.Test) # making predictions 
labels = np.argmax(predictions, 1)


# In[ ]:


labels


# **Conclusion:** 
# 
# Best Score by ULMfit was 0.545760

# ## Logistic Regression (Approach - 2)

# In[ ]:


from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import f1_score, fbeta_score


# In[ ]:


def score(y, y_pred):
    return fbeta_score(y, y_pred, beta=1, average='macro')

def evaluate_model(model, X, y):
    scores = cross_val_score(model, X, y, cv=3, n_jobs=-1)
    return scores.mean(), scores.std()


# In[ ]:


train = pd.read_csv(path/'train.csv')
train.head()


# In[ ]:


X = TfidfVectorizer(ngram_range=(1,3), stop_words='english', max_features=60000).fit_transform(train.text)
y = train.sentiment


# In[ ]:


model = LogisticRegression(class_weight='balanced')


# In[ ]:


evaluate_model(model, X,y)


# **Conclusion:**
# 
# Logistic Regression (0.724) beats ULMfit(0.546) by a really big margin. It might not be always true but for the particular dataset, logistic regression performs better than ULMfit.

# In[ ]:




