#!/usr/bin/env python
# coding: utf-8

# ## Sequence-to-sequence RNN models
# 
# ### General design
# A sequence-to-sequence (sometimes called seq-to-seq) model is a particular type of recurrent network that takes one sequence as input, and outputs a different sequence as output. This is is a common task; it is e.g. what you do when you want to train a machine translation algorithm, which takes a list of tokens in one language as input, and provides a list of tokens in a different language as output. This use case in fact is what motivated much of the research on this particular architecture.
# 
# Seq-to-seq models are composed of two pieces: an encoder module that reads in and builds an intermediate representation of the token stream, and a decoder module that reads in this intermediate representation and turns it into a token stream in a different language.
# 
# By separating the solving of input normalization from output generation based on that normalized form, seq-to-seq models decorrelate the length of the output from the length of the input. This is important in the machine translation domain, where input sentences and output sentences may have wildly different lengths and structures.
# 
# The encoder and decoder modules have the same basic layout: `Input` --> `Embedding` --> `LSTM`. A decoder module has a `TimeDistributed(Dense)` layer as well as its last layer. The encoder module takes the tokenized input sequence (e.g. `X`) as input and returns an information vector as output. The decoder module takes a bit-shifted version of the output sequence (e.g. `y`) as input *at training time*, e.g. with the first token position's value replaced with the null vector and everything else moved over one index position, and returns the target sequence. It takes masked null vectors (e.g. all-zero vectors with `mask_zero=True`) *at prediction time*, and returns the target sequence.
# 
# ![](https://i.imgur.com/hTDjh6Z.png)
# 
# ### Teacher forcing
# The encoder and decoder modules are virtually independent, except in one linkage point: the final output weights of the encoder LSTM layer are the first two input weights of the decoder LSTM layer (it is provided as input to the first two layers, not just the first one layer, because the first time-step is training on the clear with masking, so it doesn't do anything). This technique is known as **teacher forcing**. 
# 
# Teacher forcing is the use of actual or expected output from the training dataset at the current time step $y(t)$ as input in the next time step $X(t+1)$, rather than the output generated by the network. In the case of our model, the network is started off with a null token, based off of which it is expected to generate the next token. That token is then used as an input for the generation of the token after that, and *that* token is applied towards the token after that, and so on down the line, recursively.
# 
# The decoder module is trained using teacher forcing target sequence input in addition to user token input, but the predictions occurs with every one of these connections that proceeds after tokens that have already been found completely masked out, e.g. relying just on the combination of user token input and the list of tokens discovered thus far. In this way, teacher forcing allows the model to use its entire precedent of tokens already generated in generatating the next fresh token. A side effect of this construction is that performing a single translation task requires running the model as many times as there are token spaces.
# 
# Teacher forcing as it is applied to the seq-to-seq model creates the constraint that backpropogation has to generate weights on the last time-step of the encoder module LSTM layer that are equal to the weights on the first "actual" (non-masked) time-step of the decoder module LSTM layer.
# 
# Even after much thought, I wasn't able to reconstruct what this constraint means from the perspective of the data. It doesn't actually place much of a restriction on the model's respective training space, since neural networks are general enough to be able to correct any weirdness this restriction creates in the hidden vectors. It mostly seems to function as a way to introduce the advantages of teacher forcing into the model, namely (reputedly) model skill and model stability, as described [in this machinelearningmastery blog post](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) on the subject.
# 
# From the perspective of training, the linkage between the encoder and decoder module forces backpropogation to solve for the encoder and decoder simultaneously. Passing the encoder weights to the decoder weights is conceptually different from passing the encoder output to the decoder input, but computationally this is nevertheless a simultaneity restriction you can be trained on.
# 
# An example model follows. This model definition was taken from the following blog post: ["English to Katakana using Sequence to Sequence in Keras"](https://wanasit.github.io/english-to-katakana-using-sequence-to-sequence-in-keras.html).

# In[ ]:


# Note: this example does not cover tokenization.

# First we build the encoder.
#
# First, generate word embeddings for the input tokens. To learn more about word embeddings read the
# following notebook: https://www.kaggle.com/residentmario/notes-on-word-embedding-algorithms/
#
# In this example training the word embeddings is part of the training process. You can use a prebuilt
# volcabulary instead, if you'd like.
#
# mask_zero is set to True to signal that the value 0 is to be used as a mask value in the dataset.
# In that case note that 0 becomes a reserved value in the token dictionary, so you'll need to add
# a +1 to your token dictionary size.
#
# encoder_input_values  = [['token', 'list', 'to', 'be', 'processed', 'in', 'input', 'language']]
encoder_input = Input(shape=(INPUT_LENGTH,))
encoder = Embedding(input_dict_size, 64, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)

# Now the encoder's LSTM layer. return_sequences=True has the LSTM return all of the hidden vectors
# passed between time-steps, instead of just the last (output) hidden vector. unroll is a speedup
# optimization that is only effective for small input sizes.
encoder = LSTM(64, return_sequences=True, unroll=True)(encoder)

# Now we build the decoder. Again an embedding layer is the first layer.
# 
# Recall that in our model: sentence in one lag -> encoder -> information representation ->
# decoder -> sentence in another lang. So the embedding layer is receiving a discretely vectorized
# representation of the information contained in the input sentence. You can think of this as
# a list of approximately correct word vectors in the target language, that the decoder then has to
# "lint". Cf. https://i.imgur.com/R3HZK2k.png
decoder_input = Input(shape=(OUTPUT_LENGTH,))
decoder = Embedding(output_dict_size, 64, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)

# Now the decoder's LSTM layer. The main different here is that initial_state is initialized with
# initial_state of the weights set to a non-default value: the final weights discovered by
# the encoder model.
encoder_last = encoder[:,-1,:]
decoder = LSTM(64, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])

# --------------------------------------------------------------------
# Our target output is a sentence of a certain length. We want the model to vote on the likelihood
# of every individual word in the dictionary as being the next word in the output. E.g. there is one
# probability assigned per token per sentence position. This requires having one fully connected
# (Dense) node per word, e.g. output_dict_size nodes, in a TimeDistributed blanket.

# the Dense layer propogate across the OUTPUT_LENGTH time steps outputs we got from the immediately previous
# LSTM(64, return_sequences=True).
output = TimeDistributed(Dense(output_dict_size, activation="softmax"))(decoder)

# Finally we define the model.
#
# Model training solves the encoder and the decoder simultaneously, so we have to set the model
# construction likewise.
model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder])
model.compile(optimizer='adam', loss='binary_crossentropy')

model.fit(
    x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],
    verbose=2,
    batch_size=64,
    epochs=60
)

# The model has to be applied to data sequentially, with prior tokens fed into the
# decoder module (and the newly contributing neural connections unmasked) as we
# proceed through the task. The code that does this in this particular project,
# taken from the project repo
# (https://github.com/wanasit/katakana/blob/master/katakana/model.py#L92), is as
# follows:
def to_katakana(text, model, input_encoding, output_decoding,
                input_length=DEFAULT_INPUT_LENGTH,
                output_length=DEFAULT_OUTPUT_LENGTH):

    encoder_input = encoding.transform(input_encoding, [text.lower()], input_length)
    decoder_input = np.zeros(shape=(len(encoder_input), output_length))
    decoder_input[:, 0] = encoding.CHAR_CODE_START
    for i in range(1, output_length):
        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)
        decoder_input[:, i] = output[:, i]

    decoder_output = decoder_input
    return encoding.decode(output_decoding, decoder_output[0][1:])


# ### Attention
# 
# The invention of LSTMs revolutionized RNNs because they solved fundamental problems that vanilla RNNs had with utilizing information embedded in the sequence that is contained in tokens that are far away from one another. Attention was the next big "thing" after LSTMs, and caused the next largest jump in RNN performance on real-world problems.
# 
# The bottleneck that attention addresses is the fact that each time-step in an LSTM only gets to work with the output of the previous time-step. Attention adds an additional intermediary step: each LSTM steps additionally gets input from each *previous* LSTM time-step as an input.
# 
# There are several different ways of implementing this additional data flow. The most conceptually intuitive implementation is a weighted sum of the previous layers' data. This is what is meant by attention: the layer learns what previous states to pay attention to, and which previous states to ignore. With attention, it's even possible for a layer to completely ignore the layer immediately previous to it in favor of layers earlier in the time sequence. Hence the name "attention".
# 
# Presumably because "attention" is not a standardized layer, Keras does not provide a prebuilt `Attention` layer. If you want to incorporate attention into your network, you instead need to go ahead and define it yourself.
# 
# The following code is a copy of the same network we defined above, with the addition of the attention module. This attention module was implemented by the author of this model in a follow-up blog post: ["Attention-based sequence-to-sequence in keras"](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html).

# In[ ]:


encoder_input = Input(shape=(INPUT_LENGTH,))
encoder = Embedding(input_dict_size, 64, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)
encoder = LSTM(64, return_sequences=True, unroll=True)(encoder)

encoder_last = encoder[:,-1,:]

decoder_input = Input(shape=(OUTPUT_LENGTH,))
decoder = Embedding(output_dict_size, 64, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)
decoder = LSTM(64, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])

# --------------------------------------------------------------------
# The new attention code follow.

from keras.layers import Activation, dot, concatenate
attention = dot([decoder, encoder], axes=[2, 2])
attention = Activation('softmax')(attention)
context = dot([attention, encoder], axes=[2,1])
decoder_combined_context = concatenate([context, decoder])
output = TimeDistributed(Dense(64, activation="tanh"))(decoder_combined_context)
output = TimeDistributed(Dense(output_dict_size, activation="softmax"))(output)

# --------------------------------------------------------------------
# Compare with the old code:
# output = TimeDistributed(Dense(output_dict_size, activation="softmax"))(decoder)
# --------------------------------------------------------------------

model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder])
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(
    x=[training_encoder_input, training_decoder_input],
    y=[training_decoder_output],
    verbose=2,
    batch_size=64,
    epochs=60
)


# ### Another attention implementation
# 
# Here's another example implementation of an `Attention` layer, taken from the notebook [keras baseline model + attention 5-fold](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold), which was a popular baseline model in one of the Jigsaw toxic comments competitions:

# In[ ]:


class Attention(Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight((input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight((input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim

        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),
                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))

        if self.bias:
            eij += self.b

        eij = K.tanh(eij)

        a = K.exp(eij)

        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0],  self.features_dim


# An RNN model need not be seq-to-seq to have an attention layer; attention has been shown to produce improved results in all kinds of RNN models. Seq-to-seq RNN models are simply the model archetype for which attention has first developed.
# 
# ### Another seq-to-seq model
# 
# This seq-to-seq model is an especially simple one [provided in the Keras documentation as an example implementation](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py).

# In[ ]:


batch_size = 64
epochs = 100
num_samples = 10000

# "Latent dimensionality" of the encoding space --- aka the length
# of the hidden word vectors.
latent_dim = 256

# This model assumes that our inputs are already tokenized. In fact in the
# original source code the inputs are individual *characters*, not word
# tokens, as would be typical in application. This was done for the sake
# of demonstrative simplicity.
encoder_inputs = Input(shape=(None, num_encoder_tokens))

# return_state=True means that the hidden and cell state vectors of the final
# layer of the LSTM are returned, in addition to the output vector. As in the
# previous model, the weights on the LSTM output layer in the encoder model
# are used to seed the weights in the first layer of the LSTM input layer in the
# decoder model.
encoder = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(
    decoder_inputs,
    # setting the initial_state value here as before is what creates the
    # encoder-decoder dependency
    initial_state=encoder_states
)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Now define the model and train it.
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(
    optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']
)
model.fit(
    [encoder_input_data, decoder_input_data], decoder_target_data,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.2
)
model.save('s2s.h5')

# This model has a similar sequential stepping mechanism to its prediction
# application, omitted for the sake of brevity. Check the source code for that.


# ## Further optimizations to teacher forcing
# 
# Teacher forcing improves on the stability of a machine learning model that is applied recursively, but it doesn't solve all of the problems with it completely. It still has issues generalizing to target sequences that differ significantly from those which were seen in training.
# 
# This occurs because recursively feeding in prior token input is a greedy operation; taking the optimally scored output at a particular time point lead to a local optimum instead of a global one (in terms of the total fit of the generated output sentence).
# 
# There are a couple of techniques that are used to correct this effect. The first is **beam search**. In beam search, instead of taking the highest-scored token at each time step (`argmax` in the code above), we take the top `n` tokens at each time step. We then generate the top `n` probabilities of the next tokens (so `n * n` possibilities), and keep only the best-scoring `n` of them. This repeats across time steps.
# 
# Greedy search is just beam search with a beam size of 1. Benchmark problems in machine translation are solved with beam sizes between 5 and 10.
# 
# Optimal search is not possible because it is exponentially hard: e.g. it requires `n_dict ^ seq_length` operations.
# 
# The machinelearning mastery blog has a post dedicated to the subject of beam search titled ["How to Implement a Beam Search Decoder for Natural Language Processing"](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/). It provides the following toy implementation:

# In[ ]:


from math import log
from numpy import array
from numpy import argmax
 
# beam search
def beam_search_decoder(data, k):
	sequences = [[list(), 1.0]]
	# walk over each step in sequence
	for row in data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score * -log(row[j])]
				all_candidates.append(candidate)
		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best
		sequences = ordered[:k]
	return sequences
 
# define a sequence of 10 words over a vocab of 5 words
data = [[0.1, 0.2, 0.3, 0.4, 0.5],
		[0.5, 0.4, 0.3, 0.2, 0.1],
		[0.1, 0.2, 0.3, 0.4, 0.5],
		[0.5, 0.4, 0.3, 0.2, 0.1],
		[0.1, 0.2, 0.3, 0.4, 0.5],
		[0.5, 0.4, 0.3, 0.2, 0.1],
		[0.1, 0.2, 0.3, 0.4, 0.5],
		[0.5, 0.4, 0.3, 0.2, 0.1],
		[0.1, 0.2, 0.3, 0.4, 0.5],
		[0.5, 0.4, 0.3, 0.2, 0.1]]
data = array(data)
# decode sequence
result = beam_search_decoder(data, 3)
# print result
for seq in result:
	print(seq)


# Another, far more complicated form of fine-tuning not covered by a code sample here is curriculum learning. This is the subject of a future notebook however.
