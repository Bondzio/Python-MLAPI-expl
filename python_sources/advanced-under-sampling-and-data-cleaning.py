#!/usr/bin/env python
# coding: utf-8

# # Advanced under-sampling and data cleaning
# 
# ## Introduction 
# 
# In two previous notebooks I introduced [undersampling and oversampling](https://www.kaggle.com/residentmario/undersampling-and-oversampling-imbalanced-data/) and then further discussed [advanced oversampling techniques](https://www.kaggle.com/residentmario/oversampling-with-smote-and-adasyn/). This notebook is the third in this series, covering advanced undersampling techniques. The techniques covered in these three notebooks comprise the whole of the ones available in the `imblearn` `sklearn-contrib` module.
# 
# To recap, sampling is a general-purpose preprocessing technique for turning imbalanced datasets into balanced ones, and thus, imbalanced machine learning models into balanced ones. There are other approaches to achieving this end, like changing the algorithm cost function, but sampling is a general-purpose technique that works reasonably well everywhere. The simplest, most explainable, and oftentime most effective techniques for performing sampling are random under- and over- sampling. These two sampling methodologies have broadly simpler characteristics. More complex adaptations of over-sampling exist in the form of the SMOTE and ADASYN algorithms and variants. For some datasets, these approaches may have better performance.
# 
# Of course there are advanced under-sampling techniques as well. `imblearn` divides these into two broad classes. The simple `RandomUnderSampler` that we saw in the first notebook is an example of **prototype selection**: it attempts to select points which are "representative prototypes" for the cluster at large. Sampling the points randomly is just the most naive way of achieving this goal. A competing idea is **prototype generation**. Samplers which are prototype generation algorithms will not select exact points out of the dataset, but will instead generate new points somehow representative of existing ones.
# 
# Additionally this notebook will look at some data cleaning algorithms provided in `imblearn`.
# 
# ## Data
# 
# We'll use the same synthetic dataset as in the two previous sections.

# In[2]:


import seaborn as sns
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,
                           n_redundant=0, n_repeated=0, n_classes=3,
                           n_clusters_per_class=1,
                           weights=[0.01, 0.05, 0.94],
                           class_sep=0.8, random_state=0)

import matplotlib.pyplot as plt
colors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y]
kwarg_params = {'linewidth': 1, 'edgecolor': 'black'}
plt.scatter(X[:, 0], X[:, 1], c=colors, **kwarg_params)
sns.despine()


# ## Undersaming techniques
# 
# ### ClusterCentroids
# 
# The `ClusterCentroids` algorithm is the one example of a prototype generation algorithm provided in `imblearn`. This algorithm performs K-means clustering on the majority classes, then creates new points which are averages of the coordinates of the generated clusters. Here it is applied to our example dataset:

# In[3]:


from imblearn.under_sampling import ClusterCentroids
trans = ClusterCentroids(random_state=0)
X_resampled, y_resampled = trans.fit_sample(X, y)

plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
sns.despine()


# `ClusterCentroids` comes with the implicit expectation that the points being sampled belong to moderately well-defined clusters. In other words, this algorithm will not capture outlier points that are not sufficiently dense, as these points will be pulled back towards the inner points by the cluster mean operation. As a result the clusters generated by `ClusterCentroids` will thus be marginally tighter and better-defined than the ones generated by `RandomUnderSampler`. That is not to say that this algorithm will completely eliminate outliers; we can see in the plot above that there are outlier points from the white and red classes represented in the blue cluster.
# 
# Overall I find the idea behind `ClusterCentroids` to be quite appealing. 

# ### NearMiss
# 
# `NearMiss` is a prototype selection undersampling technique which creates significantly more radical class clusters. There are three modes available. The `imblearn` documentation includes a section of this algorithm's mathematical formulation, which I won't copy (you can read it yourself [here](http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#mathematical-formulation)). But basically, all three versions of this algorithm specifically select points that are near the minority class. They have pretty radical effects.

# In[6]:


from imblearn.under_sampling import NearMiss
trans = NearMiss(version=1)
X_resampled, y_resampled = trans.fit_sample(X, y)

plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
sns.despine()


# This is version 1 of `NearMiss`. We can see that the points sampled out of the white and blue classes are those which are nearest to minority class (red) outlier points in their respective point clouds.

# In[7]:


from imblearn.under_sampling import NearMiss
trans = NearMiss(version=2)
X_resampled, y_resampled = trans.fit_sample(X, y)

plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
sns.despine()


# Version 2 of `NearMiss` is similar.

# In[8]:


from imblearn.under_sampling import NearMiss
trans = NearMiss(version=3)
X_resampled, y_resampled = trans.fit_sample(X, y)

plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
sns.despine()


# Version 3 is different. This version will pick balanced clusters of points that are closest to outlier points from other classes, instead of building giant blob(s) around one or a handful of such points.
# 
# It's mildly interesting that these algorithms exist; perhaps they could be adapted for some kind of data visualization exploring where the difficult points are? But I can't see myself using any of them anytime soon.
# 
# ## Data cleaning techniques
# 
# Undersampling may be used to clean data&mdash;that is, to remove outlier points.
# 
# Machine learning algorithms which are strongly affected by the presence of outliers, regression fo example, can see significant improvements from this type of cleaning being performed before model training. To deal with this problem, the current recommendation in practice is to use regularization; that is, to modify the cost of the machine learning algorithm being trained so as to reduce its tendancy to overfit (and hence, to pick out outlier points). This approach has proven historically to be the most robust and least prone to errors than the alternative in this section&mdash;undersampling with the purpose of data cleaning. It is generally not recommended to clean your data before generating models on it, though you certainly can.
# 
# Undersample data cleaning has great value as a separate preprocessing technique. Most of the time it comes in handy during the process of de-junking the dataset, that is, dealing with datasets which have many spurious observations in them, due to the way they were generated, collected, etectera. If these points are considered "out-of-band", undersampling may be used to find them (you will almost certainly want to look through at least some of these cases by hand, to understand what's going on) and then get rid of them. 
# 
# ### Tomek links
# 
# Tomek links are points in the dataset whose nearest neighbor is a member of a different class. This includes outlier points embedded in a point cloud from another class, and boundary points in regions where it is unclear which of two or more classes is dominant. The `TomekLinks` undersampler can be used to remove these points. Here's an illustration of what this looks like, from their documentation:
# 
# 
# 
# By default it will remove only links from majority classes, but I suggest passing `ratio='all'` so that all of the classes will be considered and treated.

# In[58]:


import numpy as np
sampler = TomekLinks(random_state=0)

# minority class
X_minority = np.transpose([[1.1, 1.3, 1.15, 0.8, 0.55, 2.1],
                           [1., 1.5, 1.7, 2.5, 0.55, 1.9]])
# majority class
X_majority = np.transpose([[2.1, 2.12, 2.13, 2.14, 2.2, 2.3, 2.5, 2.45],
                           [1.5, 2.1, 2.7, 0.9, 1.0, 1.4, 2.4, 2.9]])

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

ax_arr = (ax1, ax2)
title_arr = ('Removing only majority samples',
             'Removing all samples')
for ax, title, sampler in zip(ax_arr,
                              title_arr,
                              [TomekLinks(ratio='auto', random_state=0),
                               TomekLinks(ratio='all', random_state=0)]):
    X_res, y_res = sampler.fit_sample(np.vstack((X_minority, X_majority)),
                                      np.array([0] * X_minority.shape[0] +
                                               [1] * X_majority.shape[0]))
    ax.scatter(X_res[y_res == 0][:, 0], X_res[y_res == 0][:, 1],
               label='Minority class', s=200, marker='_')
    ax.scatter(X_res[y_res == 1][:, 0], X_res[y_res == 1][:, 1],
               label='Majority class', s=200, marker='+')

    # highlight the samples of interest
    ax.scatter([X_minority[-1, 0], X_majority[1, 0]],
               [X_minority[-1, 1], X_majority[1, 1]],
               label='Tomek link', s=200, alpha=0.3)

    ax.set_title(title)
#     make_plot_despine(ax)
fig.tight_layout()

plt.show()


# Here's the result of an application to our sample dataset. It doesn't look much different overall.

# In[56]:


from imblearn.under_sampling import TomekLinks
trans = TomekLinks(ratio='all')
X_resampled, y_resampled = trans.fit_sample(X, y)

plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
sns.despine()


# But if you look at the number of points we've removed 42 outlier points.

# In[59]:


len(X), len(X_resampled)


# In the following plot I show which points from the minority class were removed, to better illustrate what _kinds_ of points get cleaned from the dataset:

# In[57]:


_X_minority = X[np.where(y == 0)[0]]
plt.scatter(_X_minority[:, 0], _X_minority[:, 1], c='white', linewidth=1, edgecolor='red')

trans = TomekLinks(ratio='all')
X_resampled, y_resampled = trans.fit_sample(X, y)
_X_minority = X_resampled[np.where(y_resampled == 0)[0]]
plt.scatter(_X_minority[:, 0], _X_minority[:, 1], c='lightgreen', edgecolor='green', linewidth=1)
plt.suptitle("Tomek Links Removed from Minority Class")
pass


# The `TomekLinks` algorithm has the advantage that it is quite conservative; that is, it is good at finding and removing points we are quite certain are outliers (though it pays to inspect the result yourself afterwards, just to be sure the right things were removed).
# 
# It's not easily tunable however; we can't tell the algorithm to be more or less aggressive. Also Tomek's Links retains some outliers. In particular, points which are outliers which are closest to *other* points that are outliers will be retained. A handful of them show up in this example.

# ### EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN
# 
# `EditedNearestNeighbours` (and a couple of similar algorithms which are tweaks thereof) use the K nearest neighbors algorithm to determine which points to move out. The algorithm allows you to remove points which contain a majority of points not in the same class as the point under consideration. Alternatively, and even more strictly, you may choose to remove points containing at least one neighor not in the neighborhood. 
# 
# This algorithm provides the flexibility that `TomekLink` lacks because it allows you to set the size of the neigborhood under consideration yourself. In fact, a `TomekLink` is actually just a special named case of this K nearest neighbors approach where `k=1`.

# In[113]:


from imblearn.under_sampling import EditedNearestNeighbours

fig, axarr = plt.subplots(2, 2, figsize=(12, 12))
kwarg_params = {'edgecolor': 'darkgray'}

trans = EditedNearestNeighbours()
X_resampled, y_resampled = trans.fit_sample(X, y)
X_diff = 5000 - len(X_resampled)
print(X_diff)
plt.sca(axarr[0][0])
plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
axarr[0][0].set_title("$kind=all, k=3$, $\Delta n =-155$")

trans = EditedNearestNeighbours(n_neighbors=10)
X_resampled, y_resampled = trans.fit_sample(X, y)
X_diff = 5000 - len(X_resampled)
print(X_diff)
plt.sca(axarr[0][1])
plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
axarr[0][1].set_title("$kind=all, k=10$, $\Delta n =-411$")

trans = EditedNearestNeighbours(n_neighbors=1, kind_sel='mode')
X_resampled, y_resampled = trans.fit_sample(X, y)
X_diff = 5000 - len(X_resampled)
print(X_diff)
plt.sca(axarr[1][0])
plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
axarr[1][0].set_title("$kind=all, k=1$, $\Delta n =-68$")

trans = EditedNearestNeighbours(n_neighbors=3, kind_sel='mode')
X_resampled, y_resampled = trans.fit_sample(X, y)
X_diff = 5000 - len(X_resampled)
print(X_diff)
plt.sca(axarr[1][1])
plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)
axarr[1][1].set_title("$kind=all, k=10$, $\Delta n =-36$")
plt.suptitle("EditedNearestNeighbours Output w/ Various Settings")
pass


# The bottom left panel in this demo is the same output we got out of the `TomekLinks` run in the previous section. Otherwise we can see that by increasing the sizes of the neighborhoods and setting `model=all` we can start to peel back quite a lot of boundary points in the dataset.
# 
# This algorithm gives us the maximum control over our data cleaning, and it's the one that I recommend using.
# 
# There are a couple of other algorithms which build on top of this one. First of all there is `RepeatedEditedNearestNeighbours`, which runs this algorithm a user-specified number of times. Each time the algorithm is rerun more outlier points will be removed, so this is a way of whittling down on poorly-behaved points that is an alternative to the somewhat totalitarian `n_neighbors` approach. I can't comment on the specific advantages of either approach, however; I suppose that you'll just want to try them both out.
# 
# `AllKNN` is a modification of `RepeatedEditedNearestNeighbours` which also expands the size of the neighborhood being considered each time it is run.
# 
# Overall I'd prefer to just use and tune `EditedNearestNeigbours`.
# 
# ### More algorithms
# 
# There are *even more* undersampling algorithms available in `imblearn`.  I won't cover them here as they start get pretty nuanced however. If you're deep in the weeds on a data cleaning problem, you should check out the official documentation with the rest of the owl here: [link](http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#mathematical-formulation). `EditedNearestNeighbors` fits most of the cases I run into however.
