#!/usr/bin/env python
# coding: utf-8

# # A Simple MLP for Reuters News Classifier in Keras for NLP
# 
# Natural language processing (NLP) is the process of automatic processing of information written or spoken in a natural language using an electronic calculator. This is made particularly difficult and complex due to the intrinsic characteristics of the ambiguity of human language. When it's necessary to make the machine learn methods of interaction with the environment typical of man, the question isn't so much that of storing data, but that of letting the machine learn how this data can be translated simultaneously to create a concept. Natural language interacts with the environment generating predictive knowledge.
# 
# In this Notebook, We will use Simple MLP with Keras layers to build a model to classify Reuter's newswire.
# 
# ### Table of interest:
# 1. About the dataset.
# 2. Data preprocessing and visualization.
# 3. Build the Classifier.
# 4. Train the model.
# 5. Evaluate the model.

# In[ ]:



import numpy as np

from keras.datasets import reuters
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.preprocessing.text import Tokenizer

import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')


# ## 1. About the dataset.
# This is a dataset of 11,228 newswires from Reuters, labeled over 46 topics. This was originally generated by parsing and preprocessing the classic Reuters-21578 dataset, but the preprocessing code is no longer packaged with Keras.
# 
# 
# Each newswire is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer "3" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: "only consider the top 10,000 most common words, but eliminate the top 20 most common words".
# 
# As a convention, "0" does not stand for a specific word, but instead is used to encode any unknown word.

# In[ ]:


(XTrain, YTrain),(XTest, YTest) = reuters.load_data(num_words=None, test_split=0.3)

print('XTrain class = ',type(XTrain))
print('YTrain class = ',type(YTrain))
print('XTest shape = ',type(XTest))
print('YTest shape = ',type(YTest))

print('XTrain shape = ',XTrain.shape)
print('XTest shape = ',XTest.shape)
print('YTrain shape = ',YTrain.shape)
print('YTest shape = ',YTest.shape)


# ## 2. Data preprocessing and visualization.

# In[ ]:


print('YTrain values = ',np.unique(YTrain))
print('YTest values = ',np.unique(YTest))

unique, counts = np.unique(YTrain, return_counts=True)
print('YTrain distribution = ',dict(zip(unique, counts)))
unique, counts = np.unique(YTest, return_counts=True)
print('YTrain distribution = ',dict(zip(unique, counts)))


# In[ ]:


plt.figure(1)
plt.subplot(121)
plt.hist(YTrain, bins='auto')
plt.xlabel("Classes")
plt.ylabel("Number of occurrences")
plt.title("YTrain data")

plt.subplot(122)
plt.hist(YTest, bins='auto')
plt.xlabel("Classes")
plt.ylabel("Number of occurrences")
plt.title("YTest data")
plt.show()

print(XTrain[1])
len(XTrain[1])


# ### Tokenization and preprocessing of labels.

# In[ ]:


#The dataset_reuters_word_index() function returns a list where the names are words and the values are integer
WordIndex = reuters.get_word_index(path="reuters_word_index.json")

print(len(WordIndex))

IndexToWord = {}
for key, value in WordIndex.items():
    IndexToWord[value] = key

print(' '.join([IndexToWord[x] for x in XTrain[1]]))
print(YTrain[1])

MaxWords = 10000

# Tokenization of words.
Tok = Tokenizer(num_words=MaxWords)
XTrain = Tok.sequences_to_matrix(XTrain, mode='binary')
XTest = Tok.sequences_to_matrix(XTest, mode='binary')

# Preprocessing of labels
NumClasses = max(YTrain) + 1
YTrain = to_categorical(YTrain, NumClasses)
YTest = to_categorical(YTest, NumClasses)

print(XTrain[1])
print(len(XTrain[1]))


# ## 3. Build the Classifier.

# In[ ]:


model = Sequential()
model.add(Dense(512, input_shape=(MaxWords,)))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(NumClasses))
model.add(Activation('softmax'))
model.summary()

model.compile(loss='categorical_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])


# ## 4. Train the model.

# In[ ]:


history = model.fit(XTrain, YTrain, 
                    validation_data=(XTest, YTest), 
                    epochs=10, 
                    batch_size=64)


# ## 5. Evaluate the model.
# ### Scoring the model.

# In[ ]:


Scores = model.evaluate(XTest, YTest, verbose=1)
print('Test loss:', Scores[0])
print('Test accuracy:', Scores[1])


# ### Plot history of training and validations cuvres.

# In[ ]:


def plotmodelhistory(history): 
    fig, axs = plt.subplots(1,2,figsize=(15,5)) 
    # summarize history for accuracy
    axs[0].plot(history.history['accuracy']) 
    axs[0].plot(history.history['val_accuracy']) 
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy') 
    axs[0].set_xlabel('Epoch')
    axs[0].legend(['train', 'validate'], loc='upper left')
    # summarize history for loss
    axs[1].plot(history.history['loss']) 
    axs[1].plot(history.history['val_loss']) 
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss') 
    axs[1].set_xlabel('Epoch')
    axs[1].legend(['train', 'validate'], loc='upper left')
    plt.show()

# list all data in history
print(history.history.keys())

plotmodelhistory(history)


# **Hope that you find this notebook helpful. More to come.**
# 
# **If it's useful for you, Please upvote this to keep me motivate for doing better.**
# 
# **Thanks for comment and suggestions.**
# 
