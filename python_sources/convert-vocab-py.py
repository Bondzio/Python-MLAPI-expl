#!/usr/bin/env python
# coding: utf-8

# i translated gpt2 vocab-file with Ground-truth bilingual dictionaries of [MUSE](https://github.com/facebookresearch/MUSE) project and translators module.  
# and train on [translated datasets@Michael Kazachok](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api/version/1)  
# LB: 0.8177 :(  
# (on local validation(not CV), i get ROC-AUC:0.951. each lang score are similar )  

# In[ ]:


get_ipython().run_cell_magic('writefile', 'convert_vocab.py', 'import argparse\nimport os\nfrom pathlib import Path\nfrom time import sleep\n\nimport re\nimport json\n\nimport translators as ts\n\nnot_en = re.compile(r\'[^a-z^A-Z]\')\n\n\ndef vocab_convertor(path_vocab, tgt_lang, path_bilingual_dict):\n\n    flag = False\n    fname, ext = path_vocab.split(\'.\')\n    path_new_vocab = fname + \'_\' + tgt_lang + \'.json\'\n    cache_path = \'translated-\' + tgt_lang  + \'.log\'\n    \n   # translated = open(\'translated-\' + tgt_lang  + \'.log\', \'a\', encoding=\'UTF-8\')\n    \n    with open(path_vocab, encoding=\'UTF-8\') as json_file:\n        vocab = json.load(json_file)\n    \n    translated_dict = {}\n    bilingual_dict = {}\n    new_vocab = {}\n    \n    \n    with open(path_bilingual_dict, encoding=\'UTF-8\') as f:\n        try:\n            for line in f:\n                print(line)\n                src, tgt = line.split(\' \')\n                if not src in bilingual_dict:\n                    bilingual_dict[src] = tgt.replace(\'\\n\', \'\')\n        except:\n            for line in f:\n                print(line)\n                src, tgt = line.split(\'\\t\')\n                if not src in bilingual_dict:\n                    bilingual_dict[src] = tgt.replace(\'\\n\', \'\')\n    \n    if not os.path.exists(cache_path):\n        Path(cache_path).touch()\n\n    with open(cache_path, encoding=\'UTF-8\') as f:\n        for line in f:\n            src_tgt__ = line.split(\' \')\n            src = src_tgt__[0]\n            tgt = src_tgt__[1]\n            translated_dict[src] = tgt.replace(\'\\n\', \'\')\n        \n        \n    for i, (k, v) in enumerate(vocab.items()):\n\n        with open(\'translated-\' + tgt_lang  + \'.log\', \'a\', encoding=\'UTF-8\') as translated:\n            print(k, v)\n        \n            _k = k.split(\'\\u0120\')\n            k_old = _k[-1]\n            k = None\n        \n        \n            if bool(not_en.search(k_old)) | (v == 220) | (1==len(k_old)):\n                k = k_old\n                \n            else:\n            \n                if k_old in bilingual_dict:\n                    k = bilingual_dict[k_old]\n                elif k_old in translated_dict:\n                    k = translated_dict[k_old]\n                else:\n                    k_ts = ts.google(k_old, \'en\', tgt_lang)\n                \n                    translated.write(k_old+\' \'+k_ts+\'\\n\')\n                    k = k_ts\n                    sleep(2)\n                    \n            \n            if 1 == len(_k):\n                new_vocab[k] = v\n            else:\n                new_vocab[\'\\u0120\'+k] = v\n            \n        \n            \n    with open(path_new_vocab,"w", encoding=\'utf-8\') as jsonfile:\n        json.dump(new_vocab,jsonfile,ensure_ascii=False)\n    \n    #"""\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument("path_vocab", type=str)\n    parser.add_argument("tgt_lang", type=str)\n    parser.add_argument("path_bilingual_dict", type=str)\n    args = parser.parse_args()\n\n    vocab_convertor(args.path_vocab, args.tgt_lang, args.path_bilingual_dict)')

