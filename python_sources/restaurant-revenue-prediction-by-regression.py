#!/usr/bin/env python
# coding: utf-8

# **Introduction to data**
# TFI has provided training data with 137 restaurants across various cities along with revenue of restaurants.
# Test data has over 1000 reataurants whose revenue is to be predicted.
# 
# Notebook created by : [Akshay SB](https://www.linkedin.com/in/akshaybidarkundi/)
# 
# Here I have used basic exploratory analysis to find relationship between columns.
# 
# **Please do upvote if you find this notebook useful, it will motivate me to work harder**
# 
# Let us look at the columns in the dataset:
# 1. Id - Unique identification number of restaurant
# 2. Open Date - Date on which restaurant was opened
# 3. City - City in which restaurant is located
# 4. City group - City in which restaurant is located belong to either of these categories. (big cities or other)
# 5. Type - Each restaurant is either FC: Food Court, IL: Inline, DT: Drive Thru
# 6. P1 - P37 - There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.
# 7. revenue - Revenue generated by each restaurant

# **Let us import the required libraries**

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge,Lasso
from sklearn.model_selection import RandomizedSearchCV

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.


# In[ ]:


df=pd.read_csv('../input/cmpe343/train.csv')
df_t=pd.read_csv('../input/cmpe343/test.csv')


# **Let us have look at 5 rows to understand the data**

# In[ ]:


df.head()


# From open date column it is seen that, date format is not same through all columns, let us make the standard date format

# In[ ]:


df['Open Date'] = pd.to_datetime(df['Open Date'], format='%m/%d/%Y') 
df_t['Open Date'] = pd.to_datetime(df_t['Open Date'], format='%m/%d/%Y')


# Let us look at summary of the data

# In[ ]:


df.describe()


# Let us check for presence of any null values.

# In[ ]:


df.isnull().sum()


# There are no null values in the dataset

# Id column is used just for identification and is of no use for us. Let us drop the 'Id' column

# In[ ]:


df.drop('Id',axis=1,inplace=True)
df_t.drop('Id',axis=1,inplace=True)


# # **Exploratory Data Analysis**

# Let us look at some of the insights from data

# In[ ]:


print(df['City Group'].value_counts())
df['City Group'].value_counts().plot(kind='bar')
plt.xlabel('Type of city')
plt.ylabel('No. of restaurants')
plt.title('No. of Restaurants in Big cities and Other cities')
plt.show()


# In[ ]:


print(df['Type'].value_counts())
df['Type'].value_counts().plot(kind='bar')
plt.xlabel('Type of Restaurants')
plt.ylabel('No. of restaurants')
plt.title('No. of Restaurants of each city')
plt.show()


# From above plot, it can be observed that there are 76 food courts, 60 in-line restaurants and 1 drive-thru restaurant.

# # **Encoding the data**
# 
# Since there are few categorical data in our dataset, we need to convert them into numerical by encoding

# In[ ]:


df1=pd.get_dummies(df,columns=['City'],drop_first=True)


# In[ ]:


df_t=pd.get_dummies(df_t,columns=['City'],drop_first=True)


# Since linear regression does not consider date values, there are many ways to convert that into numerical date, here I will use Julian date to convert date into number so that it can be used for analysis.
# Reference : [Julian Day](https://en.wikipedia.org/wiki/Julian_day)

# In[ ]:


x_d= df1.set_index('Open Date', append=False)
x_d=x_d.index.to_julian_date()


# In[ ]:


x1= df_t.set_index('Open Date', append=False)
x1=x1.index.to_julian_date()


# In[ ]:


df1['open date']=x_d


# In[ ]:


df_t['open date']=x1


# In[ ]:


df1.drop('Open Date',axis=1,inplace=True)


# In[ ]:


df_t.drop('Open Date',axis=1,inplace=True)


# In[ ]:


df1['City Group'].replace({'Big Cities':1,'Other':0},inplace=True)
df1['Type'].replace({'IL':0,'FC':1,'DT':2,'MB':3},inplace=True)
df_t['City Group'].replace({'Big Cities':1,'Other':0})
df_t['Type'].replace({'IL':0,'FC':1,'DT':2,'MB':3},inplace=True)


# In[ ]:


df1


# Here open date values are high compared to other values, let's try to standardize the open date column

# In[ ]:





# In[ ]:


cols


# In[ ]:


from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
df_x=ss.fit_transform(df1[['open date']])


# In[ ]:


df1['open date']=df_x


# In[ ]:


df1.head()


# Let us split the data into dependent and independent variables
# x consists of all the features and y consists of target variable

# # **Data splitting**

# In[ ]:


x=df1.drop('revenue',axis=1)


# In[ ]:


y=df1['revenue']


# # **Model Building**

# Let us build the model using Linear  Regression model

# In[ ]:


from sklearn.linear_model import LinearRegression
model=LinearRegression()
model.fit(x,y)
print('Coefficients are :\n')
list(zip(x.columns,model.coef_))


# In[ ]:


print('R square value: ',model.score(x,y))


# Here the coefficients are very high which may over-fit the models. Let us try regularization techniques to solve the issue
