#!/usr/bin/env python
# coding: utf-8

# For a long time, large software companies have been invested in engineering robust information systems to limit and manage operational and business risk.  Systems regularly undergo security tests and audits, and many systems are tested against simulated infrastructure, network and application failures to identify and correct for weaknesses in the system.  
# 
# In Data Science, the idea of engineering against failure is far from a new concept within industry. Many Machine Learning assumptions and best practises force practitioners to interrogate their model's performance by simulating out-of-sample accuracy using a subsampled validation dataset which the model was not trained on. This is often built-in to many machine learning libraries and frameworks, and is a standard litmus test in diagnosing overfitting. This cross-validation schema makes very limited assumptions about your data-generating process, and if your model is performing badly on these simple test samples, you are in the first Circle of Data Science Hell.  
# 
# Often, in practice, simple cross-validation is seldom enough. If you have ever computed bootstrap confidence intervals, you will have an excellent intuition for how much data can vary between random samples. The accuracy or errors of a model can be high but can be biased or rely on naive distributional assumptions which are either violated by the data or ignore the business case being investigated. It is for this reason that Data Scientists will test on many test and train samples to compare not only the average scores across many dissimilar metrics but also the variance of those metrics across multiple folds of the data. If your model's performance varies greatly across folds or between metrics, you are in the second Circle of Data Science Hell. In the second circle, this variance in metric performance across folds may be incredibly insightful for decision-makers and give them a greater understanding of the risks involved in productionizing a model.  
# 
# For anyone who has dealt with bag-of-words encodings for natural language, you may have an appreciation for how features themselves can be randomly observed. If you one hot encode a random variable for postal codes, you may have fewer or no observations of particular postal codes in different samples of the data. In fact, some postal codes may just have not been observed yet, but viewing only a subset of them may allow the model to overfit to undersampled postal codes in particular folds.  In the third Circle of Data Science Hell, data or labels can be missing, or we may have undersampled features, classes or clusters in the data.  Designing methods for cross-validation which simulate these missing labels or clusters can be useful to identify how models may fare in the real world when new cluster may emerge, new words may get added, or labels distributions change. This can be a critical step in dissecting feature importance and algorithmic bias.  This practice may also require simulating data or label noise in order to determine the robustness of our model to missing or incorrect data and may resample training data or experiment with label smoothing.  
# 
# In much of Machine Learning, data is assumed to be independent and identically distributed (iid). This means that data is unaffected and uncorrelated with previous observations, and all data is generated from the same- may be complicated- data generating process. In the fourth Circle in Data Science Hell, data is not iid and may change or drift over time.  
# 
# This is a particular problem if you think of large online retailers. Consumer behaviour is often dependent on previous purchasing decisions: if you buy a phone today, you are more likely to buy a phone case tomorrow; if groups of people buy fidget spinners today, others are likely to buy fidget spinners tomorrow as they rise and fall in popularity; and if your business grows, you may start offering new product categories which change customer baskets.  These all cause distribution of your data changes over time, and that models trained on old data may no longer accurate or informative for users in the future. 
# 
# Many the common practises in Machine Learning may be to randomly shuffle your data before splitting it into test and train sets, this can be dangerous in industries where data changes over time.  The fourth Circle of Data Science Hell requires one to critically engage and think about the data generating process, test those assumptions and design methods for validating models and assumptions based on prior knowledge.   This may involve simulating worst-case test-train splits and designing model validation procedures which simulate how you intend to update your model based on model and data drift over time.  For many Machine Learning pipelines, phenomena like seasonality or trends are often modelled indirectly and dealt with by tracking metrics in production and triggering regular model updates.  This means that without testing your update triggers, you can run the severe risk sub-optimal updates over time.  
# 
# Machine Learning models don't exist in isolation; they exist inside a system. In recent years, Machine Learning researchers have gone a step further to consider the lifecycle and information system of a model and interrogate how malicious agents or extreme outliers may interact with and exploit details of the model in production.
# 
# Adversarial robustness is a field which tries to identify potential attack vectors to machine learning models and use methods in simulation and optimization to help guard against these threats. The field considers three main areas of research: Evasion, in which adversarial agents aim to trigger erroneous predictions from a model, Extraction, in which adversarial agents try to extract an approximation to a model, and Poisoning, where agents work to feed data to a model in production to optimally degrade its accuracy. While we will not go into detail into this area, it is critical to analyze the information system your model falls into and test and simulate how users or malicious actors may interact with your system. It can be vital to engineer how your model updating trigger will cope with corrupt data fed to your system and test the mechanisms in place to handle these failures outside of the model. This is our fifth and final Circle in Data Science Hell.  
# 
# For businesses developing Machine Learning products and systems, it is critical not just to implement all these safe guards, but interrogate the risk and costs to the business in Machine Learning development. Not every model will require safeguards to all five Circles of Data Science Hell, and in some scenarios and industries, there may be some additional measure required to validate a model going into production.  Simulation and validation is key and should be a critical and ongoing discussion in all Data Science teams.  As always, knowing your data and knowing your models is critical in managing risk and being able to guard against it. Today, Machine Learning is used is a wide array of critical applications from banking to medical diagnosis.  Without adequite testing, you have the potential to create great harm. Validation is about more than risk, its a critical responsibilty which should have equal, if not greater, standing to modelling. 
